## 有规律的不确定性

在《不确定世界中的理性选择》一书中，Robyn Dawes 描述了 Tversky 做过的一个实验：<sup>1</sup>

> 20世纪50年代末和60年代初，心理学家做了许多实验，让被试预测带有随机成分但又有基准概率可循的事件结果——比如让被试预测实验者翻开的下一张牌是红色还是蓝色，在这种情境下，70%的牌是蓝色，但红蓝牌的顺序完全随机。
> 在这种情况下，能获得最高成功率的策略是每次都预测更常见的结果。例如，如果70%的牌是蓝色，那么每次都猜蓝色，成功率就是70%。
> 然而，被试通常采用的策略是概率匹配——也就是说，他们以事件出现的相对频率来预测。例如，被试会有70%的时间猜蓝色，30%的时间猜红色。这样做的成功率只有58%，因为在蓝牌出现时（概率0.70）他们有70%的正确率，在红牌出现时（概率0.30）有30%的正确率；（0.70×0.70）+（0.30×0.30）= 0.58。
> 实际上，被试预测更常见事件的概率会略高于其实际出现概率，但远远达不到100%，即使他们的预测准确会得到金钱奖励……比如，有些被试在1000次实验中，每猜对一次就奖励5美分……他们预测更常见事件的比例为76%。

不要以为这个实验只是关于赌博策略的小缺陷。它其实精炼地展示了整个理性思维中最重要的思想。

被试总是不断地猜红色，好像他们觉得自己能预测这串随机序列。Dawes 继续评论道：“即使经历了一千次反馈，被试还是无法相信自己面对的是一个无法预测的情境。”

但这个错误其实更深。即使被试以为自己发现了某种规律，他们其实没必要真的按照这个假设去下注来验证。完全可以说：“如果我的假设对，下一张应该是红色”——但下注时还是选蓝色。每次都选蓝色，尽可能多地赚取奖励，同时在心里记录自己发现的任何模式。如果他们的预测真的对了，再切换到新发现的规律也不迟。

我不会责怪被试不断尝试新假设——毕竟他们怎么知道这序列真的无法预测呢？但如果明明没必要还要用猜测去下注，尤其是在数百次猜测都被证伪的情况下，那就值得批评了。

人类真的会这么自信吗？

我怀疑其实原因更简单——被试根本没想到“全选蓝色”这个策略。

人们看到大多数是蓝牌，少数是红牌，就以为最优策略也应该是大多数选蓝，少数选红。

其实，最优策略并不一定要像实际序列那样“混合”。这是反直觉的：在信息不完全的情况下，最优的下注策略未必和实际序列的分布相似。

反直觉的是，即使环境中有随机成分，最优策略依然是“有规律地”行动。

你可能觉得自己的行为也应该像环境一样不可预测——但事实并非如此！随机的钥匙并不能打开随机的锁，仅仅因为它们“都随机”。

你不能用火攻火，而要用水灭火。但这个想法需要多走一步，需要一个问题本身不会直接激活的新概念，所以它不是人们第一时间想到的。

在蓝红牌的困境中，我们有限的知识每一轮都告诉我们：最好的选择是选蓝色。每一轮，这个建议都一样。如果我们有30%的时间违背自己的知识去选红色，那我们表现得就更差了——因为我们明知道红色的概率更低还去选。

如果你每轮都选红色，你会得到最差的结果，100%愚蠢。如果你有30%的时间选红色（面对30%的红牌），那你就是30%愚蠢。

当你的知识不完全——也就是说，世界对你来说有随机性——让自己的行为变得随机并不能解决问题。让自己的行为随机只会让你离目标更远。在本就模糊的世界里，丢掉理性只会让情况更糟。

反直觉的是，即使在不确定条件下，最优策略依然是“有规律地思考”。

所以理性主义者才如此稀少，因为大多数人在感受到世界混乱时，会用混乱对抗混乱。你必须多走一步，想出一个不会自发浮现的思路，才能想到用水灭火而不是用火攻火。

你可能听过一些未开化者说：“理性只适用于理性的人，但世界并不理性。”但面对非理性的对手，丢掉自己的理性并不会帮到你。即使对手不遵守理性法则，理性的思考方式依然能给出最优解。决策理论不会因为对手不守规则就崩溃。

这其实和面对蓝红牌时全选蓝色一样不直观。但每次你选红色，都是期望损失；每次你偏离理性之道，都是如此。

有多少《星际迷航》的剧情因此被推翻？有多少人工智能理论因此站不住脚？

---

<sup>1</sup>Tversky and Edwards, “Information versus Reward in Binary Choices,” 1966。另见 Schul and Mayo, “Searching for Certainty in an Uncertain World,”

---

## Lawful Uncertainty

In Rational Choice in an Uncertain World, Robyn Dawes describes an experiment by Tversky:<sup>1</sup>

> Many psychological experiments were conducted in the late 1950s and early 1960s in which subjects were asked to predict the outcome of an event that had a random component but yet had base-rate predictability—for example, subjects were asked to predict whether the next card the experimenter turned over would be red or blue in a context in which 70% of the cards were blue, but in which the sequence of red and blue cards was totally random.
> In such a situation, the strategy that will yield the highest proportion of success is to predict the more common event. For example, if 70% of the cards are blue, then predicting blue on every trial yields a 70% success rate.
> What subjects tended to do instead, however, was match probabilities—that is, predict the more probable event with the relative frequency with which it occurred. For example, subjects tended to predict 70% of the time that the blue card would occur and 30% of the time that the red card would occur. Such a strategy yields a 58% success rate, because the subjects are correct 70% of the time when the blue card occurs (which happens with probability .70) and 30% of the time when the red card occurs (which happens with probability .30); (.70× .70) + (.30× .30)= .58.
> In fact, subjects predict the more frequent event with a slightly higher probability than that with which it occurs, but do not come close to predicting its occurrence 100% of the time, even when they are paid for the accuracy of their predictions . . . For example, subjects who were paid a nickel for each correct prediction over a thousand trials . . . predicted [the more common event] 76% of the time.

Do not think that this experiment is about a minor flaw in gambling strategies. It compactly illustrates the most important idea in all of rationality.

Subjects just keep guessing red, as if they think they have some way of predicting the random sequence. Of this experiment Dawes goes on to say, “Despite feedback through a thousand trials, subjects cannot bring themselves to believe that the situation is one in which they cannot predict.”

But the error must go deeper than that. Even if subjects think they’ve come up with a hypothesis, they don’t have to actually bet on that prediction in order to test their hypothesis. They can say, “Now if this hypothesis is correct, the next card will be red”—and then just bet on blue. They can pick blue each time, accumulating as many nickels as they can, while mentally noting their private guesses for any patterns they thought they spotted. If their predictions come out right, then they can switch to the newly discovered sequence.

I wouldn’t fault a subject for continuing to invent hypotheses—how could they know the sequence is truly beyond their ability to predict? But I would fault a subject for betting on the guesses, when this wasn’t necessary to gather information, and literally hundreds of earlier guesses had been disconfirmed.

Can even a human be that overconfident?

I would suspect that something simpler is going on—that the all-blue strategy just didn’t occur to the subjects.

People see a mix of mostly blue cards with some red, and suppose that the optimal betting strategy must be a mix of mostly blue cards with some red.

It is a counterintuitive idea that, given incomplete information, the optimal betting strategy does not resemble a typical sequence of cards.

It is a counterintuitive idea that the optimal strategy is to behave lawfully, even in an environment that has random elements.

It seems like your behavior ought to be unpredictable, just like the environment—but no! A random key does not open a random lock just because they are “both random.”

You don’t fight fire with fire; you fight fire with water. But this thought involves an extra step, a new concept not directly activated by the problem statement, and so it’s not the first idea that comes to mind.

In the dilemma of the blue and red cards, our partial knowledge tells us—on each and every round—that the best bet is blue. This advice of our partial knowledge is the same on every single round. If 30% of the time we go against our partial knowledge and bet on red instead, then we will do worse thereby—because now we’re being outright stupid, betting on what we know is the less probable outcome.

If you bet on red every round, you would do as badly as you could possibly do; you would be 100% stupid. If you bet on red 30% of the time, faced with 30% red cards, then you’re making yourself 30% stupid.

When your knowledge is incomplete—meaning that the world will seem to you to have an element of randomness—randomizing your actions doesn’t solve the problem. Randomizing your actions takes you further from the target, not closer. In a world already foggy, throwing away your intelligence just makes things worse.

It is a counterintuitive idea that the optimal strategy can be to think lawfully, even under conditions of uncertainty.

And so there are not many rationalists, for most who perceive a chaotic world will try to fight chaos with chaos. You have to take an extra step, and think of something that doesn’t pop right into your mind, in order to imagine fighting fire with something that is not itself fire.

You have heard the unenlightened ones say, “Rationality works fine for dealing with rational people, but the world isn’t rational.” But faced with an irrational opponent, throwing away your own reason is not going to help you. There are lawful forms of thought that still generate the best response, even when faced with an opponent who breaks those laws. Decision theory does not burst into flames and die when faced with an opponent who disobeys decision theory.

This is no more obvious than the idea of betting all blue, faced with a sequence of both blue and red cards. But each bet that you make on red is an expected loss, and so too with every departure from the Way in your own thinking.

How many Star Trek episodes are thus refuted? How many theories of AI?

---

<sup>1</sup>Tversky and Edwards, “Information versus Reward in Binary Choices,” 1966. See also Schul and Mayo, “Searching for Certainty in an Uncertain World,” 2003.
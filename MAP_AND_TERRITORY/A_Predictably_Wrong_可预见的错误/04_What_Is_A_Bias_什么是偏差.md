## 什么是偏差？

可得性启发是一种人类用来得出结论的认知捷径；当这种捷径经常导致不准确的结论时，我们可以说“可得性偏差”在起作用。范围不敏感也是认知偏差的一个例子。

“认知偏差”指的是那些阻碍我们获得真理的障碍，这些障碍并非由信息成本高昂或计算能力有限造成，而是源自我们自身心理机制的结构。例如，我们的心理过程可能在进化上被适应为去相信某些并不真实的事情，这样我们在部落环境中就能赢得政治争论。或者，心理机制可能被适应为对某些事情是否真实并不在意，比如我们为了融入社会而倾向于相信他人所信的内容。又或者，偏差可能是某种有用推理捷径的副作用。可得性启发本身并不是一种偏差，但它会引发偏差；大脑采用的算法是“那些更容易浮现在脑海的事情赋予更高的证据权重”，这种算法有时能带来良好的认知效果，但也会导致系统性错误。

当我们的头脑做错了什么，经过大量实验和/或深入思考后，有人用语言明确地指出了这个问题，我们就称之为“（认知）偏差”。这与日常用语中的“那个人有偏见”不同，后者只是指“那个人对某事持有偏颇或有成见的态度”。

在认知科学中，“偏差”与因认知内容产生的错误（如学到的错误信念）是有区别的。后者我们称为“错误”而不是“偏差”，一旦我们自己发现了这些错误，通常更容易纠正。（当然，错误的根源，或者错误的根源的根源，最终也可能是某种偏差。）

“偏差”也不同于因个体大脑损伤或吸收的文化习俗而产生的错误；偏差源自于人类普遍共有的心理机制。

柏拉图并不是因为不知道广义相对论而“有偏差”——他没有办法获得那方面的信息，他的无知并不是由他心理机制的结构导致的。但如果柏拉图认为哲学家会成为更好的国王，仅仅因为他自己就是哲学家——而这种信念又源自于人类普遍适应的自我推销的政治本能，而不是因为柏拉图的父亲告诉他每个人都有道德义务让自己的职业去治理国家，或者因为柏拉图小时候吸了太多胶水——那么这就是一种偏差，无论柏拉图是否被提醒过这一点。

虽然我并不反对（如你所见）讨论定义，但我并不想暗示：更好地驾驭我们头脑的事业取决于某个特定术语的选择。如果“认知偏差”这个词没什么帮助，我们大可以不用它。

我们并不是因为偏差“坏、邪恶、绝对不能有”，才一开始就有“减少偏差”的道德义务。这种想法可能出现在那些通过社会渗透获得“理性”义务的人身上，导致他们在不理解理由的情况下，机械地执行某些技巧。（正如我小时候读的《费曼先生，您开玩笑吧》所说，这种做法本身就是“坏、邪恶、绝对不能有”。）偏差是我们获得真理目标道路上的障碍，因此它是我们需要克服的东西。

我们来到这里，是为了追寻人类伟大的求真之旅：因为我们迫切需要知识，而且我们也充满好奇。为此，让我们努力克服一切挡在我们面前的障碍，无论我们是否称它们为“偏差”。

---

## What’s a Bias?

The availability heuristic is a cognitive shortcut humans use to reach conclusions; and where this shortcut reliably causes inaccurate conclusions, we can say that an availability bias is at work. Scope insensitivity is another example of a cognitive bias.

“Cognitive biases” are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by the shape of our own mental machinery. For example, our mental processes might be evolutionarily adapted to specifically believe some things that aren’t true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.

Our brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a “(cognitive) bias.” Not to be confused with the colloquial “that person is biased,” which just means “that person has a skewed or prejudiced attitude toward something.”

In cognitive science, “biases” are distinguished from errors that arise from cognitive content, such as learned false beliefs. These we call “mistakes” rather than “biases,” and they are much easier to correct, once we’ve noticed them for ourselves. (Though the source of the mistake, or the source of the source of the mistake, may ultimately be some bias.)

“Biases” are also distinguished from errors stemming from damage to an individual human brain, or from absorbed cultural mores; biases arise from machinery that is humanly universal.

Plato wasn’t “biased” because he was ignorant of General Relativity—he had no way to gather that information, his ignorance did not arise from the shape of his mental machinery. But if Plato believed that philosophers would make better kings because he himself was a philosopher—and this belief, in turn, arose because of a universal adaptive political instinct for self-promotion, and not because Plato’s daddy told him that everyone has a moral duty to promote their own profession to governorship, or because Plato sniffed too much glue as a kid—then that was a bias, whether Plato was ever warned of it or not.

While I am not averse (as you can see) to discussing definitions, I don’t want to suggest that the project of better wielding our own minds rests on a particular choice of terminology. If the term “cognitive bias” turns out to be unhelpful, we should just drop it.

We don’t start out with a moral duty to “reduce bias,” simply because biases are bad and evil and Just Not Done. This is the sort of thinking someone might end up with if they acquired a deontological duty of “rationality” by social osmosis, which leads to people trying to execute techniques without appreciating the reason for them. (Which is bad and evil and Just Not Done, according to Surely You’re Joking, Mr. Feynman, which I read as a kid.) A bias is an obstacle to our goal of obtaining truth, and thus in our way.

We are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, we’re curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them “biases” or not.
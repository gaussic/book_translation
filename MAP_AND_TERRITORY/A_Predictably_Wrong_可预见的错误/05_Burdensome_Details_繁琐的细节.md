## 繁琐的细节

        只是一些辅助性的细节，旨在为本来单调且难以置信的叙述增添艺术上的逼真感……
                                                     ——普巴，《吉尔伯特与沙利文：御用大臣》

结合谬误是指：人们会把“甲且乙”这种命题的概率评估得比单独“甲”或“乙”还高，尽管在概率论中，联合事件的概率永远不会大于其任一部分。例如，在一项实验中，68%的受试者认为“里根将为未婚母亲提供联邦资助并削减对地方政府的联邦支持”比“里根将为未婚母亲提供联邦资助”更有可能发生。<sup>1</sup>

一系列巧妙设计的实验，排除了其他假设，确立了标准解释，证实了结合谬误的发生是因为我们“用代表性判断代替了概率判断”。<sup>2</sup> 通过添加额外细节，你可以让某个结果看起来更符合生成它的过程。比如，给“里根会支持未婚母亲”这个说法再加上“里根还会削减对地方政府的支持”，就让整个情景听起来更可信。一项不太可信的主张，会被另一项较可信的主张“平均”掉。

也就是说：添加细节可以让一个情景**听起来更可信**，但实际上事件的概率**必然变得更低**。

如果真是这样，那么，假设来说，我们可能会发现未来学家编造出极其详尽且貌似合理的未来历史，或者有人会把一大堆未经证实的主张与几个听起来很有力的断言捆绑在一起，照单全收。

如果你在一个赤裸裸、直接对比的情境下遇到结合谬误，你也许能通过有意识地纠正自己，在那道题上答对。但这只是头痛医头、脚痛医脚，并不能从根本上解决问题。

在1982年的一项实验中，专业预测者系统性地认为“俄罗斯入侵波兰，随后美苏断交”比“美苏断交”更有可能。<sup>3</sup> 每组实验对象只看到其中一个命题。作为一个整体，这些预测者能采取什么策略来避免结合谬误？当没有任何个人直接知道对比内容，甚至没人知道实验的主题是结合谬误时，他们又该如何做出更好的概率判断？

只是在某个特殊情境下打补丁，并不能解决普遍性的问题。那些“坑”只是症状，不是病因。

那么，预测者们如何在没有直接对比、甚至不知道自己会被考察结合谬误的情况下，避免这种谬误呢？在我看来，他们需要注意到“和（and）”这个词。他们不仅要警惕，甚至要本能地避开它。即使不知道研究者之后会专门考察结合谬误，他们也需要对两个完整细节的结合保持警觉，对有人让他们认可如此荒谬复杂的预测感到震惊。他们还需要大幅度地降低概率——根据实验细节，至少要降低四倍。

预测者们如果能思考一下美苏断交可能的各种原因，也许也会有帮助。情景不是“美苏无缘无故突然断交”，而是“美苏因任何原因断交”。

对于那些认为“里根会为未婚母亲提供联邦资助并削减对地方政府支持”更可能的受试者来说，他们同样需要对“和（and）”这个词感到震惊。此外，他们需要把荒谬性相加（荒谬性就是对数概率，可以直接相加），而不是平均。他们应该想：“里根可能会不会削减对地方政府的支持（1比特），但他支持未婚母亲的可能性极低（4比特）。总荒谬性：5比特。”或者，“里根不会支持未婚母亲，一票否决，另一个主张只会让情况更糟。”

类似地，看看特沃斯基和卡尼曼（1983）基于一个六面骰子的实验，骰子有四面是绿色，两面是红色。<sup>4</sup> 受试者要在以下三种序列中下注，哪种会在二十次掷骰中出现：（1）RGRRR，（2）GRGRRR，（3）GRRRRR。65%的受试者选择了GRGRRR，但实际上任何包含GRGRRR的序列也必然包含RGRRR，因此RGRRR严格优于GRGRRR。受试者本可以怎么做得更好？也许是注意到包含关系；但这也只是权宜之计，不能解决根本问题。如果能明确计算概率，当然能解决根本问题，但你并不总能算出精确概率。

受试者用启发式方法失败了，因为他们想：“啊哈！序列2的绿/红比例最高！我应该押2！”要用启发式方法获胜，他们应该想：“啊哈！序列1更短！我应该选1！”

他们需要更强烈地感受到奥卡姆剃刀的情感冲击——每增加一个细节，哪怕只是多掷一次骰子，都要觉得是负担。

有一次，我和一个被某位不够谨慎的未来学家（喜欢加很多听起来很酷的细节）迷住的人聊天。我试图解释，为什么我不会被这些惊人、不可思议的理论迷住。于是我讲了结合谬误，特别是“断交±入侵波兰”实验。他说：“好吧，但这和——”我说：“宇宙因任何原因复制的概率，总是大于‘宇宙通过黑洞复制，因为高级文明制造黑洞，因为宇宙进化出让他们这么做的机制’的概率。”他答：“哦。”

在那之前，他并没有把这些额外细节当作额外负担。相反，这些细节成了辅助性细节，让叙述更逼真。有人给你一堆奇怪的想法，其中之一是宇宙会自我复制。然后他们为“宇宙会自我复制”这个主张提供支持。但这并不是对整个想法包裹的支持，尽管它们被讲成了一个故事。

你必须把细节拆开。你要把每个细节单独拎出来，问一句：“我们怎么知道这个细节？”有人描绘了人类陷入纳米技术战争的图景，说中国拒绝遵守国际管控协议，接着爆发军备竞赛……等一下——你怎么知道一定是中国？你兜里有水晶球，还是只是喜欢当未来学家？这些细节都从哪儿来的？那个具体细节又是怎么来的？

    正如所言：
        如果你能减轻负担，就必须这么做。
        没有哪根稻草不能压垮你的脊背。

---

<sup>1</sup>Tversky and Kahneman, “Judgments of and by Representativeness,” 1982.

<sup>2</sup>See Tversky and Kahneman, “Extensional Versus Intuitive Reasoning,” 1983 and Kahneman
and Frederick, “Representativeness Revisited: Attribute Substitution in Intuitive Judgment,”
2002 for more information.

<sup>3</sup>Tversky and Kahneman, “Extensional Versus Intuitive Reasoning,” 1983.

---

## Burdensome Details

        Merely corroborative detail, intended to give artistic verisimilitude to an otherwise bald and unconvincing narrative . . .
                                                     —Pooh-Bah, in Gilbert and Sullivan’s The Mikado

The conjunction fallacy is when humans assign a higher probability to a proposition of the form “A and B” than to one of the propositions “A” or “B” in isolation, even though it is a theorem that conjunctions are never likelier than their conjuncts. For example, in one experiment, 68% of the subjects ranked it more likely that “Reagan will provide federal support for unwed mothers and cut federal support to local governments” than that “Reagan will provide federal support for unwed mothers.”<sup>1</sup>

A long series of cleverly designed experiments, which weeded out alternative hypotheses and nailed down the standard interpretation, confirmed that conjunction fallacy occurs because we “substitute judgment of representativeness for judgment of probability.”<sup>2</sup> By adding extra details, you can make an outcome seem more characteristic of the process that generates it. You can make it sound more plausible that Reagan will support unwed mothers, by adding the claim that Reagan will also cut support to local governments. The implausibility of one claim is compensated by the plausibility of the other; they “average out.”

Which is to say: Adding detail can make a scenario SOUND MORE PLAUSIBLE, even though the event necessarily BECOMES LESS PROBABLE.

If so, then, hypothetically speaking, we might find futurists spinning unconscionably plausible and detailed future histories, or find people swallowing huge packages of unsupported claims bundled with a few strongsounding assertions at the center.

If you are presented with the conjunction fallacy in a naked, direct comparison, then you may succeed on that particular problem by consciously correcting yourself. But this is only slapping a band-aid on the problem, not fixing it in general.

In the 1982 experiment where professional forecasters assigned systematically higher probabilities to “Russia invades Poland, followed by suspension of diplomatic relations between the USA and the USSR” than to “Suspension of diplomatic relations between the USA and the USSR,” each experimental group was only presented with one proposition.<sup>3</sup> What strategy could these forecasters have followed, as a group, that would have eliminated the conjunction fallacy, when no individual knew directly about the comparison? When no individual even knew that the experiment was about the conjunction fallacy? How could they have done better on their probability judgments?

Patching one gotcha as a special case doesn’t fix the general problem. The gotcha is the symptom, not the disease.

What could the forecasters have done to avoid the conjunction fallacy, without seeing the direct comparison, or even knowing that anyone was going to test them on the conjunction fallacy? It seems to me, that they would need to notice the word “and.” They would need to be wary of it—not just wary, but leap back from it. Even without knowing that researchers were afterward going to test them on the conjunction fallacy particularly. They would need to notice the conjunction of two entire details, and be shocked by the audacity of anyone asking them to endorse such an insanely complicated prediction. And they would need to penalize the probability substantially—a factor of four, at least, according to the experimental details.

It might also have helped the forecasters to think about possible reasons why the US and Soviet Union would suspend diplomatic relations. The scenario is not “The US and Soviet Union suddenly suspend diplomatic relations for no reason,” but “The US and Soviet Union suspend diplomatic relations for any reason.”

And the subjects who rated “Reagan will provide federal support for unwed mothers and cut federal support to local governments”? Again, they would need to be shocked by the word “and.” Moreover, they would need to add absurdities—where the absurdity is the log probability, so you can add it—rather than averaging them. They would need to think, “Reagan might or might not cut support to local governments (1 bit), but it seems very unlikely that he will support unwed mothers (4 bits). Total absurdity: 5 bits.” Or maybe, “Reagan won’t support unwed mothers. One strike and it’s out. The other proposition just makes it even worse.”

Similarly, consider Tversky and Kahneman’s (1983) experiment based around a six-sided dice with four green faces and two red faces.4 The subjects had to bet on the sequence (1) RGRRR, (2) GRGRRR, or (3) GRRRRR appearing anywhere in twenty rolls of the dice. Sixty-five percent of the subjects chose GRGRRR, which is strictly dominated by RGRRR, since any sequence containing GRGRRR also pays off for RGRRR. How could the subjects have done better? By noticing the inclusion? Perhaps; but that is only a bandaid, it does not fix the fundamental problem. By explicitly calculating the probabilities? That would certainly fix the fundamental problem, but you can’t always calculate an exact probability.

The subjects lost heuristically by thinking: “Aha! Sequence 2 has the highest proportion of green to red! I should bet on Sequence 2!” To win heuristically, the subjects would need to think: “Aha! Sequence 1 is short! I should go with Sequence 1!”

They would need to feel a stronger emotional impact from Occam’s Razor—feel every added detail as a burden, even a single extra roll of the dice.

Once upon a time, I was speaking to someone who had been mesmerized by an incautious futurist (one who adds on lots of details that sound neat). I was trying to explain why I was not likewise mesmerized by these amazing, incredible theories. So I explained about the conjunction fallacy, specifically the “suspending relations ± invading Poland” experiment. And he said, “Okay, but what does this have to do with—” And I said, “It is more probable that universes replicate for any reason, than that they replicate via black holes because advanced civilizations manufacture black holes because universes evolve to make them do it.” And he said, “Oh.”

Until then, he had not felt these extra details as extra burdens. Instead they were corroborative detail, lending verisimilitude to the narrative. Someone presents you with a package of strange ideas, one of which is that universes replicate. Then they present support for the assertion that universes replicate. But this is not support for the package, though it is all told as one story.

You have to disentangle the details. You have to hold up every one independently, and ask, “How do we know this detail?” Someone sketches out a picture of humanity’s descent into nanotechnological warfare, where China refuses to abide by an international control agreement, followed by an arms race . . . Wait a minute—how do you know it will be China? Is that a crystal ball in your pocket or are you just happy to be a futurist? Where are all these details coming from? Where did that specific detail come from?

    For it is written:
        If you can lighten your burden you must do so.
        There is no straw that lacks the power to break your back.

---

<sup>1</sup>Tversky and Kahneman, “Judgments of and by Representativeness,” 1982.

<sup>2</sup>See Tversky and Kahneman, “Extensional Versus Intuitive Reasoning,” 1983 and Kahneman
and Frederick, “Representativeness Revisited: Attribute Substitution in Intuitive Judgment,”
2002 for more information.

<sup>3</sup>Tversky and Kahneman, “Extensional Versus Intuitive Reasoning,” 1983.
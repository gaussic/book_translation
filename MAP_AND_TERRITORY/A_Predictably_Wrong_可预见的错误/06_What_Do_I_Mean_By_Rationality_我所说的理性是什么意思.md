## 我所说的“理性”是什么意思？

我指的是两件事：
1. 认知理性：系统性地提升你信念的准确性。
2. 工具理性：系统性地实现你的价值目标。

第一个概念很简单。当你睁开眼睛环顾房间时，你会知道你的笔记本电脑在桌子旁，书架靠着墙。如果你的眼睛或大脑出了问题，你的心理模型可能会让你以为某处有书架，实际上那里什么都没有，当你走过去取书时就会失望。

这就是持有错误信念的感觉——你的世界地图与真实领地不符。认知理性就是要建立准确的地图。这种信念与现实的对应关系通常被称为“真理”，我也乐于这样称呼它。另一方面，工具理性则是驾驭现实——让未来朝你希望的方向发展。它是一种选择行动的艺术，使结果在你的偏好排序中更高。我有时把这称为“获胜”。

所以，理性就是形成真实的信念，并做出有助于你“获胜”的决策。（这里的“真理”并不等于“确定无疑”，因为即使我们不确定，也可以做很多事来提升信念的准确概率；而“获胜”也不是“以他人为代价的胜利”，因为我们的价值观包含了我们关心的一切，包括他人。）

当人们说“X 是理性的！”时，通常只是更强烈地表达“我认为 X 是真的”或“我认为 X 是好的”。那么，为什么除了“真实”和“善”之外，还要有“理性”这个词呢？

类似的论证也可以用来反对“真实”这个词。你没必要说“雪是白色的这件事是真的”，直接说“雪是白的”就行了。“真理”这个概念的用处在于，它让我们能够讨论地图与领地对应的一般特征。“真实的模型通常比虚假的模型能做出更好的实验预测”是一个有用的总结，而不用“真实”或“准确”这样的概念，你无法表达这个意思。

同样，“理性代理人会做出最大化一致效用函数概率期望的决策”这种说法，依赖于（工具）理性的概念；而“吃蔬菜是理性的”其实可以换成“吃蔬菜有用”或“吃蔬菜符合你的利益”。我们需要“理性”这样的概念，来总结那些系统性地产生真理或价值的思维方式的普遍规律——以及我们偏离这些标准的系统性方式。

正如我们在前面的文章中看到的，实验心理学家有时会发现人类推理非常奇怪。例如，有人会认为“比尔会演奏爵士乐”的概率小于“比尔是一名会演奏爵士乐的会计师”的概率。这似乎很奇怪，因为任何一位会演奏爵士乐的会计师显然也是会演奏爵士乐的人。但我们凭什么说这种判断是错的呢？

实验心理学家用两大金标准：概率论和决策理论。

概率论是一套支撑理性信念的法则。概率的数学原理同样适用于“弄清楚你的书架在哪里”以及“估算凯撒大帝头上有多少根头发”，尽管我们对“凯撒大帝秃头”这个说法的证据可能比“我房间里有书架”复杂和间接得多。这归根结底都是如何处理证据和观察、更新信念的问题。同样，决策理论是一套支撑理性行动的法则，无论你的目标和可选项是什么，它都适用。

用“P(某事)”表示“某事发生的概率”，“P(A, B)”表示“A 和 B 同时发生的概率”。由于概率论的普遍法则是 P(A) ≥ P(A, B)，所以如果有人判断 P(比尔会演奏爵士乐) 小于 P(比尔会演奏爵士乐且是会计师)，那就是错误的。

严格来说，这种概率判断是非贝叶斯式的。符合一致概率分布的信念，以及最大化一致效用函数概率期望的决策，被称为“贝叶斯式”的。

我需要强调，这并不是流行文化中常见的“理性”概念。人们可能用同样的发音“ra-tio-nal”来指“像《星际迷航》里的斯波克那样行动”或“像贝叶斯主义者那样行动”；但这并不意味着像斯波克那样就能在认知理性或工具理性上更进一步。<sup>2</sup>

这些还不能完全涵盖“理性”在实践中的含义，主要有两个原因：

首先，完整形式的贝叶斯公式在大多数现实问题上计算量太大，无法实际操作。没人能真的算出所有数学，就像没人能通过计算夸克的运动来预测股市一样。

这也是为什么有一个叫“Less Wrong”的网站，而不是一页纸写下公理就完事了。要在人的头脑中发现真理、实现价值，还有一整套更深的艺术：我们必须学会认识自己的缺陷，克服偏见，防止自欺，调整好情绪去面对真相并采取必要行动，等等。

其次，有时连数学本身的意义也会受到质疑。例如，在观察者数量不确定的人择问题中，概率论的精确规则会被质疑；在其他智能体可能提前预测你决策的新康布问题中，决策理论的精确规则也会被质疑。<sup>3</sup>

当我们最好的形式化方法仍然不够用时，我们可以回到“真理”和“获胜”这些更简单的想法。如果你是刚开始研究火的科学家，直接指着篝火说“火就是那团橙色发亮的热东西”可能比说“我把火定义为释放燃素的炼金转化过程”要明智得多。你当然不该因为无法定义某物就忽略它。我背不出广义相对论的方程，但如果我从悬崖上跳下去，照样会摔下去。对于认知偏差和其他真理障碍也是如此——即使我们无法简明地定义“非理性”，它们的伤害也不会减少。

在这种情况下，试图通过给“理性”下个新定义，然后说“所以我喜欢的答案就是‘理性’的含义”，是徒劳的。这只会引出一个问题：为什么别人要在意你的定义。我对概率论感兴趣，不是因为它是拉普拉斯传下来的圣言，而是因为我相信贝叶斯式的信念更新（配合奥卡姆先验）能让我们系统性地更接近准确——即地图更贴合领地。

还有一些思考方式，似乎概率论和决策理论都无法完全回答——比如当你获得真相后该如何面对它。在这里，试图用某种方式定义“理性”并不能给出答案，只是预设了一个答案。

我在这里不是为了争论一个词的含义，即使这个词是“理性”。给特定概念贴上字母序列，是为了让两个人能交流——帮助思想从一个大脑传递到另一个大脑。你无法通过操纵词语的含义来改变现实或证明观点。

所以，只要你大致明白我用“理性”这个词，以及“认知理性”“工具理性”这些子概念时想表达什么，我们就已经完成了关于“理性”定义的全部交流。剩下要讨论的，不是“ra-tio-na-li-ty”这几个音节该指什么，而是：什么才是好的思考方式。

如果你说：“相信 X 对我来说是（认知上）理性的，但真相其实是 Y”，那你很可能用“理性”表达的意思和我不一样。（比如，“理性”应该在反思下保持一致——“理性地”看证据和“理性地”思考大脑如何处理证据，不应得出两个不同结论。）

同样，如果你发现自己说：“对我来说，（工具上）理性的做法是 X，但正确的做法是 Y”，那你几乎肯定用“理性”或“正确”表达的意思和我不同。我用“理性”这个词，是规范性的，用来指代值得追求的思维模式。

在这种情况下——或者在任何其他人们对词义有分歧的情况下——你都应该用更具体的语言替代“理性”：比如“对自己有利的做法是逃跑，但我希望至少会试着把孩子从铁轨上拉开”，或者“因果决策理论通常认为你应该在纽康姆问题中选两个盒子，但我更想要一百万美元。”

事实上，我建议你回头把这篇文章里所有的“理性”都替换成“foozal”，看看这是否改变了我的表达。如果有，我会说：追求的不是理性，而是 foozality。

“理性”这个词有潜在的陷阱，但在很多非边界情形下，它完全可以传达我的意思。“非理性”也是如此。在这些情况下，我并不介意使用它。

但我们还是要小心不要滥用这个词。光是大声说出来，并不会加分。如果你过于谈论“道”，你就无法得道。

---

- <sup>1</sup>关于真理的更详细讨论，见本卷最后的《简单的真理》一文。  
- <sup>2</sup>认为“理性”就是严格优先于情感的语言推理，这正是一个典型的误区。贝叶斯理性同样适用于冲动、直觉、感知和无言的本能，而不仅仅是断言。我举过一个例子：睁开眼睛环顾四周，并在脑海中建立一个房间的模型，墙边有书架。现代理性的概念足够广泛，能够把你的眼睛和大脑的视觉区域都视为“制图工具”，并把本能和情感纳入信念与目标的计算之中。
- <sup>3</sup>关于纽康姆问题的非正式表述，见 Holt, “Thinking Inside the Boxes,” 2002, http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html。

---

## What Do I Mean By “Rationality”?

I mean two things:
    1. Epistemic rationality: systematically improving the accuracy of your beliefs.
    2. Instrumental rationality: systematically achieving your values.

The first concept is simple enough. When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed.

This is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called “truth,” and I’m happy to call it that.1 Instrumental rationality, on the other hand, is about steering reality— sending the future where you want it to go. It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.”

So rationality is about forming true beliefs and making decisions that help you win. (Where truth doesn’t mean “certainty,” since we can do plenty to increase the probability that our beliefs are accurate even though we’re uncertain; and winning doesn’t mean “winning at others’ expense,” since our values include everything we care about, including other people.)

When people say “X is rational!” it’s usually just a more strident way of saying “I think X is true” or “I think X is good.” So why have an additional word for “rational” as well as “true” and “good”?

An analogous argument can be given against using “true.” There is no need to say “it is true that snow is white” when you could just say “snow is white.” What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. “True models usually produce better experimental predictions than false models” is a useful generalization, and it’s not one you can make without using a concept like “true” or “accurate.”

Similarly, “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function” is the kind of thought that depends on a concept of (instrumental) rationality, whereas “It’s rational to eat vegetables” can probably be replaced with “It’s useful to eat vegetables” or “It’s in your interest to eat vegetables.” We need a concept like “rational” in order to note general facts about those ways of thinking that systematically produce truth or value—and the systematic ways in which we fall short of those standards.

As we’ve observed in the previous essays, experimental psychologists sometimes uncover human reasoning that seems very strange. For example, someone rates the probability “Bill plays jazz” as less than the probability “Bill is an accountant who plays jazz.” This seems like an odd judgment, since any particular jazz-playing accountant is obviously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is wrong?

Experimental psychologists use two gold standards: probability theory, and decision theory.

Probability theory is the set of laws underlying rational belief. The mathematics of probability applies equally to “figuring out where your bookcase is” and “estimating how many hairs were on Julius Caesar’s head,” even though our evidence for the claim “Julius Caesar was bald” is likely to be more complicated and indirect than our evidence for the claim “there’s a bookcase in my room.” It’s all the same problem of how to process the evidence and observations to update one’s beliefs. Similarly, decision theory is the set of laws underlying rational action, and is equally applicable regardless of what one’s goals and available options are.

Let “P(such-and-such)” stand for “the probability that such-andsuch happens,” and “P(A, B)” for “the probability that both A and B happen.” Since it is a universal law of probability theory that P(A) ≥ P(A, B), the judgment that P(Bill plays jazz) is less than P(Bill plays jazz, Bill is an accountant) is labeled incorrect.

To keep it technical, you would say that this probability judgment is non-Bayesian. Beliefs that conform to a coherent probability distribution, and decisions that maximize the probabilistic expectation of a coherent utility function, are called “Bayesian.”

I should emphasize that this isn’t the notion of rationality that’s common in popular culture. People may use the same string of sounds, “ra-tio-nal,” to refer to “acting like Mr. Spock of Star Trek” and “acting like a Bayesian”; but this doesn’t mean that acting Spock-like helps one hair with epistemic or instrumental rationality.2

All of this does not quite exhaust the problem of what is meant in practice by “rationality,” for two major reasons:

First, the Bayesian formalisms in their full form are computationally intractable on most real-world problems. No one can actually calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.

This is why there is a whole site called “Less Wrong,” rather than a single page that simply states the formal axioms and calls it a day. There’s a whole further art to finding the truth and accomplishing value from inside a human mind: we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera.

Second, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g., anthropic problems in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens.3

In cases where our best formalizations still come up short, we can return to simpler ideas like “truth” and “winning.” If you are a scientist just beginning to investigate fire, it might be a lot wiser to point to a campfire and say “Fire is that orangey-bright hot stuff over there,” rather than saying “I define fire as an alchemical transmutation of substances which releases phlogiston.” You certainly shouldn’t ignore something just because you can’t define it. I can’t quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I’ll fall. And we can say the same of cognitive biases and other obstacles to truth—they won’t hit any less hard if it turns out we can’t define compactly what “irrationality” is.

In cases like these, it is futile to try to settle the problem by coming up with some new definition of the word “rational” and saying, “Therefore my preferred answer, by definition, is what is meant by the word ‘rational.’ ” This simply raises the question of why anyone should pay attention to your definition. I’m not interested in probability theory because it is the holy word handed down from Laplace. I’m interested in Bayesian-style beliefupdating (with Occam priors) because I expect that this style of thinking gets us systematically closer to, you know, accuracy, the map that reflects the territory.

And then there are questions of how to think that seem not quite answered by either probability theory or decision theory—like the question of how to feel about the truth once you have it. Here, again, trying to define “rationality” a particular way doesn’t support an answer, but merely presumes one.

I am not here to argue the meaning of a word, not even if that word is “rationality.” The point of attaching sequences of letters to particular concepts is to let two people communicate—to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words.

So if you understand what concept I am generally getting at with this word “rationality,” and with the sub-terms “epistemic rationality” and “instrumental rationality,” we have communicated: we have accomplished everything there is to accomplish by talking about how to define “rationality.” What’s left to discuss is not what meaning to attach to the syllables “ra-tio-na-li-ty”; what’s left to discuss is what is a good way to think.

If you say, “It’s (epistemically) rational for me to believe X, but the truth is Y,” then you are probably using the word “rational” to mean something other than what I have in mind. (E.g., “rationality” should be consistent under reflection—“rationally” looking at the evidence, and “rationally” considering how your mind processes the evidence, shouldn’t lead to two different conclusions.)

Similarly, if you find yourself saying, “The (instrumentally) rational thing for me to do is X, but the right thing for me to do is Y,” then you are almost certainly using some other meaning for the word “rational” or the word “right.” I use the term “rationality” normatively, to pick out desirable patterns of thought.

In this case—or in any other case where people disagree about word meanings—you should substitute more specific language in place of “rational”: “The self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,” or “Causal decision theory as usually formulated says you should two-box on Newcomb’s Problem, but I’d rather have a million dollars.”

In fact, I recommend reading back through this essay, replacing every instance of “rational” with “foozal,” and seeing if that changes the connotations of what I’m saying any. If so, I say: strive not for rationality, but for foozality.

The word “rational” has potential pitfalls, but there are plenty of non-borderline cases where “rational” works fine to communicate what I’m getting at. Likewise “irrational.” In these cases I’m not afraid to use it.

Yet one should be careful not to overuse that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it.

---

- <sup>1</sup>For a longer discussion of truth, see “The Simple Truth” at the very end of this volume.
- <sup>2</sup>The idea that “rationality” is about strictly privileging verbal reasoning over feelings is a case in point. Bayesian rationality applies to urges, hunches, perceptions, and wordless intuitions, not just to assertions. I gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern idea of rationality is general enough to include your eyes and your brain’s visual areas as things-that-map, and to include instincts and emotions in the belief-and-goal calculus.
- <sup>3</sup>For an informal statement of Newcomb’s Problem, see Holt, “Thinking Inside the Boxes,” 2002, http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html.
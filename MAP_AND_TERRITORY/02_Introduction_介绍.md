## 介绍 

Rob Bensinger

想象你把手伸进一个装有七十个白球和三十个红球的罐子里，然后抽出十个神秘的球。

也许你抽出的十个球中有三个是红球，这样你就能正确猜出罐子里红球的总数。或者你碰巧抽到了四个红球，或者其他数量。那你很可能就会猜错罐子里红球的总数。

这种随机误差是知识不完整所带来的代价，而就误差而言，这其实并不算糟糕。你的估算平均来看不会出错，而且你了解得越多，误差通常就会越小。另一方面，假设白球更重，会沉到罐子的底部。那么你抽取的样本就可能会持续地出现偏差，无法代表整体。

这种误差被称为“统计偏差”。当你了解世界的方法存在偏差时，获取更多信息未必有帮助。收集更多数据甚至可能会让有偏的预测变得越来越糟。

如果你习惯于高度重视知识和探索，这种前景会让人感到害怕。如果我们想确保学习更多真的能帮助我们，而不是让我们变得比以前更糟，我们就需要发现并纠正数据中的偏差。

心理学中的“认知偏差”概念与此类似。认知偏差是一种我们思维方式中的系统性错误，不同于随机错误或仅仅由无知造成的错误。统计偏差会让样本与总体的相似度降低，而认知偏差则会让我们的思维对真相的把握变得不那么准确（或者说，不那么可靠地服务于我们的其他目标）。

也许你有乐观偏差，你发现红球可以用来治疗困扰你兄弟的一种罕见热带疾病，于是你因为希望罐子里的球大多是红色的，就高估了罐子里红球的数量。

和统计偏差一样，认知偏差也会扭曲我们对现实的看法，单纯收集更多数据并不总能修正它们，而且它们的影响会随着时间积累。但当你要修正的“校准错误的测量工具”正是你自己时，去偏差化就成了一项独特的挑战。

不过，这显然是一个最该从这里开始的问题。因为如果你都无法信任自己的大脑，又怎么能信任其他任何东西呢？

### 觉察偏差

想象你第一次见到某人，除了知道他们很害羞之外，对他们一无所知。

问题：你觉得这个人更有可能是图书管理员，还是推销员？

大多数人会回答“图书管理员”。但这其实是个错误：害羞的推销员远比害羞的图书管理员常见，因为总体上推销员在美国的人数是图书管理员的七十五倍。<sup>1</sup>

这就是“基率忽视”：人们在判断时，过于依赖特征之间的契合感，而忽略了这些特征在总体中出现的概率有多大。<sup>2</sup> 认知偏差的另一个例子是“沉没成本谬误”——人们倾向于对自己过去已经投入资源的事物产生执念，而实际上他们应该及时止损、重新开始。

不幸的是，了解这些偏差，并不能让你免疫于它们。甚至并不意味着你能在它们发生时察觉到它们的存在。

在一项关于“偏见盲区”的研究中，实验对象预测，如果他们知道某些画作出自著名艺术家之手，他们在中立评价这些画作质量时会更加困难。事实上，当实验者后来检验他们的预测时，这些受试者确实表现出了他们自己预期的偏见。然而，在事后被问及时，这些受试者却声称他们对画作的评价是客观的，没有受到偏见影响。<sup>3</sup>

即使我们能够正确识别他人的偏见，在面对自身缺陷时，我们依然会表现出“偏见盲区”。<sup>4</sup> 当我们内省时没有察觉到任何“带有偏见的想法”，我们就会得出结论，认为自己比其他人更少有偏见。<sup>5</sup>

然而，识别和克服偏差是可能的，只是这并不容易。例如，已有研究表明，通过将概率看作是某种对象或事件出现的频率，受试者可以减少基率忽视的发生。

本书采用的去偏差方法，是系统性地阐释良好推理为何有效，以及大脑在这方面存在哪些不足。只要本书能达成这一目标，其方法就可以与 Serfas（2010）中描述的方法相比较。Serfas 指出，“多年与金融相关的工作经验”并不会影响人们对沉没成本偏差的易感性，而“所修会计课程的数量”却有帮助。

因此，我们可能有必要区分“经验”和“专业知识”。这里的“专业知识”指的是“对问题有概念性理解，并形成系统性原理的能力”，而这反过来又能让决策者识别出特定的偏差。然而，将专业知识作为应对偏差的手段，不仅仅要求你熟悉具体情境或在某一领域是专家。它要求你能够充分理解相关偏差背后的原理，能够在特定情境中识别出偏差，并且手头有合适的工具来对抗这些偏差。<sup>6</sup>

本书的目标，是为建立理性“专业知识”打下基础。这意味着要深入理解一个极其普遍的问题的结构：人类的偏见、自我欺骗，以及复杂思维自我挫败的千百种路径。

### 关于本书的一点说明

《地图与领地》最初是一系列由决策理论家埃利泽·尤德科斯基撰写的随笔，这些随笔于2006年至2009年间发表在经济学博客 Overcoming Bias 及其衍生社区博客 Less Wrong 上。主题相关的文章被归为“系列”，而相关的系列又被归为书籍。《地图与领地》是六本此类书籍中的第一本，这一系列的总名为《理性：从人工智能到僵尸》。<sup>7</sup>

在风格上，这个系列涵盖了“生动的教科书”、“轶事集锦”到“激进宣言”等多种形式，内容也因此丰富多样。这本理性入门书常常带有个人色彩和不拘一格的风格——比如引用尤德科斯基与其正统犹太教母亲（精神科医生）和父亲（物理学家）的经历，以及聊天室和邮件列表中的讨论。熟悉尤德科斯基的读者，尤其是通过他那本以科学为主题、改编自J.K.罗琳《哈利·波特》的《哈利·波特与理性方法》认识他的人，会发现这里同样有着打破常规的精神和许多相似的主题。

哲学家阿尔弗雷德·科尔齐布斯基曾写道：“地图不是它所代表的领地，但如果它是正确的，它就与领地有相似的结构，这正是它有用的原因。”正如科尔齐布斯基所说，这里关于地图的话，同样适用于信念、断言和语言。

“地图不是领地。” 这个看似简单的命题，是本书的核心思想，也是本书所收录的四个系列文章的主线：第一部分“可预见的错误”，探讨我们的信念如何系统性地未能映射真实世界；第二部分“虚假信念”，讨论什么样的信念才算是一张“地图”；第三部分“觉察困惑”，分析我们大脑如何进行世界映射；第四部分“神秘的答案”，将这些观点汇聚碰撞。最后以“简单的真理”收尾，这是一篇关于真理本身的独立对话。

人类并不理性；但正如行为经济学家丹·艾瑞里所说，我们的非理性是有规律可循的。我们出错的方式有其模式，我们不出错时的行为也有其规律。两者都值得更深入的理解，而这种理解也许能帮助我们为自己创造一个更美好的未来。

### 致谢

我非常感谢 Nate Soares、Elizabeth Morningstar、Paul Crowley、Brienne Yudkowsky、Adam Freese、Helen Toner 以及数十位志愿者为本书部分内容所做的校对工作。
特别感谢 Alex Vermeer，他推动了本书原始版本的完成；感谢 Tsvi Benson-Tilsen，他细致审阅了《地图与领地》及《理性：从人工智能到僵尸》系列的其他卷，确保了它们的可读性和一致性；还要感谢 Chana Messinger，她的编辑让文本更加清晰和优美。

---

<sup>1</sup>Weiten，《心理学：主题与变迁（简明版）》，第八版，2010年。

<sup>2</sup>Heuer，《情报分析心理学》，1999年。  

<sup>3</sup>Hansen 等人，“人们在明知使用有偏策略后仍声称客观”，2014年。 

<sup>4</sup>Pronin、Lin 和 Ross，“偏见盲区：对自我与他人偏见的感知”，2002年。  

<sup>5</sup>Ehrlinger、Gilovich 和 Ross，“窥视偏见盲区：人们对自身与他人偏见的评估”，2005年。

<sup>6</sup>Serfas，《资本投资情境中的认知偏差：规范理性违背的理论考察与实证实验》，2010年。 
 
<sup>7</sup>《理性：从人工智能到僵尸》第一版最初作为一部庞大的电子书发布，后来经过编辑拆分为多个独立卷册。全书亦可在 http://lesswrong.com/rationality 阅读。

---

## Introduction 

by Rob Bensinger

Imagine reaching into an urn that contains seventy white balls and thirty red ones, and plucking out ten mystery balls.

Perhaps three of the ten balls will be red, and you’ll correctly guess how many red balls total were in the urn. Or perhaps you’ll happen to grab four red balls, or some other number. Then you’ll probably get the total number wrong.

This random error is the cost of incomplete knowledge, and as errors go, it’s not so bad. Your estimates won’t be incorrect on average, and the more you learn, the smaller your error will tend to be. On the other hand, suppose that the white balls are heavier, and sink to the bottom of the urn. Then your sample may be unrepresentative in a consistent direction.

That kind of error is called “statistical bias.” When your method of learning about the world is biased, learning more may not help. Acquiring more data can even consistently worsen a biased prediction.

If you’re used to holding knowledge and inquiry in high esteem, this is a scary prospect. If we want to be sure that learning more will help us, rather than making us worse off than we were before, we need to discover and correct for biases in our data.

The idea of cognitive bias in psychology works in an analogous way. A cognitive bias is a systematic error in how we think, as opposed to a random error or one that’s merely caused by our ignorance. Whereas statistical bias skews a sample so that it less closely resembles a larger population, cognitive biases skew our thinking so that it less accurately tracks the truth (or less reliably serves our other goals).

Maybe you have an optimism bias, and you find out that the red balls can be used to treat a rare tropical disease besetting your brother, and you end up overestimating how many red balls the urn contains because you wish the balls were mostly red.

Like statistical biases, cognitive biases can distort our view of reality, they can’t always be fixed by just gathering more data, and their effects can add up over time. But when the miscalibrated measuring instrument you’re trying to fix is you, debiasing is a unique challenge.

Still, this is an obvious place to start. For if you can’t trust your brain, how can you trust anything else?

### Noticing Bias

Imagine meeting someone for the first time, and knowing nothing about them except that they’re shy.

Question: Is it more likely that this person is a librarian, or a salesperson?

Most people answer “librarian.” Which is a mistake: shy salespeople are much more common than shy librarians, because salespeople in general are much more common than librarians—seventy-five times as common, in the United States.<sup>1</sup>

This is base rate neglect: grounding one’s judgments in how well sets of characteristics feel like they fit together, and neglecting how common each characteristic is in the population at large.<sup>2</sup> Another example of a cognitive bias is the sunk cost fallacy—people’s tendency to feel committed to things they’ve spent resources on in the past, when they should be cutting their losses and moving on.

Knowing about these biases, unfortunately, doesn’t make you immune to them. It doesn’t even mean you’ll be able to notice them in action.

In a study of bias blindness, experimental subjects predicted that they would have a harder time neutrally evaluating the quality of paintings if they knew the paintings were by famous artists. And indeed, these subjects exhibited the very bias they had predicted when the experimenters later tested their prediction. When asked afterward, however, the very same subjects claimed that their assessments of the paintings had been objective and unaffected by the bias.<sup>3</sup>

Even when we correctly identify others’ biases, we exhibit a bias blind spot when it comes to our own flaws.<sup>4</sup> Failing to detect any “biased-feeling thoughts” when we introspect, we draw the conclusion that we must just be less biased than everyone else.<sup>5</sup>

Yet it is possible to recognize and overcome biases. It’s just not trivial. It’s known that subjects can reduce base rate neglect, for example, by thinking of probabilities as frequencies of objects or events.

The approach to debiasing in this book is to communicate a systematic understanding of why good reasoning works, and of how the brain falls short of it. To the extent this volume does its job, its approach can be compared to the one described in Serfas (2010), who notes that “years of financially related work experience” didn’t affect people’s susceptibility to the sunk cost bias, whereas “the number of accounting courses attended” did help.

As a consequence, it might be necessary to distinguish between experience and expertise, with expertise meaning “the development of a schematic principle that involves conceptual understanding of the problem,” which in turn enables the decision maker to recognize particular biases. However, using expertise as countermeasure requires more than just being familiar with the situational content or being an expert in a particular domain. It requires that one fully understand the underlying rationale of the respective bias, is able to spot it in the particular setting, and also has the appropriate tools at hand to counteract the bias.<sup>6</sup>

The goal of this book is to lay the groundwork for creating rationality “expertise.” That means acquiring a deep understanding of the structure of a very general problem: human bias, self-deception, and the thousand paths by which sophisticated thought can defeat itself.

### A Word About This Text

Map and Territory began its life as a series of essays by decision theorist Eliezer Yudkowsky, published between 2006 and 2009 on the economics blog Overcoming Bias and its spin-off community blog Less Wrong. Thematically linked essays were grouped together in “sequences,” and thematically linked sequences were grouped into books. Map and Territory is the first of six such books, with the series as a whole going by the name Rationality: From AI to Zombies. <sup>7</sup>

In style, this series run the gamut from “lively textbook” to “compendium of vignettes” to “riotous manifesto,” and the content is correspondingly varied. The resultant rationality primer is frequently personal and irreverent—drawing, for example, from Yudkowsky’s experiences with his Orthodox Jewish mother (a psychiatrist) and father (a physicist), and from conversations on chat rooms and mailing lists. Readers who are familiar with Yudkowsky from Harry Potter and the Methods of Rationality, his science-oriented take-off of J.K. Rowling’s Harry Potter books, will recognize the same iconoclasm, and many of the same themes.

The philosopher Alfred Korzybski once wrote: “A map is not the territory it represents, but, if correct, it has a similar structure to the territory, which accounts for its usefulness.” And what can be said of maps here, as Korzybski noted, can also be said of beliefs, and assertions, and words.

“The map is not the territory.” This deceptively simple claim is the organizing idea behind this book, and behind the four sequences of essays collected here: Predictably Wrong, which concerns the systematic ways our beliefs fail to map the real world; Fake Beliefs, on what makes a belief a “map” in the first place; Noticing Confusion, on how this world-mapping thing our brains do actually works; and Mysterious Answers, which collides these points together. The book then concludes with “The Simple Truth,” a stand-alone dialogue on the idea of truth itself.

Humans aren’t rational; but, as behavioral economist Dan Ariely notes, we’re predictably irrational. There are patterns to how we screw up. And there are patterns to how we behave when we don’t screw up. Both admit of fuller understanding, and with it, the hope of leaning on that understanding to build a better future for ourselves.

### Acknowledgments

I am stupendously grateful to Nate Soares, Elizabeth Morningstar, Paul Crowley, Brienne Yudkowsky, Adam Freese, Helen Toner, and dozens of volunteers for proofreading portions of this book.

Special and sincere thanks to Alex Vermeer, who steered the original version of this book to completion; to Tsvi Benson-Tilsen, who combed through Map and Territory and the other volumes of Rationality: From AI to Zombies to ensure their readability and consistency; and to Chana Messinger, whose edits have helped make the text a clearer and more graceful whole.

---

<sup>1</sup>Weiten, Psychology: Themes and Variations, Briefer Version, Eighth Edition, 2010.

<sup>2</sup>Heuer, Psychology of Intelligence Analysis, 1999.

<sup>3</sup>Hansen et al., “People Claim Objectivity After Knowingly Using Biased Strategies,” 2014.

<sup>4</sup>Pronin, Lin, and Ross, “The Bias Blind Spot: Perceptions of Bias in Self versus Others,” 2002.

<sup>5</sup>Ehrlinger, Gilovich, and Ross, “Peering Into the Bias Blind Spot: People’s Assessments of Bias in Themselves and Others,” 2005.

<sup>6</sup>Serfas, Cognitive Biases in the Capital Investment Context: Theoretical Considerations and Empirical Experiments on Violations of Normative Rationality, 2010.

<sup>7</sup>The first edition of Rationality: From AI to Zombies was released as a single sprawling ebook, before the series was edited and split up into separate volumes. The full book can also be found on http://lesswrong.com/rationality.

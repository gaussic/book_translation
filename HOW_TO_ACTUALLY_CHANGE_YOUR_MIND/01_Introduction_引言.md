## 引言

作者：Rob Bensinger

1951年秋，达特茅斯和普林斯顿之间的一场橄榄球比赛异常激烈。达特茅斯的心理学家阿尔伯特·哈斯托夫和普林斯顿的哈德利·坎特里尔决定询问两校学生，哪支队伍率先打出了粗野动作。几乎所有人都同意不是普林斯顿先动手；但86%的普林斯顿学生认为是达特茅斯先动的手，而只有36%的达特茅斯学生认为是自己学校先动手。（大多数达特茅斯学生认为“双方都先动手了”。）

后来，研究者让学生们观看比赛录像，并让他们统计看到的犯规次数。达特茅斯学生声称看到达特茅斯队犯规平均4.3次（其中一半被认为是“轻微犯规”），而普林斯顿学生则声称看到达特茅斯队犯规平均9.8次（三分之一为“轻微犯规”）。<sup>1</sup>

当我们所珍视的东西受到威胁——无论是我们的世界观、群体、社会地位，还是其他我们在意的事物——我们的思想和感知就会自发地为其辩护。<sup>2</sup>,<sup>3</sup> 有些心理学家甚至假设，人类之所以进化出为自己结论寻找理由的能力，就是为了帮助我们在争论中获胜。<sup>4</sup>

20世纪心理学的一个基本洞见是：人类行为常常受复杂的无意识过程驱动，而我们为自己动机和理由编造的故事，比我们意识到的要更有偏见、更不靠谱。事实上，我们常常根本没意识到自己在“讲故事”。当我们通过内省“直接感知”到自己的一些想法时，往往其实只是建立在脆弱的隐含因果模型之上。<sup>5</sup>,<sup>6</sup> 当我们试图为自己的信念辩护时，往往能编出与最初得出信念毫无关系的理由。<sup>7</sup> 我们并不是根据解释的预测能力来信任它们，而是根据故事的心理吸引力来信任。

我们该如何做得更好？当我们如此容易自我合理化时，如何获得对世界的真实看法？当我们对思维的思考本身也值得怀疑时，如何获得对自己内心生活的真实理解？

我们能把信任放在最不容易动摇的地方吗？

### 理性的数学

20世纪初，数学家们用简单的（比如集合论的）公理来描述算术，这让他们有了更清晰的标准来判断结论的正确性。如果一个人或计算器输出“2 + 2 = 4”，我们现在不仅能说“这看起来直观上没错”，还能解释为什么它是对的，并证明它的正确性与算术体系中其他结论的正确性有系统联系。

但数学不仅能让我们建模口袋计算器那样的物理系统，还能让我们形式化更有趣的行为。我们甚至可以用概率论来形式化理性信念，从而找出所有成功推理方式的共同特征。我们还可以借助决策理论，形式化理性行为。

概率论定义了在面对不确定性时，理想情况下我们该如何推理——如果我们有足够的时间、计算能力和意志力。给定一些背景知识（先验）和一条新证据，概率论唯一且精确地定义了我应该采纳的最佳新信念（后验）。同样，决策理论定义了我应该基于信念采取什么行动。对于我可能拥有的任何一致的信念和偏好，都有一个决策理论上的答案，告诉我该如何行动才能最大程度地满足自己的偏好。

假设你发现你的六个同学中有一个暗恋你——比如你收到了一封匿名情书，你确定是这六个人中的一个，但不知道具体是谁。鲍勃正好是这六人之一。如果你没有理由认为鲍勃比其他五人更有可能（或更不可能）喜欢你，那么鲍勃是暗恋者的概率是多少？

答案是1:5。有六种可能性，随便猜一次，平均每猜对1次就会猜错5次。

我们不能说：“我不知道谁喜欢我，也许是鲍勃，也许不是，所以我就说概率各一半。”即使我们更想说“我不知道”或“也许”，正确答案依然是1:5。这是因为我们假设有六种可能性，且没有理由偏向其中任何一个。<sup>8</sup>

假设你还注意到，有人喜欢你时会对你眨眼的概率是普通人的十倍。如果鲍勃对你眨了眼，这就是一条新证据。在这种情况下，如果你还对鲍勃是不是暗恋者持怀疑态度，那就是错误的；因为“随机一个对我眨眼的人喜欢我”的概率是10:1，远远超过了“鲍勃喜欢我”的1:5。

但如果你说：“这个证据太强了，肯定就是他喜欢我！我以后就假定鲍勃喜欢我。”那也是错误的。自信过头和自信不足一样糟糕。

事实上，这个问题也只有一个合理答案。要把1:5的先验概率根据10:1的证据更新，只需左右两边分别相乘，得到10:5的后验概率，也就是2:1。也就是说，“鲍勃喜欢你”的概率是2/3。任何其他置信度都是不一致的。

事实证明，只要满足很宽松的条件，“我该相信什么？”这个问题就有一个客观正确答案。即使你充满不确定，也有一个正确的置信度。即使看起来只是“个人信念”而不是专家认证的“事实”，也总有一个正确的置信度。

然而我们常常说，“既然有不确定和分歧，信念就只是个人口味。”我们会说“这只是我的观点”或“你有权持有你的观点”，仿佛科学和数学的断言属于更高层次，而“私人”或“主观”的信念则低一等。经济学家罗宾·汉森对此回应道：<sup>9</sup>

    你永远没有权利拥有自己的观点。永远没有！你甚至没有权利说“我不知道”。你有权拥有自己的欲望，有时有权做出选择。你可以拥有选择权，如果你能选择自己的偏好，你也许有权这样做。但你的信念不是关于你的；信念是关于世界的。你的信念应该是你对事物本来面貌的最佳估计；除此之外都是谎言。[……]

    确实，有些话题专家有更强的机制来解决分歧。在其他话题上，我们的偏见和世界的复杂性让得出强有力的结论变得更难。[……]

    但永远不要忘记，无论是关于世界本来面貌的哪个问题，在任何信息情境下，总有一个最佳估计。你唯一有权利做的，是尽最大努力找到这个最佳估计；除此之外都是谎言。

我们的文化还没有真正吸收概率论的教训——像“我能多确定鲍勃喜欢我？”这样的问题，其正确答案和代数测验或地质教科书里的问题一样受逻辑约束。

我们的头脑是自然选择拼凑出来的。人类既不是完美的推理者，也不是完美的决策者，就像我们不是完美的计算器一样。即使在最佳状态下，我们也无法精确计算“我该怎么想？”和“我该怎么做？”的正确答案。<sup>10</sup>

但即使我们无法变得完全一致，我们依然可以变得更好。知道有一个理想标准可以对照——研究者称之为贝叶斯理性——可以指引我们改进自己的思想和行为。虽然我们永远无法成为完美的贝叶斯主义者，但理性的数学可以帮助我们理解为什么某个答案是正确的，也能帮助我们准确发现自己哪里出错了。

想象一下，如果你只靠死记硬背来学数学。你可能被告知“10 + 3 = 13”，“31 + 108 = 139”，等等，但如果你不理解这些符号背后的规律，这对你没什么用。当你没有一个通用的框架来判断方法是否有效时，想要提升理性就会更难。本书的目的，就是帮助大家为自己建立这样的框架。

### 理性的应用

《如何真正改变你的想法》中的这些紧密关联的文章，最初由 Eliezer Yudkowsky 为博客 Overcoming Bias 撰写。2000年代末，这些文章激发了一个充满活力、关注理性与自我提升的社区的成长。

《地图与领地》是第一本这样的合集。《如何真正改变你的想法》是第二本。完整的六卷本合集《Rationality: From AI to Zombies》可在 Less Wrong 网站（http://lesswrong.com/rationality）上找到。

理性社区最受欢迎的作家之一 Scott Alexander 曾经指出：<sup>11</sup>

    显然，拥有尽可能多的证据是有用的，就像拥有尽可能多的钱一样有用。但同样显然，能够明智地利用有限的证据也很重要，就像明智地利用有限的钱一样重要。

理性技巧帮助我们在证据有限、或偏见扭曲我们解读证据时，最大化证据的价值。

这不仅适用于我们的个人生活，比如鲍勃的故事，也适用于政治派别和球迷之间的分歧，还适用于哲学难题和关于技术与社会未来走向的争论。认识到同样的数学规则适用于这些领域（而且很多情况下同样的认知偏差也会出现），《如何真正改变你的想法》在不同主题间自由切换。

本书的第一组文章“过于方便的借口”，聚焦于概率上“简单”的问题——那些概率极端、系统性错误特别容易被发现的问题……

接下来，我们进入更复杂的领域——“政治与理性”。政治（更准确地说，是电视评论员热议的主流国家政治）以愤怒和无效的讨论著称。表面上看，这很奇怪。为什么我们会把政治分歧看得如此个人化，哪怕国家政治的运作和影响离我们很远？更进一步，为什么我们在处理自己认为重要的问题时，反而不会更加谨慎和严谨地对待证据？

达特茅斯-普林斯顿之战给出了一个线索。我们的大部分推理其实都是自我合理化——讲故事让我们当前的信念看起来更连贯、更有道理，但未必更准确。“反对自我合理化”这一部分正是针对这个问题，接下来是“用新眼光看世界”，讨论如何识别那些不符合我们预期和假设的证据。

在实践中，提升理性往往意味着遇到有趣而强大的新思想，也意味着与现实中的理性社区碰撞。《死亡螺旋》讨论了那些围绕共同兴趣和闪亮新思想而团结起来的群体可能遭遇的重要风险，理性主义者如果想把高尚的理念转化为现实世界的成效，就必须克服这些风险。《如何真正改变你的想法》最后以“放下”为主题收尾。

我们的本能状态并不是像贝叶斯主义者那样改变想法。让达特茅斯和普林斯顿的学生注意到自己实际看到的东西，并不像给他们背概率论公理那么简单。正如慈善研究分析师 Luke Muehlhauser 在《能动性的力量》中写道：<sup>12</sup>

    你不是一个被认知偏差“污染”的贝叶斯小人。你本身就是认知偏差。

确认偏误、现状偏误、对应偏误等等，并不是附加在我们推理上的东西；它们就是我们推理的本质。

但这并不意味着去偏见是不可能的。我们在算术错误之下也不是完美的计算器。我们许多数学上的局限，源于人脑运作方式的深层事实。但我们依然可以训练自己的数学能力，学会何时信任、何时怀疑自己的数学直觉，也可以调整环境让自己更容易做对。如果今天错了，明天可以错得更少。

---

<sup>1</sup>Hastorf and Cantril, “They Saw a Game: A Case Study,” 1954, http://www2.psych.ubc.ca/~schaller/Psyc590Readings/Hastorf1954.pdf.

<sup>2</sup>Pronin, “How We See Ourselves and How We See Others,” 2008.

<sup>3</sup>Vallone, Ross, and Lepper, “The Hostile Media Phenomenon: Biased Perception and Perceptions of Media Bias in Coverage of the Beirut Massacre,” 1985, http://ssc.wisc.edu/~jpiliavi/965/hwang.pdf.

<sup>4</sup>Mercier and Sperber, “Why Do Humans Reason? Arguments for an Argumentative Theory,” 2011, http://hal.archives-ouvertes.fr/file/index/docid/904097/filename/MercierSperberWhydohumansreason.pdf.

<sup>5</sup>Nisbett and Wilson, “Telling More than We Can Know: Verbal Reports on Mental Processes,” 1977, http://people.virginia.edu/~tdw/nisbett&wilson.pdf.

<sup>6</sup>Schwitzgebel, Perplexities of Consciousness, 2011.

<sup>7</sup>Haidt, “The Emotional Dog and Its Rational Tail,” 2001.

<sup>8</sup>我们还假设（其实不现实）你真的能确定暗恋者就是这六个人之一，没有忽略其他可能性。（如果有不止一个同学喜欢你怎么办？）

<sup>9</sup>Hanson, “You Are Never Entitled to Your Opinion,” 2006, http://www.overcomingbias.com/2006/12/you_are_never_e.html.

<sup>10</sup>我们缺乏足够的计算资源（进化也没有工程师的专业知识和远见）来消除所有缺陷。事实上，即使是真实世界中最有效率的推理者，也必须依赖启发式和近似算法。现实中最优的可计算信念更新算法，依然无法达到概率论的一致性。

<sup>11</sup>Alexander, “Why I Am Not Rene Descartes,” 2014, http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/.

Muehlhauser, “The Power of Agency,” 2011, http://lesswrong.com/lw/5i8/the_power_of_agency/.

---

## Introduction

by Rob Bensinger

In the autumn of 1951, a football game between Dartmouth and Princeton turned unusually rough. A pair of psychologists, Dartmouth’s Albert Hastorf and Princeton’s Hadley Cantril, decided to ask students from both schools which team had initiated the rough play. Nearly everyone agreed that Princeton hadn’t started it; but 86% of Princeton students believed that Dartmouth had started it, whereas only 36% of Dartmouth students blamed Dartmouth. (Most Dartmouth students believed “both started it.”)

When shown a film of the game later and asked to count the infractions they saw, Dartmouth students claimed to see a mean of 4.3 infractions by the Dartmouth team (and identified half as “mild”), whereas Princeton students claimed to see a mean of 9.8 Dartmouth infractions (and identified a third as “mild”).<sup>1</sup>

When something we value is threatened—our world-view, our in-group, our social standing, or something else we care about—our thoughts and perceptions rally to their defense.<sup>2</sup>,<sup>3</sup> Some psychologists go so far as to hypothesize that the human ability to come up with explicit justifications for our conclusions evolved specifically to help us win arguments.<sup>4</sup>

One of the basic insights of 20th-century psychology is that human behavior is often driven by sophisticated unconscious processes, and the stories we tell ourselves about our motives and reasons are much more biased and confabulated than we realize. We often fail, in fact, to realize that we’re doing any story-telling. When we seem to “directly perceive” things about ourselves in introspection, it often turns out to rest on tenuous implicit causal models.<sup>5</sup>,<sup>6</sup> When we try to argue for our beliefs, we can come up with shaky reasoning bearing no relation to how we first arrived at the belief.<sup>7</sup> Rather than trusting explanations in proportion to their predictive power, we tend to trust stories in proportion to their psychological appeal.

How can we do better? How can we arrive at a realistic view of the world, when we’re so prone to rationalization? How can we come to a realistic view of our mental lives, when our thoughts about thinking are also suspect?

What’s the least shaky place we could put our weight down?

### The Mathematics of Rationality

At the turn of the 20th century, coming up with simple (e.g., set-theoretic) axioms for arithmetic gave mathematicians a clearer standard by which to judge the correctness of their conclusions. If a human or calculator outputs “2 + 2 = 4,” we can now do more than just say “that seems intuitively right.” We can explain why it’s right, and we can prove that its rightness is tied in systematic ways to the rightness of the rest of arithmetic.

But mathematics lets us model the behaviors of physical systems that are a lot more interesting than a pocket calculator. We can also formalize rational belief in general, using probability theory to pick out features held in common by all successful forms of inference. We can even formalize rational behavior in general by drawing upon decision theory.

Probability theory defines how we would ideally reason in the face of uncertainty, if we had the requisite time, computing power, and mental control. Given some background knowledge (priors) and a new piece of evidence, probability theory uniquely and precisely defines the best set of new beliefs (posterior) I could adopt. Likewise, decision theory defines what action I should take based on my beliefs. For any consistent set of beliefs and preferences I could have, there is a decision-theoretic answer to how I should then act in order to satisfy my preferences.

Suppose you find out that one of your six classmates has a crush on you—perhaps you get a letter from a secret admirer, and you’re sure it’s from one of those six—but you have no idea which of the six it is. Bob happens to be one of those six classmates. If you have no special reason to think Bob’s any likelier (or any less likely) than the other five candidates, then what are the odds that Bob is the one with the crush?

Answer: The odds are 1:5. There are six possibilities, so a wild guess would result in you getting it right once for every five times you got it wrong, on average.

We can’t say, “Well, I have no idea who has a crush on me; maybe it’s Bob, or maybe it’s not. So I’ll just say the odds are fifty-fifty.” Even if we would rather say “I don’t know” or “Maybe” and stop there, the right answer is still 1:5. This follows from the assumption that there are six possibilities and you have no reason to favor one of them over any of the others.<sup>8</sup>

Suppose that you’ve also noticed you get winked at by people ten times as often when they have a crush on you. If Bob then winks at you, that’s a new piece of evidence. In that case, it would be a mistake to stay skeptical about whether Bob is your secret admirer; the 10:1 odds in favor of “a random person who winks at me has a crush on me” outweigh the 1:5 odds against “Bob has a crush on me.”

It would also be a mistake to say, “That evidence is so strong, it’s a sure bet that he’s the one who has the crush on me! I’ll just assume from now on that Bob is into me.” Overconfidence is just as bad as underconfidence.

In fact, there’s only one viable answer to this question too. To change our mind from the 1:5 prior odds in response to the evidence’s 10:1 likelihood ratio, we multiply the left sides together and the right sides together, getting 10:5 posterior odds, or 2:1 odds in favor of “Bob has a crush on me.” Given our assumptions and the available evidence, guessing that Bob has a crush on you will turn out to be correct 2 times for every 1 time it turns out to be wrong. Equivalently: the probability that he’s attracted to you is 2/3. Any other confidence level would be inconsistent.

It turns out that given very modest constraints, the question “What should I believe?” has an objectively right answer. It has a right answer when you’re wracked with uncertainty, not just when you have a conclusive proof. There is always a correct amount of confidence to have in a statement, even when it looks more like a “personal belief” instead of an expert-verified “fact.”

Yet we often talk as though the existence of uncertainty and disagreement makes beliefs a mere matter of taste. We say “that’s just my opinion” or “you’re entitled to your opinion,” as though the assertions of science and math existed on a different and higher plane than beliefs that are merely “private” or “subjective.” To which economist Robin Hanson has responded:<sup>9</sup>

	You are never entitled to your opinion. Ever! You are not even entitled to “I don’t know.” You are entitled to your desires, and sometimes to your choices. You might own a choice, and if you can choose your preferences, you may have the right to do so. But your beliefs are not about you; beliefs are about the world. Your beliefs should be your best available estimate of the way things are; anything else is a lie. [ . . . ]

	It is true that some topics give experts stronger mechanisms for resolving disputes. On other topics our biases and the complexity of the world make it harder to draw strong conclusions. [ . . . ]
	
	But never forget that on any question about the way things are (or should be), and in any information situation, there is always a best estimate. You are only entitled to your best honest effort to find that best estimate; anything else is a lie.

Our culture hasn’t internalized the lessons of probability theory—that the correct answer to questions like “How sure can I be that Bob has a crush on me?” is just as logically constrained as the correct answer to a question on an algebra quiz or in a geology textbook.

Our brains are kludges slapped together by natural selection. Humans aren’t perfect reasoners or perfect decision-makers, any more than we’re perfect calculators. Even at our best, we don’t compute the exact right answer to “what should I think?” and “what should I do?”<sup>10</sup>

And yet, knowing we can’t become fully consistent, we can certainly still get better. Knowing that there’s an ideal standard we can compare ourselves to—what researchers call Bayesian rationality—can guide us as we improve our thoughts and actions. Though we’ll never be perfect Bayesians, the mathematics of rationality can help us understand why a certain answer is correct, and help us spot exactly where we messed up.

Imagine trying to learn math through rote memorization alone. You might be told that “10 + 3 = 13,” “31 + 108 = 139,” and so on, but it won’t do you a lot of good unless you understand the pattern behind the squiggles. It can be a lot harder to seek out methods for improving your rationality when you don’t have a general framework for judging a method’s success. The purpose of this book is to help people build for themselves such frameworks.

### Rationality Applied

The tightly linked essays in How to Actually Change Your Mind were originally written by Eliezer Yudkowsky for the blog Overcoming Bias. Published in the late 2000s, these posts helped inspire the growth of a vibrant community interested in rationality and self-improvement.

Map and Territory was the first such collection. How to Actually Change Your Mind is the second. The full six-book set, titled Rationality: From AI to Zombies, can be found on Less Wrong at http://lesswrong.com/rationality.

One of the rationality community’s most popular writers, Scott Alexander, has previously observed:<sup>11</sup>

	[O]bviously it’s useful to have as much evidence as possible, in the same way it’s useful to have as much money as possible. But equally obviously it’s useful to be able to use a limited amount of evidence wisely, in the same way it’s useful to be able to use a limited amount of money wisely.

Rationality techniques help us get more mileage out of the evidence we have, in cases where the evidence is inconclusive or our biases are distorting how we interpret the evidence.

This applies to our personal lives, as in the tale of Bob. It applies to disagreements between political factions and sports fans. And it applies to philosophical puzzles and debates about the future trajectory of technology and society. Recognizing that the same mathematical rules apply to each of these domains (and that in many cases the same cognitive biases crop up), How to Actually Change Your Mind freely moves between a wide range of topics.

The first sequence of essays in this book, Overly Convenient Excuses, focuses on probabilistically “easy” questions—ones where the odds are extreme, and systematic errors seem like they should be particularly easy to spot.…

From there, we move into murkier waters with Politics and Rationality. Politics—or rather, mainstream national politics of the sort debated by TV pundits—is famous for its angry, unproductive discussions. On the face of it, there’s something surprising about that. Why do we take political disagreements so personally, even though the machinery and effects of national politics are often so distant from us in space or in time? For that matter, why do we not become more careful and rigorous with the evidence when we’re dealing with issues we deem important?

The Dartmouth-Princeton game hints at an answer. Much of our reasoning process is really rationalization—story-telling that makes our current beliefs feel more coherent and justified, without necessarily improving their accuracy. Against Rationalization speaks to this problem, followed by Seeing with Fresh Eyes, on the challenge of recognizing evidence that doesn’t fit our expectations and assumptions.

In practice, leveling up in rationality often means encountering interesting and powerful new ideas and colliding more with the in-person rationality community. Death Spirals discusses some important hazards that can afflict groups united around common interests and amazing shiny ideas, which rationalists will need to overcome if they’re to translate their high-minded ideas into real-world effectiveness. How to Actually Change Your Mind then concludes with a sequence on Letting Go.

Our natural state isn’t to change our minds like a Bayesian would. Getting the Dartmouth and Princeton students to notice what they’re actually seeing won’t be as easy as reciting the axioms of probability theory to them. As philanthropic research analyst Luke Muehlhauser writes in “The Power of Agency”:<sup>12</sup>

	You are not a Bayesian homunculus whose reasoning is “corrupted” by cognitive biases. You just are cognitive biases.

Confirmation bias, status quo bias, correspondence bias, and the like are not tacked on to our reasoning; they are its very substance.

That doesn’t mean that debiasing is impossible. We aren’t perfect calculators underneath all our arithmetic errors, either. Many of our mathematical limitations result from very deep facts about how the human brain works. Yet we can train our mathematical abilities; we can learn when to trust and distrust our mathematical intuitions; we can shape our environments to make things easier on us. And if we’re wrong today, we can be less so tomorrow.

---

<sup>1</sup>Hastorf and Cantril, “They Saw a Game: A Case Study,” 1954, http://www2.psych.ubc.ca/~schaller/Psyc590Readings/Hastorf1954.pdf.

<sup>2</sup>Pronin, “How We See Ourselves and How We See Others,” 2008.

<sup>3</sup>Vallone, Ross, and Lepper, “The Hostile Media Phenomenon: Biased Perception and Perceptions of Media Bias in Coverage of the Beirut Massacre,” 1985, http://ssc.wisc.edu/~jpiliavi/965/hwang.pdf.

<sup>4</sup>Mercier and Sperber, “Why Do Humans Reason? Arguments for an Argumentative Theory,” 2011, http://hal.archives-ouvertes.fr/file/index/docid/904097/filename/MercierSperberWhydohumansreason.pdf.

<sup>5</sup>Nisbett and Wilson, “Telling More than We Can Know: Verbal Reports on Mental Processes,” 1977, http://people.virginia.edu/~tdw/nisbett&wilson.pdf.

<sup>6</sup>Schwitzgebel, Perplexities of Consciousness, 2011.

<sup>7</sup>Haidt, “The Emotional Dog and Its Rational Tail,” 2001.

<sup>8</sup>We’re also assuming, unrealistically, that you can really be certain the admirer is one of those six people, and that you aren’t neglecting other possibilities. (What if more than one of your classmates has a crush on you?)

<sup>9</sup>Hanson, “You Are Never Entitled to Your Opinion,” 2006, http://www.overcomingbias.com/2006/12/you_are_never_e.html.

<sup>10</sup>We lack the computational resources (and evolution lacked the engineering expertise and foresight) to iron out all our bugs. Indeed, even a maximally efficient reasoner in the real world would still need to rely on heuristics and approximations. The best possible computationally tractable algorithms for changing beliefs would still fall short of probability theory’s consistency.

<sup>11</sup>Alexander, “Why I Am Not Rene Descartes,” 2014, http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/.

<sup>12</sup>Muehlhauser, “The Power of Agency,” 2011, http://lesswrong.com/lw/5i8/the_power_of_agency/.
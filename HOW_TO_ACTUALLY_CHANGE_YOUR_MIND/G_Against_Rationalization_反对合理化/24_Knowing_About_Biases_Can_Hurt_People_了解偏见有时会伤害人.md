## 了解偏见有时会伤害人

有一次我试图跟我妈妈讲专家校准的问题，我说：“所以当专家说他们有99%的把握时，实际上只有大约70%的时候是对的。”说到这里我停顿了一下，突然意识到我是在和我妈妈说话，于是赶紧补充道：“当然，你得确保把这种怀疑精神公平地用在自己身上，而不是只用来反驳你不同意的观点——”

我妈妈说：“你在开玩笑吗？这太棒了！我以后一定会经常用这个！”

Taber 和 Lodge 的《政治信念评估中的动机性怀疑》验证了六个预测：

1. 先入为主效应。对某个议题有强烈态度的受试者——即使被鼓励保持客观——也会更偏向支持性论据，而不是反对性论据。
2. 反证偏误。受试者会花更多时间和认知资源去贬低反对性论据，而不是支持性论据。
3. 确认偏误。受试者如果可以自由选择信息来源，会倾向于选择支持自己立场的来源，而不是相反的来源。
4. 态度极化。让受试者接触到看似平衡的正反论据，反而会加剧他们原有的极化倾向。
5. 态度强度效应。表达态度更强烈的受试者，更容易出现上述偏见。
6. 老练效应。政治知识更丰富的受试者，因为拥有更多反驳不一致事实和论据的“弹药”，更容易出现上述偏见。

如果你本来就不理性，知道得越多反而会伤害你。对于真正的贝叶斯主义者来说，信息永远不会带来负的期望效用。但人类不是完美的贝叶斯推理者；如果我们不小心，反而会伤到自己。

我见过一些人因为了解偏见反而把自己搞得很糟。他们有了更多“弹药”去反驳任何自己不喜欢的东西。而这种“弹药太多”的问题，正是 Stanovich 所说的“理性失调症”——高智力者变得愚蠢的主要方式之一。

你肯定能想到这样的人吧？智力很高，但因为太擅长辩论，反而变得效率低下？你觉得如果只是告诉他们一堆经典偏见清单，会让他们变得更理性、更有效吗？

我记得有个人学会了校准/过度自信的问题。没多久他就说：“专家不可信，他们经常错——实验已经证明了。所以我预测未来时，更愿意假设事情会像历史上一样继续发展——”然后就陷入了一套复杂、容易出错、极其可疑的推断。奇怪的是，当涉及到他自己喜欢的结论时，那些偏见和谬误就不那么显眼了——远不如他需要反驳别人时那么容易想到。

我讲过反证偏误和老练论证的问题，结果下次我说了他不喜欢的话，他就指责我“太会辩了”。他并没有指出我哪里具体用了“老练论证”，也没有指出具体的错误——只是摇头叹气，说我居然用自己的聪明才智来反驳自己。他又获得了一个“万能反驳”。

甚至“老练论证者”这个概念本身也很危险——只要你遇到一个看似聪明但说了你不喜欢的话的人，这个标签就会立刻浮现脑海。

我努力从自己的错误中学习。上次我做启发式与偏见的讲座时，我先用合取谬误和代表性启发介绍了这个主题。然后我才讲确认偏误、反证偏误、老练论证、动机性怀疑和其他态度效应。接下来的三十分钟，我反复从不同角度强调这个主题。

我想让听众对这个话题感兴趣。其实只讲合取谬误和代表性启发就够了。但如果他们真的感兴趣呢？偏见相关的文献大多只是为认知心理学本身而写。我必须在那一讲里把警告说清楚，否则他们可能永远听不到。

无论是写作还是演讲，我现在都尽量不在讲校准和过度自信之前，先讲反证偏误、动机性怀疑、老练论证和高智力者的理性失调。首先，不要伤害！

---

## Knowing About Biases Can Hurt People

Once upon a time I tried to tell my mother about the problem of expert calibration, saying: “So when an expert says they’re 99% confident, it only happens about 70% of the time.” Then there was a pause as, suddenly, I realized I was talking to my mother, and I hastily added: “Of course, you’ve got to make sure to apply that skepticism evenhandedly, including to yourself, rather than just using it to argue against anything you disagree with—”

And my mother said: “Are you kidding? This is great! I’m going to use it all the time!”

Taber and Lodge’s “Motivated Skepticism in the Evaluation of Political Beliefs” describes the confirmation of six predictions:

1. Prior attitude effect. Subjects who feel strongly about an issue—even when encouraged to be objective—will evaluate supportive arguments more favorably than contrary arguments.
2. Disconfirmation bias. Subjects will spend more time and cognitive resources denigrating contrary arguments than supportive arguments.
3. Confirmation bias. Subjects free to choose their information sources will seek out supportive rather than contrary sources.
4. Attitude polarization. Exposing subjects to an apparently balanced set of pro and con arguments will exaggerate their initial polarization.
5. Attitude strength effect. Subjects voicing stronger attitudes will be more prone to the above biases.
6. Sophistication effect. Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to the above biases.

If you’re irrational to start with, having more knowledge can hurt you. For a true Bayesian, information would never have negative expected utility. But humans aren’t perfect Bayes-wielders; if we’re not careful, we can cut ourselves.

I’ve seen people severely messed up by their own knowledge of biases. They have more ammunition with which to argue against anything they don’t like. And that problem—too much ready ammunition—is one of the primary ways that people with high mental agility end up stupid, in Stanovich’s “dysrationalia” sense of stupidity.

You can think of people who fit this description, right? People with high g-factor who end up being less effective because they are too sophisticated as arguers? Do you think you’d be helping them—making them more effective rationalists—if you just told them about a list of classic biases?

I recall someone who learned about the calibration/overconfidence problem. Soon after he said: “Well, you can’t trust experts; they’re wrong so often—as experiments have shown. So therefore, when I predict the future, I prefer to assume that things will continue historically as they have—” and went off into this whole complex, error-prone, highly questionable extrapolation. Somehow, when it came to trusting his own preferred conclusions, all those biases and fallacies seemed much less salient—leapt much less readily to mind—than when he needed to counter-argue someone else.

I told the one about the problem of disconfirmation bias and sophisticated argument, and lo and behold, the next time I said something he didn’t like, he accused me of being a sophisticated arguer. He didn’t try to point out any particular sophisticated argument, any particular flaw—just shook his head and sighed sadly over how I was apparently using my own intelligence to defeat itself. He had acquired yet another Fully General Counterargument.

Even the notion of a “sophisticated arguer” can be deadly, if it leaps all too readily to mind when you encounter a seemingly intelligent person who says something you don’t like.

I endeavor to learn from my mistakes. The last time I gave a talk on heuristics and biases, I started out by introducing the general concept by way of the conjunction fallacy and representativeness heuristic. And then I moved on to confirmation bias, disconfirmation bias, sophisticated argument, motivated skepticism, and other attitude effects. I spent the next thirty minutes hammering on that theme, reintroducing it from as many different perspectives as I could.

I wanted to get my audience interested in the subject. Well, a simple description of conjunction fallacy and representativeness would suffice for that. But suppose they did get interested. Then what? The literature on bias is mostly cognitive psychology for cognitive psychology’s sake. I had to give my audience their dire warnings during that one lecture, or they probably wouldn’t hear them at all.

Whether I do it on paper, or in speech, I now try to never mention calibration and overconfidence unless I have first talked about disconfirmation bias, motivated skepticism, sophisticated arguers, and dysrationalia in the mentally agile. First, do no harm!
## 反对“论据大军”的一个理由

我曾经讨论过一种推理方式：在这种方式下，哪怕只出现一个反对论据也不被允许，结果就是每一个不支持的观察都必须被辩解掉。这里我想说的是，当人们遇到反对论据时，他们会通过反复回忆已知的支持论据，来阻止自己下调信心。

假设弗里多尼亚正在辩论邻国西尔瓦尼亚是否要为最近一连串陨石袭击其城市负责。有几条证据支持这个观点：陨石袭击了靠近西尔瓦尼亚边境的城市；袭击发生前，西尔瓦尼亚股市出现了异常活动；西尔瓦尼亚大使特伦蒂诺还被听到嘀咕“天谴”之类的话。

有人来对你说：“我觉得西尔瓦尼亚不该为陨石袭击负责。他们每年和我们有数十亿第纳尔的贸易。”你回答：“可是，陨石袭击了靠近西尔瓦尼亚的城市，他们的股市有可疑活动，大使还说了天谴。”这三条论据比对方那一条更有分量，所以你依然相信西尔瓦尼亚要负责——你在定性上是相信而不是怀疑。显然，证据的天平倾向于西尔瓦尼亚有责。

然后又有人对你说：“我觉得西尔瓦尼亚不该为陨石袭击负责。操控小行星撞击其实很难，西尔瓦尼亚甚至没有航天计划。”你又回答：“但陨石袭击了靠近西尔瓦尼亚的城市，他们的投资者提前知道了，大使还亲口承认了！”同样，这三条论据比对方那一条更有分量（3比1），所以你依然相信西尔瓦尼亚要负责。

事实上，你的信念还被加强了。你已经在两次不同的场合评估了证据的天平，两次都是3比1地倾向于西尔瓦尼亚有责。

你不断遇到支持西尔瓦尼亚的“叛徒”提出的反对论据——一次又一次，甚至一百次——但每次新的论据都被你用那三条老论据轻松击败。每次你都觉得自己对西尔瓦尼亚有责的信心更强了，因为你根据“证据天平”的感觉不断上调自己的先验概率。

问题在于，通过反复回忆你已经知道的论据，你其实是在重复计算同一份证据。即使你把所有证据都重复计算，这也是严重的错误。（想象一下，一个科学家用50个受试者做实验，没得到统计学显著结果，于是把所有数据算两遍。）

但如果你只是选择性地重复计算部分证据，那就完全是闹剧了。我小时候看过一个动画片，里面的反派分赃时用的是这样的算法：“一个给你，一个给我。一个给你，一二个给我。一个给你，一二三个给我。”

正如我在上一篇文章中强调的，即使你珍视的信念是真的，理性主义者在整合所有证据时有时也需要下调概率。没错，支持的证据天平也许依然倾向于你珍视的信念。但你还是得把概率下调——没错，就是下调——从你听到反对证据之前的水平下调。反复回忆支持论据没有任何意义，因为你早就已经把它们考虑进去了。

然而，我确实发现，当人们遇到新的反对论据时，他们会下意识地寻找理由不去下调信心，当然，他们总能找到自己已经知道的支持论据。我自己也必须时刻警惕不要这样做！这感觉就像用手边的盾牌去格挡剑击一样自然。

只要推理方式足够错误，哪怕只有几条支持，甚至只有一个论据，也能抵挡住一大堆反对论据。

---

## One Argument Against An Army

I talked about a style of reasoning in which not a single contrary argument is allowed, with the result that every non-supporting observation has to be argued away. Here I suggest that when people encounter a contrary argument, they prevent themselves from downshifting their confidence by rehearsing already-known support.

Suppose the country of Freedonia is debating whether its neighbor, Sylvania, is responsible for a recent rash of meteor strikes on its cities. There are several pieces of evidence suggesting this: the meteors struck cities close to the Sylvanian border; there was unusual activity in the Sylvanian stock markets before the strikes; and the Sylvanian ambassador Trentino was heard muttering about “heavenly vengeance.”

Someone comes to you and says: “I don’t think Sylvania is responsible for the meteor strikes. They have trade with us of billions of dinars annually.” “Well,” you reply, “the meteors struck cities close to Sylvania, there was suspicious activity in their stock market, and their ambassador spoke of heavenly vengeance afterward.” Since these three arguments outweigh the first, you keep your belief that Sylvania is responsible—you believe rather than disbelieve, qualitatively. Clearly, the balance of evidence weighs against Sylvania.

Then another comes to you and says: “I don’t think Sylvania is responsible for the meteor strikes. Directing an asteroid strike is really hard. Sylvania doesn’t even have a space program.” You reply, “But the meteors struck cities close to Sylvania, and their investors knew it, and the ambassador came right out and admitted it!” Again, these three arguments outweigh the first (by three arguments against one argument), so you keep your belief that Sylvania is responsible.

Indeed, your convictions are strengthened. On two separate occasions now, you have evaluated the balance of evidence, and both times the balance was tilted against Sylvania by a ratio of 3 to 1.

You encounter further arguments by the pro-Sylvania traitors—again, and again, and a hundred times again—but each time the new argument is handily defeated by 3 to 1. And on every occasion, you feel yourself becoming more confident that Sylvania was indeed responsible, shifting your prior according to the felt balance of evidence.

The problem, of course, is that by rehearsing arguments you already knew, you are double-counting the evidence. This would be a grave sin even if you double-counted all the evidence. (Imagine a scientist who does an experiment with 50 subjects and fails to obtain statistically significant results, so the scientist counts all the data twice.)

But to selectively double-count only some evidence is sheer farce. I remember seeing a cartoon as a child, where a villain was dividing up loot using the following algorithm: “One for you, one for me. One for you, one-two for me. One for you, one-two-three for me.”

As I emphasized in the last essay, even if a cherished belief is true, a rationalist may sometimes need to downshift the probability while integrating all the evidence. Yes, the balance of support may still favor your cherished belief. But you still have to shift the probability down—yes, down—from whatever it was before you heard the contrary evidence. It does no good to rehearse supporting arguments, because you have already taken those into account.

And yet it does appear to me that when people are confronted by a new counterargument, they search for a justification not to downshift their confidence, and of course they find supporting arguments they already know. I have to keep constant vigilance not to do this myself! It feels as natural as parrying a sword-strike with a handy shield.

With the right kind of wrong reasoning, a handful of support—or even a single argument—can stand off an army of contradictions.
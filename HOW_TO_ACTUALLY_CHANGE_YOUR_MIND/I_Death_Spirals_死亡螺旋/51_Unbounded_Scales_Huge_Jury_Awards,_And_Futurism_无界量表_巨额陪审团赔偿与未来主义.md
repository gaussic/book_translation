## 无界量表、巨额陪审团赔偿与未来主义

“心理物理学”（psychophysics），尽管名字听起来奇怪，其实是一个严肃的学科，研究物理效应与感官效应之间的关系。如果你向空气中注入声能——也就是制造噪音——那么对一个人来说，这个声音有多大？要让噪音听起来大一倍，需要增加多少声能？答案不是两倍，而是大约八倍。

声能和光子都很容易测量。当你想知道一个声音听起来有多响、一个光源看起来有多亮时，通常会直接问听众或观众。这可以用有界量表来做，比如“非常安静”到“非常响亮”，或“非常暗”到“非常明亮”。你也可以用无界量表，其零点是“完全听不见”或“完全看不见”，但从那里开始可以无限增加。当你用无界量表时，通常会给观察者一个恒定的刺激作为基准（modulus），并赋予一个固定分值。例如，某个声音被定为响度10。然后，观察者可以用20来表示比基准响两倍的声音。

这种方法被证明是相当可靠的。但如果你给受试者一个无界量表，却不给基准点呢？也就是说，零到无穷大，没有任何固定的参考值？那么他们当然会自己设定基准。不同受试者之间，刺激之间的比例依然会高度相关。A受试者说声音X响度为10，声音Y为15；B受试者说声音X为100，那么B很可能会把Y定在150左右。但如果你不知道C受试者用的基准——也就是他们的缩放因子——那你就无法猜到C会给声音X打多少分。可能是1，也可能是1000。

对于用无界量表、没有固定标准来评价单一刺激的受试者来说，几乎所有的方差都来自于基准的随意选择，而不是刺激本身。

“嗯，”你可能会想，“这听起来很像陪审团在讨论惩罚性赔偿时的情形。难怪赔偿金额差异那么大！”这是个有趣的类比，但要怎么用实验来证明呢？

Kahneman 等人让867名有资格担任陪审员的受试者阅读法律案件描述（比如一个孩子的衣服被点燃），然后让他们：

1. 用有界量表评价被告行为的“令人愤慨”程度，
2. 用有界量表评价被告应受的惩罚程度，或
3. 给出惩罚性赔偿的美元金额。<sup>1</sup>

结果显示，受试者在愤慨评分和惩罚评分上的相关性非常高，但在惩罚性赔偿金额上则分布极广。然而，受试者对惩罚性赔偿的排序——从最低到最高的顺序——在不同受试者之间却高度相关。

如果你问“惩罚”量表上的方差有多少可以用具体案例来解释——即同一个案件被不同受试者评价——那么即使是原始分数，解释度也有0.49。美元赔偿排序的解释度为0.51。而原始美元金额的解释度只有0.06！

也就是说，如果你知道案件内容（比如那个衣服被点燃的孩子），你可以很好地猜到惩罚评分，也能猜到赔偿金额在所有案件中的排序，但具体的赔偿金额则完全无法预测。

随机选取12个受试者的中位数也没什么帮助。

所以，陪审团给出的惩罚性赔偿金额，与其说是经济评估，不如说是态度表达——是一种用无界量表、没有标准基准表达愤慨的心理物理测量。

我注意到，许多关于未来的预测其实也最好被看作是一种态度表达。比如“我们距离拥有具有人类水平的AI还有多久？”我见过的回答五花八门。有一次，一位主流AI专家对我说：“五百年。”（！！）

至于为什么“距离AI诞生还有多久”这个问题本身就难以预测，这本身可以写一大篇。但说“500年”的那个人显然不是在真正预测未来。他也不可能用摩尔定律那种标准的伪方法算出来。那么“500”这个数字到底意味着什么？

据我猜测，这就像我问他：“在一个零代表‘一点也不难’的量表上，你觉得AI问题有多难？”如果这是有界量表，每个理智的受访者都会在最右端标记“极其困难”。当你不知道怎么做时，一切都感觉极其困难。但现在用的是无界量表，没有标准基准。所以人们就随便选个数字来代表“极其困难”，可能是50、100，甚至500。然后他们在后面加上“年”，这就成了他们的未来预测。

“你觉得AI问题有多难？”并不是唯一可以被替换的问题。还有人回答得像是我问“你对AI有多乐观？”——只是数字越小越乐观——然后他们也在后面加上“年”。但如果这些“时间估算”除了表达态度之外还有别的含义，我还没能发现。

---

<sup>1</sup>Kahneman, Schkade, and Sunstein, “Shared Outrage and Erratic Awards,” 1998；Kahneman, Ritov, and Schkade, “Economic Preferences or Attitude Expressions?,” 1999.

---

## Unbounded Scales, Huge Jury Awards, and Futurism

“Psychophysics,” despite the name, is the respectable field that links physical effects to sensory effects. If you dump acoustic energy into air—make noise— then how loud does that sound to a person, as a function of acoustic energy? How much more acoustic energy do you have to pump into the air, before the noise sounds twice as loud to a human listener? It’s not twice as much; more like eight times as much.

Acoustic energy and photons are straightforward to measure. When you want to find out how loud an acoustic stimulus sounds, how bright a light source appears, you usually ask the listener or watcher. This can be done using a bounded scale from “very quiet” to “very loud,” or “very dim” to “very bright.” You can also use an unbounded scale, whose zero is “not audible at all” or “not visible at all,” but which increases from there without limit. When you use an unbounded scale, the observer is typically presented with a constant stimulus, the modulus, which is given a fixed rating. For example, a sound that is assigned a loudness of 10. Then the observer can indicate a sound twice as loud as the modulus by writing 20.

And this has proven to be a fairly reliable technique. But what happens if you give subjects an unbounded scale, but no modulus? Zero to infinity, with no reference point for a fixed value? Then they make up their own modulus, of course. The ratios between stimuli will continue to correlate reliably between subjects. Subject A says that sound X has a loudness of 10 and sound Y has a loudness of 15. If subject B says that sound X has a loudness of 100, then it’s a good guess that subject B will assign loudness in the vicinity of 150 to sound Y . But if you don’t know what subject C is using as their modulus—their scaling factor—then there’s no way to guess what subject C will say for sound X. It could be 1. It could be 1,000.

For a subject rating a single sound, on an unbounded scale, without a fixed standard of comparison, nearly all the variance is due to the arbitrary choice of modulus, rather than the sound itself.

“Hm,” you think to yourself, “this sounds an awful lot like juries deliberating on punitive damages. No wonder there’s so much variance!” An interesting analogy, but how would you go about demonstrating it experimentally?

Kahneman et al. presented 867 jury-eligible subjects with descriptions of legal cases (e.g., a child whose clothes caught on fire) and asked them to either

1. Rate the outrageousness of the defendant’s actions, on a bounded scale,
2. Rate the degree to which the defendant should be punished, on a bounded scale, or
3. Assign a dollar value to punitive damages.<sup>1</sup>

And, lo and behold, while subjects correlated very well with each other in their outrage ratings and their punishment ratings, their punitive damages were all over the map. Yet subjects’ rank-ordering of the punitive damages— their ordering from lowest award to highest award—correlated well across subjects.

If you asked how much of the variance in the “punishment” scale could be explained by the specific scenario—the particular legal case, as presented to multiple subjects—then the answer, even for the raw scores, was 0.49. For the rank orders of the dollar responses, the amount of variance predicted was 0.51. For the raw dollar amounts, the variance explained was 0.06!

Which is to say: if you knew the scenario presented—the aforementioned child whose clothes caught on fire—you could take a good guess at the punishment rating, and a good guess at the rank-ordering of the dollar award relative to other cases, but the dollar award itself would be completely unpredictable.

Taking the median of twelve randomly selected responses didn’t help much either.

So a jury award for punitive damages isn’t so much an economic valuation as an attitude expression—a psychophysical measure of outrage, expressed on an unbounded scale with no standard modulus.

I observe that many futuristic predictions are, likewise, best considered as attitude expressions. Take the question, “How long will it be until we have human-level AI?” The responses I’ve seen to this are all over the map. On one memorable occasion, a mainstream AI guy said to me, “Five hundred years.” (!!)

Now the reason why time-to-AI is just not very predictable, is a long discussion in its own right. But it’s not as if the guy who said “Five hundred years” was looking into the future to find out. And he can’t have gotten the number using the standard bogus method with Moore’s Law. So what did the number 500 mean?

As far as I can guess, it’s as if I’d asked, “On a scale where zero is ‘not difficult at all,’ how difficult does the AI problem feel to you?” If this were a bounded scale, every sane respondent would mark “extremely hard” at the right-hand end. Everything feels extremely hard when you don’t know how to do it. But instead there’s an unbounded scale with no standard modulus. So people just make up a number to represent “extremely difficult,” which may come out as 50, 100, or even 500. Then they tack “years” on the end, and that’s their futuristic prediction.

“How hard does the AI problem feel?” isn’t the only substitutable question. Others respond as if I’d asked “How positive do you feel about AI?”— except lower numbers mean more positive feelings—and then they also tack “years” on the end. But if these “time estimates” represent anything other than attitude expressions on an unbounded scale with no modulus, I have been unable to determine it.

---

<sup>1</sup>Kahneman, Schkade, and Sunstein, “Shared Outrage and Erratic Awards,” 1998; Kahneman, Ritov, and Schkade, “Economic Preferences or Attitude Expressions?,” 1999.
## 从虚构证据中泛化的逻辑谬误

每当我试图引入高级人工智能这个话题时，我最常听到的第一句话是什么？

“哦，你是说像《终结者》/《黑客帝国》/阿西莫夫的机器人那样吗！”

我会回答：“呃，不完全是。我尽量避免从虚构证据中泛化的逻辑谬误。”

有些人立刻明白了，还会笑。也有人会为自己的例子辩护，不认为这是一种谬误。

用电影或小说作为讨论的起点到底有什么问题？毕竟没人声称那是真的。谎言在哪里，理性主义者的“罪”又在哪里？科幻小说代表了作者对未来的想象；我们为什么不利用这些已经有人帮我们做过的思考，而要从头开始？

理性的精密舞步中，并不是每一步错误都表现为直接相信谬误；还有更微妙的出错方式。

首先，让我们抛开“科幻小说是对未来的理性预测”这种想法。即使最严谨的科幻作家，首先也是讲故事的人；讲故事的需求和预测未来的需求并不一样。正如 Nick Bostrom 指出：<sup>1</sup>

　　你上一次看到一部讲述人类突然灭绝（没有预警，也没有被其他文明取代）的电影是什么时候？虽然这种情景比人类英雄成功击退怪兽或机器人军团的情景更有可能发生，但它可没什么观赏性。

所以，虚构作品中有特定的扭曲。<sup>2</sup> 但试图只纠正这些特定扭曲还远远不够。故事从来不是理性分析的产物，即使是最严谨的科幻作家也一样，因为故事不会用概率分布。我举个例子：

　　鲍勃·默克尔瑟德小心翼翼地穿过外星飞船的舱门，先向右看，再向左看（或者先左后右），以确认可怕的太空怪兽是否还在。他身边带着唯一对太空怪兽有效的武器——一把纯钛打造的太空剑（概率30%），一根普通的铁撬棍（概率20%），一只在巨石阵废墟中发现的黑色圆盘（概率45%），剩下5%分布在太多小结果上，这里不一一列举。

　　默克尔瑟德（当然也有相当概率是苏珊·威弗福）向前走了两步，或者后退一步，这时一声巨响打破了黑色气闸的寂静！或者是白色气闸的背景嗡嗡声！虽然 Amfer 和 Woofi（1997）认为默克尔瑟德此时被吞噬了，但 Spacklebackle（2003）指出——

角色可以无知，但作者不能说出那三个神奇的字：“我不知道。”主角必须在未来的单线上穿行，故事中充满了细节，从威弗福对女权主义的未来态度，到她耳环的颜色。

然后，所有这些繁琐的细节和可疑的假设被打包成一个简短的标签，制造出它们是一个整体的错觉。<sup>3</sup>

在答案空间极大的问题上，最大难点不是验证正确答案，而是首先在答案空间中定位它。如果有人一上来就问“人工智能会不会像《黑客帝国》那样把我们关进胶囊里？”，他们其实是在跳到一个100比特的信息命题上，却没有相应的98比特证据来让它值得被认真考虑。只要再有几比特证据，这个可能性就会接近确定，这说明几乎所有工作都在前面完成了。

“值得明确考虑的可能性”这个初步步骤，包括：权衡你知道和不知道的、能预测和不能预测的；有意识地避免荒谬偏见，扩大置信区间；思考哪些问题才是真正重要的，努力考虑黑天鹅和未知的未知。直接跳到“黑客帝国：是还是不是？”就跳过了所有这些。

任何职业谈判者都知道，控制辩论的框架几乎等于控制了结果。如果你一开始就想到《黑客帝国》，你脑海里浮现的就是机器人军团与人类苦战——而不是超级智能打个响指就用纳米技术解决一切。它让你聚焦于“我们 vs. 他们”的斗争，关注“谁会赢？”“谁应该赢？”“人工智能真的会那样吗？”这样的问题，营造出一种娱乐氛围，“你对未来有什么惊人设想？”

被遗忘在回声中的，是：人工智能可能实现的多种心智设计；未来对初始条件的依赖；超越人类智能的力量及其不可预测性的论证；人们认真对待这个问题并试图采取行动。

如果有个阴险的辩论操控者想让大家一开始就反驳《终结者》，那他就成功地歪曲了讨论框架。就像在枪支管制辩论中，NRA发言人不希望被介绍为“射击狂”，反对者也不希望被介绍为“解除受害者武装的倡导者”。你为什么要让好莱坞编剧以同样的方式（哪怕是无意中）歪曲你的思维框架？

记者不会直接告诉我，“未来会像《2001太空漫游》那样。”但他们会问：“未来会像《2001》那样，还是像《人工智能》那样？”这和问“我们应该削减残疾退伍军人福利，还是给富人加税？”一样，是巨大的框架问题。

在祖先的环境中，没有电影；你亲眼所见就是真的。一个词的短暂一瞥就能启动相关联想，显著影响概率判断。你觉得一部两小时的电影会对你的判断造成多大影响？即使靠刻意集中注意力也很难消除这种影响——你为什么要主动邀请吸血鬼进屋？在围棋或国际象棋中，每一步浪费都是损失；在理性中，任何非证据性的影响（平均而言）都是熵增。

观众真的能做到不相信电影里看到的东西吗？据我观察，很少有人真的把电影当作亲眼见到的地球未来。看过《终结者》的人并没有在1997年8月29日躲进防空洞。但犯这种谬误的人，似乎就像在另一个星球上亲眼见过这些事件一样；不是地球，而是某个类似地球的地方。

你说：“假设我们造出了非常聪明的人工智能。”他们说：“可《终结者》里不是引发了核战争吗？”据我观察，这和有人说：“可那不是导致了半人马座阿尔法星的核战争吗？”或者“那不是导致了十四世纪皮科洛意大利城邦的灭亡吗？”的推理方式完全一样。电影不是被当作预言，而是被当作历史案例。历史会重演吗？谁知道呢？

最近一次关于智能爆炸的讨论中，有人提到 Vinge 似乎认为脑机接口不会大幅提升智能，并引用了《迷失在实时中》和 Tunç Blumenthal，说他是最先进的旅行者，但似乎也没那么强大。我愤愤地回应：“可 Tunç 丢了大部分硬件！他是被削弱了！”然后我愣了一下，心想：我到底在说什么。

难道这个问题不应该独立讨论，而不是看 Vinge 怎么写角色？Tunç Blumenthal 不是“被削弱了”，他根本就不存在。我可以说“Vinge 出于某些原因选择把 Tunç 写成了被削弱的样子，这些原因未必和他对未来的预测有关”，这样才算给作者选择一个合理的权重。但我不能说“Tunç 被削弱了”。Tunç Blumenthal 根本没有“曾经”。

我特意保留了初稿中的一个错误：“有些人为自己的例子辩护，不认为这是一种谬误。”但《黑客帝国》不是一个“例子”！

一个相邻的错误是从虚构证据推理的逻辑谬误：“如果你真的去到彩虹尽头，你会发现一锅金子——这正好证明了我的观点！”（用预测但未观察到的证据来更新，与事后偏见在数学上是镜像关系。）

大脑有很多机制会根据观察进行泛化，不只是可得性启发。你看到三只斑马，就会形成“斑马”这个类别，这个类别自带自动的感知推断。长得像马、身上有黑白条纹的动物被归为“斑马”，因此它们很快、很好吃，也会被预期和其他斑马类似。

所以人们看到（三个）博格的（动态）画面，大脑就自动创建了“博格”这个类别，并自动推断：有脑机接口的人类属于“博格”类，会和其他博格一样——冷酷、无情、穿黑皮衣、走路机械。记者并不相信未来真的会有博格——他们不认为《星际迷航》是预言。但当有人谈到脑机接口时，他们会想：“未来会有博格吗？”而不是“我怎么知道脑控心灵感应会让人变坏？”不是“我从没见过博格，别人也没有。”不是“我正在基于零证据形成种族刻板印象。”

正如乔治·奥威尔在谈到陈词滥调时说：<sup>4</sup>

　　最重要的是，要让意义选择词语，而不是反过来……当你想到抽象事物时，你更倾向于一开始就用词语，除非你有意识地阻止，否则现成的方言会蜂拥而至，替你完成表达，但代价是模糊甚至改变了你的本意。

但在我看来，使用他人想象力最有害的地方在于，它让人们停止了自己的思考。正如罗伯特·波西格所说：<sup>5</sup>

　　她之所以卡住，是因为她试图在写作中重复她已经听过的东西，就像他第一天上课时试图重复自己早已决定要说的话一样。她想不出要写博兹曼什么，是因为她回忆不出有什么值得重复的内容。她奇怪地没有意识到，她可以在写作时自己去观察、去发现，而不必首先考虑别人说过什么。

记忆中的虚构会冲进来替你思考；它们取代了你自己的观察——这是最致命的便利。

---

<sup>1</sup>Bostrom, “Existential Risks,” 2002, http://www.jetpress.org/volume9/risks.html.

<sup>2</sup>例如 Hanson（2006）“科幻小说的偏见”，http://www.overcomingbias.com/2006/12/biases_of_scien.html。

<sup>3</sup>参见本书中的《第三种选择》，以及《地图与领地》中的《奥卡姆剃刀》和《繁琐细节》。

<sup>4</sup>奥威尔，《政治与英语语言》，1946年。

<sup>5</sup>波西格，《禅与摩托车维修艺术》，1974年。

---

## The Logical Fallacy of Generalization from Fictional Evidence

When I try to introduce the subject of advanced AI, what’s the first thing I hear, more than half the time?

“Oh, you mean like the Terminator movies / The Matrix / Asimov’s robots!”

And I reply, “Well, no, not exactly. I try to avoid the logical fallacy of generalizing from fictional evidence.”

Some people get it right away, and laugh. Others defend their use of the example, disagreeing that it’s a fallacy.

What’s wrong with using movies or novels as starting points for the discussion? No one’s claiming that it’s true, after all. Where is the lie, where is the rationalist sin? Science fiction represents the author’s attempt to visualize the future; why not take advantage of the thinking that’s already been done on our behalf, instead of starting over?

Not every misstep in the precise dance of rationality consists of outright belief in a falsehood; there are subtler ways to go wrong.

First, let us dispose of the notion that science fiction represents a fullfledged rational attempt to forecast the future. Even the most diligent science fiction writers are, first and foremost, storytellers; the requirements of storytelling are not the same as the requirements of forecasting. As Nick Bostrom points out:<sup>1</sup>

	When was the last time you saw a movie about humankind suddenly going extinct (without warning and without being replaced by some other civilization)? While this scenario may be much more probable than a scenario in which human heroes successfully repel an invasion of monsters or robot warriors, it wouldn’t be much fun to watch.

So there are specific distortions in fiction.<sup>2</sup> But trying to correct for these specific distortions is not enough. A story is never a rational attempt at analysis, not even with the most diligent science fiction writers, because stories don’t use probability distributions. I illustrate as follows:

	Bob Merkelthud slid cautiously through the door of the alien spacecraft, glancing right and then left (or left and then right) to see whether any of the dreaded Space Monsters yet remained. At his side was the only weapon that had been found effective against the Space Monsters, a Space Sword forged of pure titanium with 30% probability, an ordinary iron crowbar with 20% probability, and a shimmering black discus found in the smoking ruins of Stonehenge with 45% probability, the remaining 5% being distributed over too many minor outcomes to list here.

	Merklethud (though there’s a significant chance that Susan Wifflefoofer was there instead) took two steps forward or one step back, when a vast roar split the silence of the black airlock! Or the quiet background hum of the white airlock! Although Amfer and Woofi (1997) argue that Merklethud is devoured at this point, Spacklebackle (2003) points out that—

Characters can be ignorant, but the author can’t say the three magic words “I don’t know.” The protagonist must thread a single line through the future, full of the details that lend flesh to the story, from Wifflefoofer’s appropriately futuristic attitudes toward feminism, down to the color of her earrings.

Then all these burdensome details and questionable assumptions are wrapped up and given a short label, creating the illusion that they are a single package.<sup>3</sup>

On problems with large answer spaces, the greatest difficulty is not verifying the correct answer but simply locating it in answer space to begin with. If someone starts out by asking whether or not AIs are gonna put us into capsules like in The Matrix, they’re jumping to a 100-bit proposition, without a corresponding 98 bits of evidence to locate it in the answer space as a possibility worthy of explicit consideration. It would only take a handful more evidence after the first 98 bits to promote that possibility to near-certainty, which tells you something about where nearly all the work gets done.

The “preliminary” step of locating possibilities worthy of explicit consideration includes steps like: weighing what you know and don’t know, what you can and can’t predict; making a deliberate effort to avoid absurdity bias and widen confidence intervals; pondering which questions are the important ones, trying to adjust for possible Black Swans and think of (formerly) unknown unknowns. Jumping to “The Matrix: Yes or No?” skips over all of this.

Any professional negotiator knows that to control the terms of a debate is very nearly to control the outcome of the debate. If you start out by thinking of The Matrix, it brings to mind marching robot armies defeating humans after a long struggle—not a superintelligence snapping nanotechnological fingers. It focuses on an “Us vs. Them” struggle, directing attention to questions like “Who will win?” and “Who should win?” and “Will AIs really be like that?” It creates a general atmosphere of entertainment, of “What is your amazing vision of the future?”

Lost to the echoing emptiness are: considerations of more than one possible mind design that an “artificial intelligence” could implement; the future’s dependence on initial conditions; the power of smarter-than-human intelligence and the argument for its unpredictability; people taking the whole matter seriously and trying to do something about it.

If some insidious corrupter of debates decided that their preferred outcome would be best served by forcing discussants to start out by refuting Terminator, they would have done well in skewing the frame. Debating gun control, the NRA spokesperson does not wish to be introduced as a “shooting freak,” the anti-gun opponent does not wish to be introduced as a “victim disarmament advocate.” Why should you allow the same order of frame-skewing by Hollywood scriptwriters, even accidentally?

Journalists don’t tell me, “The future will be like 2001.” But they ask, “Will the future be like 2001, or will it be like A.I.?” This is just as huge a framing issue as asking, “Should we cut benefits for disabled veterans, or raise taxes on the rich?”

In the ancestral environment, there were no moving pictures; what you saw with your own eyes was true. A momentary glimpse of a single word can prime us and make compatible thoughts more available, with demonstrated strong influence on probability estimates. How much havoc do you think a two-hour movie can wreak on your judgment? It will be hard enough to undo the damage by deliberate concentration—why invite the vampire into your house? In Chess or Go, every wasted move is a loss; in rationality, any non-evidential influence is (on average) entropic.

Do movie-viewers succeed in unbelieving what they see? So far as I can tell, few movie viewers act as if they have directly observed Earth’s future. People who watched the Terminator movies didn’t hide in fallout shelters on August 29, 1997. But those who commit the fallacy seem to act as if they had seen the movie events occurring on some other planet; not Earth, but somewhere similar to Earth.

You say, “Suppose we build a very smart AI,” and they say, “But didn’t that lead to nuclear war in The Terminator?” As far as I can tell, it’s identical reasoning, down to the tone of voice, of someone who might say: “But didn’t that lead to nuclear war on Alpha Centauri?” or “Didn’t that lead to the fall of the Italian city-state of Piccolo in the fourteenth century?” The movie is not believed, but it is cognitively available. It is treated, not as a prophecy, but as an illustrative historical case. Will history repeat itself? Who knows?

In a recent intelligence explosion discussion, someone mentioned that Vinge didn’t seem to think that brain-computer interfaces would increase intelligence much, and cited Marooned in Realtime and Tunç Blumenthal, who was the most advanced traveller but didn’t seem all that powerful. I replied indignantly, “But Tunç lost most of his hardware! He was crippled!” And then I did a mental double-take and thought to myself: What the hell am I saying.

Does the issue not have to be argued in its own right, regardless of how Vinge depicted his characters? Tunç Blumenthal is not “crippled,” he’s unreal. I could say “Vinge chose to depict Tunç as crippled, for reasons that may or may not have had anything to do with his personal best forecast,” and that would give his authorial choice an appropriate weight of evidence. I cannot say “Tunç was crippled.” There is no was of Tunç Blumenthal.

I deliberately left in a mistake I made, in my first draft of the beginning of this essay: “Others defend their use of the example, disagreeing that it’s a fallacy.” But The Matrix is not an example!

A neighboring flaw is the logical fallacy of arguing from imaginary evidence: “Well, if you did go to the end of the rainbow, you would find a pot of gold—which just proves my point!” (Updating on evidence predicted, but not observed, is the mathematical mirror image of hindsight bias.)

The brain has many mechanisms for generalizing from observation, not just the availability heuristic. You see three zebras, you form the category “zebra,” and this category embodies an automatic perceptual inference. Horse-shaped creatures with white and black stripes are classified as “Zebras,” therefore they are fast and good to eat; they are expected to be similar to other zebras observed.

So people see (moving pictures of) three Borg, their brain automatically creates the category “Borg,” and they infer automatically that humans with brain-computer interfaces are of class “Borg” and will be similar to other Borg observed: cold, uncompassionate, dressing in black leather, walking with heavy mechanical steps. Journalists don’t believe that the future will contain Borg—they don’t believe Star Trek is a prophecy. But when someone talks about brain-computer interfaces, they think, “Will the future contain Borg?” Not, “How do I know computer-assisted telepathy makes people less nice?” Not, “I’ve never seen a Borg and never has anyone else.” Not, “I’m forming a racial stereotype based on literally zero evidence.”

As George Orwell said of cliches:<sup>4</sup>

What is above all needed is to let the meaning choose the word, and not the other way around . . . When you think of something abstract you are more inclined to use words from the start, and unless you make a conscious effort to prevent it, the existing dialect will come rushing in and do the job for you, at the expense of blurring or even changing your meaning.

Yet in my estimation, the most damaging aspect of using other authors’ imaginations is that it stops people from using their own. As Robert Pirsig said:<sup>5</sup>

	She was blocked because she was trying to repeat, in her writing, things she had already heard, just as on the first day he had tried to repeat things he had already decided to say. She couldn’t think of anything to write about Bozeman because she couldn’t recall anything she had heard worth repeating. She was strangely unaware that she could look and see freshly for herself, as she wrote, without primary regard for what had been said before.

Remembered fictions rush in and do your thinking for you; they substitute for seeing—the deadliest convenience of all.

---

<sup>1</sup>Bostrom, “Existential Risks,” 2002, http://www.jetpress.org/volume9/risks.html.

<sup>2</sup>E.g., Hanson’s (2006) “Biases of Science Fiction.” http://www.overcomingbias.com/2006/12/biases_of_scien.html.

<sup>3</sup>See “The Third Alternative” in this volume, and “Occam’s Razor” and “Burdensome Details” in Map and Territory.

<sup>4</sup>Orwell, “Politics and the English Language,” 1946.

<sup>4</sup>Pirsig, Zen and the Art of Motorcycle Maintenance, 1974.
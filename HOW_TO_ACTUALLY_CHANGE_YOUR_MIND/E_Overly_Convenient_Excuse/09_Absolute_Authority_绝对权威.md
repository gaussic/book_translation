## 绝对权威

有人会居高临下地对你说：“科学其实什么都不知道。你们只有理论——你们无法确定自己是对的。你们科学家还会改变对引力的看法——谁能保证你们明天不会改变对进化论的看法呢？”

请看这深不可测的文化鸿沟。如果你以为几句话就能跨越它，你一定会大失所望。

在未开化者的世界里，只有权威和非权威。值得信任的就能信任，不值得信任的就该丢弃。信息有好坏之分。如果科学家历史上曾经改变过说法，那么科学就不是真正的权威，从此再也不能信任——就像证人自相矛盾，或员工被发现偷钱一样。

而且，对方理所当然地认为，主张某个观点的人就应该为它辩护到底，绝不承认任何问题。所有主张都会因此被打折扣。如果连科学的支持者都承认科学并不完美，那科学一定毫无价值。

如果一个人习惯了确定性，你不能只对他说：“科学是概率性的，就像所有知识一样。”他们会把前半句当作认罪，后半句则当作推卸责任、拉别人下水。

你已经承认自己不值得信任——那就滚吧，科学，别再来烦我们了！

这种思维模式的一个明显来源是宗教，经典被认为来自上帝；因此承认任何瑕疵都会彻底摧毁其权威；任何怀疑都是罪过，无论你是否真的确定，都必须声称自己确定无疑。<sup>1</sup>

但我怀疑，传统学校教育也有影响。老师告诉你某些事，你必须相信，并在考试中背出来。但如果是学生在课堂上提出建议，你可以同意也可以不同意（至少表面如此），没人会因此惩罚你。

我担心，这种经历把信念的领域映射到了社会权威、命令和法律的领域。在社会领域，绝对法则和非绝对法则、命令和建议、权威和非权威之间有本质区别。似乎有严格知识和不严格知识，就像有严格规定和不严格规定。严格的权威必须服从，不严格的建议则可以听也可以不听，取决于个人喜好。而科学既然承认自己可能出错，就只能归为第二类。

（顺便说一句，我发现有些人认为，如果你没有从老师那里拿到一张写着“权威概率”的纸，或者没有类似不可争辩的来源下达的概率，那么你的不确定性就不属于贝叶斯概率论的范畴。<sup>2</sup> 有人甚至会——天哪！——质疑你对先验概率的估计。在未完全开化者看来，贝叶斯先验属于学生提出的信念，而不是老师下达的信念——所以不算真正的知识。）

“权威之道”和“量化之道”之间的文化鸿沟，对我们这些理性主义者来说非常恼人。对方相信自己拥有比科学“概率性猜测”更可靠的知识——比如预测月亮明天会在预定位置和相位升起，就像自有天文记录以来的每一个夜晚一样，正如物理理论所预测，而这些理论的预测已被成功验证到小数点后十四位。那未开化者所推崇的知识到底是什么？为什么？很可能只是某些发霉的古老卷轴，早已被无数次证伪。可他们却说这比科学更可靠，因为它从不承认错误，从不改变观点，无论被证伪多少次。他们把“确定性”这个词像网球一样随意抛来抛去，轻如鸿毛——而科学家却被怀疑压得喘不过气，努力追求哪怕一丁点概率。“我完美无缺，”他们毫不在意地说，“我当然比你们这些还在努力改进自己的人高明多了。”

你无法用一句话说服他们——没有快速、致命的反驳。你可以仔细思考，也许能在公开辩论中说服观众。不幸的是，你不能直接脱口而出：“愚蠢的凡人，量化之道超出你的理解，你口口声声的‘确定’还不如我们最弱小的假说有保障。”这是一种根本的生活方式差异，根本无法用语言快速描述清楚。

那你在观众面前可以怎么说呢？很难说……也许可以这样：

- “科学的力量在于我们能改变想法、承认错误。如果你从未承认过错误，并不代表你犯的错误更少。”
- “任何人都可以声称自己绝对确定。真正难的是永远不犯错。科学家明白其中的区别，所以他们不会说自己绝对确定。仅此而已。这并不意味着他们有任何具体的怀疑理由——哪怕所有证据都指向同一个方向，所有星辰都像多米诺骨牌一样支持某个假说，科学家还是不会说自己绝对确定，因为他们的标准更高。这并不意味着科学家比那些总是自信满满的政客更没有资格确定。”
- “科学家说‘不绝对确定’时，并不是你日常对话中理解的意思。比如，你去看医生，做了血检，医生回来告诉你：‘我们做了些测试，不能绝对确定你不是奶酪做的，也不能排除你肠子里有二十个巧克力仙女在唱《我爱你》歌。’你肯定要赶紧换医生了。当科学家说同样的话时，意思是概率小到连电子显微镜都看不到，但如果你真有证据，他们也愿意接受。”
- “如果你看到足够的证据，你愿意改变对那些‘确定’之事的看法吗？比如，上帝亲自降临告诉你，你的宗教全都是真的，除了童贞女怀孕。如果你会因此改变看法，那你就不能说自己对童贞女怀孕‘绝对确定’。从概率论的技术角度说，只要你理论上有可能改变想法，概率就不可能恰好等于1。这个不确定性也许比尘埃还小，但它必须存在。如果你连上帝亲自否认都不会改变想法，那你拒绝承认错误的问题已经超出了我这种凡人能说服你的范围。”

不过，更有趣的问题也许是：如果不是在观众面前，你该怎么和对方交流？你如何开始教会一个人，在没有确定性的宇宙中生活？

我认为，第一步应该是理解：你完全可以没有确定性地生活——假如你永远无法确定任何事，这也不会剥夺你做出道德或事实判断的能力。借用 Lois Bujold 的话说：“不要更用力推，而要降低阻力。”

绝对权威的常见辩护之一，是我称之为“灰色谬误的谬误论证”：

- 道德相对主义者说：
    - 世界不是非黑即白的，因此：
    - 一切都是灰色的，因此：
    - 没有人比别人更好，因此：
    - 我想干什么就干什么，你也管不着，哈哈哈。
- 但我们必须能阻止人杀人。
- 所以必须有某种绝对确定的方法，否则道德相对主义者就赢了。

反向愚蠢不是智慧。你不能通过把一个得出坏结论的论证每一步都反过来，来得到正确答案——这样只会让傻瓜对你有太多控制权。数学论证每一步都必须正确。而且，道德相对主义者说“世界不是非黑即白的”，并不意味着这句话是错的，就像斯大林相信2+2=4，并不意味着“2+2=4”是错的一样。错误（只需一个）在于从双色世界跳到单色世界，即认为所有灰色都是同一种色调。

如果你同意“只有绝对善和绝对恶的绝对知识才能做出道德选择”这个前提，那就等于把整个论证都让给对方。你完全可以在对相对更好和更坏的选项有不确定知识的情况下做出选择。这其实应该是常态，没必要大惊小怪。

是的，如果你必须在A和B之间选择，并且你真的能建立起100%校准、完全确定的信心，认为A绝对好、B绝对坏，那这当然足以让你选A。但这不是必要条件。

还有一点：逻辑谬误——诉诸信念后果。

他们还需要知道什么？比如，整个理性主义文化都认为怀疑、质疑和承认错误并不是可耻的事。

还有“通过观察事物获得信息”的观念，而不是被灌输。当你更仔细地观察，有时会发现事物和你第一眼看到的不一样；但这并不意味着大自然欺骗了你，也不意味着你该放弃观察。

还有“置信度校准”的概念——“概率”并不是你脑海里那个衡量情感投入的小进度条。它更像是现实生活中，在某种信念状态下，人们说的话有多大比例是真的。如果你让一百个人各说一句他们“绝对确定”的话，这些话有多少是真的？绝不会是一百句。

事实上，人们最狂热坚信的那些话，往往比“太阳比月亮大”这种显而易见、没人激动的话更不靠谱。你能找到多少“绝对确定”的断言，就能找到多少“绝对确定”其反面的断言，因为这种狂热的信念表达只有在有反对者时才会出现。所以，人们脑海里那个衡量情感投入的进度条，并不能很好地转化为置信度校准——甚至连单调性都没有。

至于“绝对确定”——如果你说某事有99.9999%的概率，那意味着你认为自己可以连续做一百万次同样强度、独立的断言，平均只错一次。这已经够不可思议了。（我们居然能对“你不会中彩票”这种事有这种信心，真是神奇。）所以我们就别再说概率1.0了。当你意识到生活中根本不需要1.0的概率，你就会明白，认为人脑能达到1.0有多荒谬。概率1.0不仅仅是确定性，那是无限的确定性。

事实上，我觉得为了避免公众误解，科学家也许应该说“我们不是无限确定”，而不是“我们不确定”。因为后者在日常对话中，听起来像是你有某种具体的怀疑理由。

---

<sup>1</sup>参见《宣称与欢呼》，收录于《地图与领地》，可在 rationalitybook.com 和 lesswrong.com/rationality 找到。

<sup>2</sup>参见《聚焦你的不确定性》，收录于《地图与领地》。

---

## Absolute Authority

The one comes to you and loftily says: “Science doesn’t really know anything. All you have are theories—you can’t know for certain that you’re right. You scientists changed your minds about how gravity works—who’s to say that tomorrow you won’t change your minds about evolution?”

Behold the abyssal cultural gap. If you think you can cross it in a few sentences, you are bound to be sorely disappointed.

In the world of the unenlightened ones, there is authority and unauthority. What can be trusted, can be trusted; what cannot be trusted, you may as well throw away. There are good sources of information and bad sources of information. If scientists have changed their stories ever in their history, then science cannot be a true Authority, and can never again be trusted—like a witness caught in a contradiction, or like an employee found stealing from the till.

Plus, the one takes for granted that a proponent of an idea is expected to defend it against every possible counterargument and confess nothing. All claims are discounted accordingly. If even the proponent of science admits that science is less than perfect, why, it must be pretty much worthless.

When someone has lived their life accustomed to certainty, you can’t just say to them, “Science is probabilistic, just like all other knowledge.” They will accept the first half of the statement as a confession of guilt; and dismiss the second half as a flailing attempt to accuse everyone else to avoid judgment.

You have admitted you are not trustworthy—so begone, Science, and trouble us no more!

One obvious source for this pattern of thought is religion, where the scriptures are alleged to come from God; therefore to confess any flaw in them would destroy their authority utterly; so any trace of doubt is a sin, and claiming certainty is mandatory whether you’re certain or not.<sup>1</sup>

But I suspect that the traditional school regimen also has something to do with it. The teacher tells you certain things, and you have to believe them, and you have to recite them back on the test. But when a student makes a suggestion in class, you don’t have to go along with it—you’re free to agree or disagree (it seems) and no one will punish you.

This experience, I fear, maps the domain of belief onto the social domains of authority, of command, of law. In the social domain, there is a qualitative difference between absolute laws and nonabsolute laws, between commands and suggestions, between authorities and unauthorities. There seems to be strict knowledge and unstrict knowledge, like a strict regulation and an unstrict regulation. Strict authorities must be yielded to, while unstrict suggestions can be obeyed or discarded as a matter of personal preference. And Science, since it confesses itself to have a possibility of error, must belong in the second class.

(I note in passing that I see a certain similarity to they who think that if you don’t get an Authoritative probability written on a piece of paper from the teacher in class, or handed down from some similar Unarguable Source, then your uncertainty is not a matter for Bayesian probability theory.<sup>2</sup> Someone might—gasp!—argue with your estimate of the prior probability. It thus seems to the not-fully-enlightened ones that Bayesian priors belong to the class of beliefs proposed by students, and not the class of beliefs commanded you by teachers—it is not proper knowledge).

The abyssal cultural gap between the Authoritative Way and the Quantitative Way is rather annoying to those of us staring across it from the rationalist side. Here is someone who believes they have knowledge more reliable than science’s mere probabilistic guesses—such as the guess that the Moon will rise in its appointed place and phase tomorrow, just like it has every observed night since the invention of astronomical record-keeping, and just as predicted by physical theories whose previous predictions have been successfully confirmed to fourteen decimal places. And what is this knowledge that the unenlightened ones set above ours, and why? It’s probably some musty old scroll that has been contradicted eleventeen ways from Sunday, and from Monday, and from every day of the week. Yet this is more reliable than Science (they say) because it never admits to error, never changes its mind, no matter how often it is contradicted. They toss around the word “certainty” like a tennis ball, using it as lightly as a feather—while scientists are weighed down by dutiful doubt, struggling to achieve even a modicum of probability. “I’m perfect,” they say without a care in the world, “I must be so far above you, who must still struggle to improve yourselves.”

There is nothing simple you can say to them—no fast crushing rebuttal. By thinking carefully, you may be able to win over the audience, if this is a public debate. Unfortunately you cannot just blurt out, “Foolish mortal, the Quantitative Way is beyond your comprehension, and the beliefs you lightly name ‘certain’ are less assured than the least of our mighty hypotheses.” It’s a difference of life-gestalt that isn’t easy to describe in words at all, let alone quickly.

What might you try, rhetorically, in front of an audience? Hard to say . . . maybe:

- “The power of science comes from having the ability to change our minds and admit we’re wrong. If you’ve never admitted you’re wrong, it doesn’t mean you’ve made fewer mistakes.”
- “Anyone can say they’re absolutely certain. It’s a bit harder to never, ever make any mistakes. Scientists understand the difference, so they don’t say they’re absolutely certain. That’s all. It doesn’t mean that they have any specific reason to doubt a theory—absolutely every scrap of evidence can be going the same way, all the stars and planets lined up like dominos in support of a single hypothesis, and the scientists still won’t say they’re absolutely sure, because they’ve just got higher standards. It doesn’t mean scientists are less entitled to certainty than, say, he politicians who always seem so sure of everything.”
- “Scientists don’t use the phrase ‘not absolutely certain’ the way you’re used to from regular conversation. I mean, suppose you went to the doctor, and got a blood test, and the doctor came back and said, ‘We ran some tests, and it’s not absolutely certain that you’re not made out of cheese, and there’s a non-zero chance that twenty fairies made out of sentient chocolate are singing the “I love you” song from Barney inside your lower intestine.’ Run for the hills, your doctor needs a doctor. When a scientist says the same thing, it means that they think the probability is so tiny that you couldn’t see it with an electron microscope, but the scientist is willing to see the evidence in the extremely unlikely event that you have it.”
- “Would you be willing to change your mind about the things you call ‘certain’ if you saw enough evidence? I mean, suppose that God himself descended from the clouds and told you that your whole religion was true except for the Virgin Birth. If that would change your mind, you can’t say you’re absolutely certain of the Virgin Birth. For technical reasons of probability theory, if it’s theoretically possible for you to change your mind about something, it can’t have a probability exactly equal to one. The uncertainty might be smaller than a dust speck, but it has to be there. And if you wouldn’t change your mind even if God told you otherwise, then you have a problem with refusing to admit you’re wrong that transcends anything a mortal like me can say to you, I guess.”

But, in a way, the more interesting question is what you say to someone not in front of an audience. How do you begin the long process of teaching someone to live in a universe without certainty?

I think the first, beginning step should be understanding that you can live without certainty—that if, hypothetically speaking, you couldn’t be certain of anything, it would not deprive you of the ability to make moral or factual distinctions. To paraphrase Lois Bujold, “Don’t push harder, lower the resistance.”

One of the common defenses of Absolute Authority is something I call “The Argument from the Argument from Gray,” which runs like this:

- Moral relativists say:
	- The world isn’t black and white, therefore:
	- Everything is gray, therefore:
	– No one is better than anyone else, therefore:
	– I can do whatever I want and you can’t stop me bwahahaha.
- But we’ve got to be able to stop people from committing murder.
- Therefore there has to be some way of being absolutely certain, or the moral relativists win.

Reversed stupidity is not intelligence. You can’t arrive at a correct answer by reversing every single line of an argument that ends with a bad conclusion—it gives the fool too much detailed control over you. Every single line must be correct for a mathematical argument to carry. And it doesn’t follow, from the fact that moral relativists say “The world isn’t black and white,” that this is false, any more than it follows, from Stalin’s belief that 2 + 2 = 4, that “2 + 2 = 4” is false. The error (and it only takes one) is in the leap from the two-color view to the single-color view, that all grays are the same shade.

It would concede far too much (indeed, concede the whole argument) to agree with the premise that you need absolute knowledge of absolutely good options and absolutely evil options in order to be moral. You can have uncertain knowledge of relatively better and relatively worse options, and still choose. It should be routine, in fact, not something to get all dramatic about.

I mean, yes, if you have to choose between two alternatives A and B, and you somehow succeed in establishing knowably certain well-calibrated 100% confidence that A is absolutely and entirely desirable and that B is the sum of everything evil and disgusting, then this is a sufficient condition for choosing A over B. It is not a necessary condition.

Oh, and: Logical fallacy: Appeal to consequences of belief.

Let’s see, what else do they need to know? Well, there’s the entire rationalist culture which says that doubt, questioning, and confession of error are not terrible shameful things.

There’s the whole notion of gaining information by looking at things, rather than being proselytized. When you look at things harder, sometimes you find out that they’re different from what you thought they were at first glance; but it doesn’t mean that Nature lied to you, or that you should give up on seeing.

Then there’s the concept of a calibrated confidence—that “probability” isn’t the same concept as the little progress bar in your head that measures your emotional commitment to an idea. It’s more like a measure of how often, pragmatically, in real life, people in a certain state of belief say things that are actually true. If you take one hundred people and ask them each to make a statement of which they are “absolutely certain,” how many of these statements will be correct? Not one hundred.

If anything, the statements that people are really fanatic about are far less likely to be correct than statements like “the Sun is larger than the Moon” that seem too obvious to get excited about. For every statement you can find of which someone is “absolutely certain,” you can probably find someone “absolutely certain” of its opposite, because such fanatic professions of belief do not arise in the absence of opposition. So the little progress bar in people’s heads that measures their emotional commitment to a belief does not translate well into a calibrated confidence—it doesn’t even behave monotonically.

As for “absolute certainty”—well, if you say that something is 99.9999% probable, it means you think you could make one million equally strong independent statements, one after the other, over the course of a solid year or so, and be wrong, on average, around once. This is incredible enough. (It’s amazing to realize we can actually get that level of confidence for “Thou shalt not win the lottery.”) So let us say nothing of probability 1.0. Once you realize you don’t need probabilities of 1.0 to get along in life, you’ll realize how absolutely ridiculous it is to think you could ever get to 1.0 with a human brain. A probability of 1.0 isn’t just certainty, it’s infinite certainty.

In fact, it seems to me that to prevent public misunderstanding, maybe scientists should go around saying “We are not INFINITELY certain” rather than “We are not certain.” For the latter case, in ordinary discourse, suggests you know some specific reason for doubt.

---

<sup>1</sup>See “Professing and Cheering,” collected in Map and Territory and findable at rationalitybook.com and lesswrong.com/rationality.

<sup>2</sup>See “Focus Your Uncertainty” in Map and Territory.
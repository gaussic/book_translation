## “但还是有可能，对吧？”

几年前，我和一个人聊天时，他随口说自己不相信进化论。我说：“现在可不是十九世纪了。达尔文刚提出进化论时，怀疑它也许还算合理。但现在是二十一世纪了。我们可以读取基因。人类和黑猩猩有98%的DNA相同。我们知道人类和黑猩猩是亲缘物种。这个问题已经结束了。”

他说：“也许DNA的相似只是巧合呢。”

我说：“这种巧合的概率大概是二的七亿五千万次方分之一。”

他说：“但还是有可能，对吧？”

其实，我的过去的自己在这场对话中并不能算完全占据道德高地。一方面，我已经不记得自己当时是怎么得出2<sup>750,000,000</sup>这个数字的，虽然大致数量级可能没错。另一方面，我当时也没有应用“置信度校准”这个概念。毕竟，在人类历史上，每当有人算出某事发生的概率是2<sup>750,000,000</sup>分之一时，他们实际出错的概率肯定远高于这个数字。比如，人类和黑猩猩共享基因的比例后来被修正为95%，不是98%——而且这可能只适用于已知的三万个基因，而不是整个基因组，那就完全不是一个数量级了。

不过，我还是觉得对方的回答挺有意思的。

我不记得自己后来怎么回应的——大概只是说了句“不”——但我记得这次对话，因为它让我对“未开化者”的思维方式有了不少新认识。

我首先意识到，人类的直觉会在“完全不可能”和“极小概率但值得关注”之间做出质的区分。你可以在 Overcoming Bias 博客关于彩票的讨论中看到这种现象。

问题在于，概率论有时确实能算出一个概率小到根本不值得你花脑力去关注的结果——但等你算出来的时候，你已经关注它了。人们会把地图和领地混淆，所以在直觉层面上，追踪一个用符号描述的概率，会让人觉得“这是值得关注的机会”，即使这个概率小到如果它是尘埃，你根本看不见。我们可以用语言描述如此微小的数字，但无法用感觉来感知——因为那样的感觉根本不存在，不足以激活足够的神经元或释放足够的神经递质让你有感觉。这也是为什么人们会买彩票——没人能真正“感受到”概率有多小。

但更让我着迷的是，人们会在“确定性论证”和“不确定性论证”之间做出质的区分，只要论证不是绝对确定的，你就可以忽略它。比如，如果概率是零，你就必须放弃原有信念；但如果概率是一谷歌分之一，你就可以继续坚持。

当然，这是自由国家，没人会因为“非法推理”而把你关进监狱。但如果你打算忽略一个概率是一谷歌分之一的论据，为什么不干脆也忽略概率为零的论据呢？反正你都要无视证据了，为什么无视确定性证据就比无视不确定性证据更糟糕？

我发现，生活中我常常能从别人那些明显的糟糕例子中学到东西，然后把它们推广到更微妙的情形。在这个例子里，反过来的教训是：如果你因为主观愿望不能忽略一谷歌分之一的概率，那你也不能因为主观愿望忽略0.9的概率。其实这都是同一条滑坡。

下次你想“但你不能证明我错了”时，不妨想想这个例子。如果你都能无视一个概率性反驳，为什么不能无视一个严格的证明呢？

---

## But There’s Still a Chance, Right?

Years ago, I was speaking to someone when he casually remarked that he didn’t believe in evolution. And I said, “This is not the nineteenth century. When Darwin first proposed evolution, it might have been reasonable to doubt it. But this is the twenty-first century. We can read the genes. Humans and chimpanzees have 98% shared DNA. We know humans and chimps are related. It’s over. ”

He said, “Maybe the DNA is just similar by coincidence.”

I said, “The odds of that are something like two to the power of seven hundred and fifty million to one.”

He said, “But there’s still a chance, right?”

Now, there’s a number of reasons my past self cannot claim a strict moral victory in this conversation. One reason is that I have no memory of whence I pulled that 2<sup>750,000,000</sup> figure, though it’s probably the right meta-order of magnitude. The other reason is that my past self didn’t apply the concept of a calibrated confidence. Of all the times over the history of humanity that a human being has calculated odds of 2<sup>750,000,000</sup>:1 against something, they have undoubtedly been wrong more often than once in 2<sup>750,000,000</sup> times. E.g., the shared genes estimate was revised to 95%, not 98%—and that may even apply only to the 30,000 known genes and not the entire genome, in which case it’s the wrong meta-order of magnitude.

But I think the other guy’s reply is still pretty funny. 

I don’t recall what I said in further response—probably something like “No”—but I remember this occasion because it brought me several insights into the laws of thought as seen by the unenlightened ones.

It first occurred to me that human intuitions were making a qualitative distinction between “No chance” and “A very tiny chance, but worth keeping track of.” You can see this in the Overcoming Bias lottery debate.

The problem is that probability theory sometimes lets us calculate a chance which is, indeed, too tiny to be worth the mental space to keep track of it—but by that time, you’ve already calculated it. People mix up the map with the territory, so that on a gut level, tracking a symbolically described probability feels like “a chance worth keeping track of,” even if the referent of the symbolic description is a number so tiny that if it were a dust speck, you couldn’t see it. We can use words to describe numbers that small, but not feelings—a feeling that small doesn’t exist, doesn’t fire enough neurons or release enough neurotransmitters to be felt. This is why people buy lottery tickets—no one can feel the smallness of a probability that small.

But what I found even more fascinating was the qualitative distinction between “certain” and “uncertain” arguments, where if an argument is not certain, you’re allowed to ignore it. Like, if the likelihood is zero, then you have to give up the belief, but if the likelihood is one over googol, you’re allowed to keep it.

Now it’s a free country and no one should put you in jail for illegal reasoning, but if you’re going to ignore an argument that says the likelihood is one over googol, why not also ignore an argument that says the likelihood is zero? I mean, as long as you’re ignoring the evidence anyway, why is it so much worse to ignore certain evidence than uncertain evidence?

I have often found, in life, that I have learned from other people’s nicely blatant bad examples, duly generalized to more subtle cases. In this case, the flip lesson is that, if you can’t ignore a likelihood of one over googol because you want to, you can’t ignore a likelihood of 0.9 because you want to. It’s all the same slippery cliff.

Consider his example if you ever you find yourself thinking, “But you can’t prove me wrong.” If you’re going to ignore a probabilistic counterargument, why not ignore a proof, too?
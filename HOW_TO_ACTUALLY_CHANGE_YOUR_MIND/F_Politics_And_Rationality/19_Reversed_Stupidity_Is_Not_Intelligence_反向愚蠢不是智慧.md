## 反向愚蠢不是智慧

> “……然后我们那条时间线上的人开始采取纠正措施。看这里。”
> 他擦掉了屏幕，然后开始输入各种组合。页面一页页地出现，都是声称见过神秘圆盘的人们的报告，每一份都比上一份更离奇。
> “标准的掩盖技术。”Verkan Vall 咧嘴一笑，“我只听到过一点关于‘飞碟’的传闻，而且都是开玩笑的。在那种文化里，只要制造十个明显虚假的故事，就能让一个真实故事也被质疑。”
> ——H. Beam Piper，《警察行动》

Piper 说得很有道理。就我个人而言，我并不相信这里真的有隐藏得很差的外星人。但我的不信，并不是因为飞碟邪教的荒谬和尴尬——至少我希望不是。

你我都相信，飞碟邪教的出现完全不是因为真的有飞碟。只要有人类的愚蠢，几乎任何想法都能诞生邪教。这种愚蠢和外星人是否存在是正交的：无论有没有飞碟，我们都预期会有飞碟邪教。即使真的有隐藏得很差的外星人，飞碟邪教的出现概率也不会更低。除非你假设外星人会刻意打压飞碟邪教，否则 P(有外星人|有邪教) 不会比 P(没外星人|有邪教) 小。按照贝叶斯证据的定义，“存在飞碟邪教”这个观察结果，并不能作为反对飞碟存在的证据。它对两种假设都没什么影响。

这其实是一个通用原则的应用，正如 Robert Pirsig 所说：“世界上最愚蠢的人也可能说‘太阳在照耀’，但这并不会让天变黑。”<sup>2</sup>

如果你认识一个在是非题上 99.99% 都答错的人，你只要反着选就能获得 99.99% 的正确率。但要做到这一点，他得先获得和现实纠缠的高质量证据，并且有条理地处理这些证据，才能如此稳定地反相关。要想一直答错，他得超级聪明才行。

一辆坏掉的汽车并不能以 200 英里每小时的速度倒着开——哪怕它真的坏得很彻底。

如果愚蠢和真理之间都不能稳定地反相关，那人类的邪恶和真理之间就更不可能反相关了。光环效应的反面就是“恶魔效应”：所有被感知为负面的特质都会相关。如果斯大林是邪恶的，那他讲的每句话都应该是错的。你总不想和斯大林观点一致吧？

但斯大林也相信 2 + 2 = 4。可如果你为斯大林说过的任何一句话辩护，哪怕是“2 + 2 = 4”，人们只会看到你“和斯大林站在一起”；你一定是他那一边的。

这个原则的推论有：

- 如果你想诚实地反驳一个观点，你应该反驳最强支持者的最佳论据。反驳弱者证明不了什么，因为再强的观点也会吸引弱支持者。如果你想反对超人类主义或智能爆炸，必须直接挑战 Nick Bostrom 或 2003 年后的 Eliezer Yudkowsky 的论据。最不方便的路径才是唯一有效的路径。<sup>3</sup>
- 展示那些因某个理念而疯狂的可怜疯子，并不能作为反对该理念的证据。许多新时代信徒因为“量子力学”而变得更疯狂。
- 有人说过：“不是所有保守派都愚蠢，但大多数愚蠢的人都是保守派。”如果你无法让自己进入一种状态，让这句话（无论真假）在评判保守主义时显得完全无关紧要，那你还没准备好理性地思考政治。
- 人身攻击不是有效的论证方式。
- 你需要能够在不提“希特勒想灭绝犹太人”的情况下反对种族灭绝。如果希特勒没主张过种族灭绝，难道这就变得可以接受了吗？
- 用汉森的术语说：你本能上愿意相信某事，和你愿意和哪些以相信它著称的人结盟是同步变化的——和这个信念本身是否为真无关。有些人不愿相信上帝不存在，并不是因为有证据证明上帝存在，而是因为他们不愿和理查德·道金斯或那些“咄咄逼人”的无神论者站在一起。
- 如果你现在的电脑坏了，你不能就此断定所有和当前系统一样的东西都错了，必须换掉 AMD 处理器、ATI 显卡、Maxtor 硬盘或机箱风扇——哪怕你的电脑有这些部件但坏了。也许你只需要换根电源线。
- 如果一百个发明家用金属、木头和帆布造飞行器都失败了，这并不意味着你需要用骨头和肉来造飞行器。如果一千个人用电子计算机造人工智能都失败了，这也不代表电才是问题根源。在你真正理解问题之前，盲目反向操作极难碰巧得到正确答案。<sup>4</sup>

---

<sup>1</sup>“P(cults|aliens)” 读作“在有外星人访问地球的前提下，出现 UFO 邪教的概率”；“P(cults|¬aliens)” 读作“在没有外星人访问地球的前提下，出现 UFO 邪教的概率”。

<sup>2</sup>Pirsig, 《禅与摩托车维修艺术》，1974。

<sup>3</sup>见 Alexander, “The Least Convenient Possible World,” 2018年12月2日, http://lesswrong.com/lw/2k/the_least_convenient_possible_world/。

<sup>4</sup>另见 “Selling Nonapples.” http://lesswrong.com/lw/vs/selling_nonapples.

---

## Reversed Stupidity Is Not Intelligence

> “. . . then our people on that time-line went to work with corrective action. Here.”
> He wiped the screen and then began punching combinations. Page after page appeared, bearing accounts of people who had claimed to have seen the mysterious disks, and each report was more fantastic than the last.
> “The standard smother-out technique,” Verkan Vall grinned. “I only heard a little talk about the ‘flying saucers,’ and all of that was in joke. In that order of culture, you can always discredit one true story by setting up ten others, palpably false, parallel to it.”
> —H. Beam Piper, Police Operation

Piper had a point. Pers’nally, I don’t believe there are any poorly hidden aliens infesting these parts. But my disbelief has nothing to do with the awful embarrassing irrationality of flying saucer cults—at least, I hope not.

You and I believe that flying saucer cults arose in the total absence of any flying saucers. Cults can arise around almost any idea, thanks to human silliness. This silliness operates orthogonally to alien intervention: We would expect to see flying saucer cults whether or not there were flying saucers. Even if there were poorly hidden aliens, it would not be any less likely for flying saucer cults to arise. The conditional probability P(cults|aliens) isn’t less than P(cults|¬aliens), unless you suppose that poorly hidden aliens would deliberately suppress flying saucer cults.<sup>1</sup> By the Bayesian definition of evidence, the observation “flying saucer cults exist” is not evidence against the existence of flying saucers. It’s not much evidence one way or the other.

This is an application of the general principle that, as Robert Pirsig puts it, “The world’s greatest fool may say the Sun is shining, but that doesn’t make it dark out.”<sup>2</sup>

If you knew someone who was wrong 99.99% of the time on yes-or-no questions, you could obtain 99.99% accuracy just by reversing their answers. They would need to do all the work of obtaining good evidence entangled with reality, and processing that evidence coherently, just to anticorrelate that reliably. They would have to be superintelligent to be that stupid.

A car with a broken engine cannot drive backward at 200 mph, even if the engine is really really broken.

If stupidity does not reliably anticorrelate with truth, how much less should human evil anticorrelate with truth? The converse of the halo effect is the horns effect: All perceived negative qualities correlate. If Stalin is evil, then everything he says should be false. You wouldn’t want to agree with Stalin, would you?

Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by Stalin, even “2 + 2 = 4,” people will see only that you are “agreeing with Stalin”; you must be on his side.

Corollaries of this principle:

- To argue against an idea honestly, you should argue against the best arguments of the strongest advocates. Arguing against weaker advocates proves nothing, because even the strongest idea will attract weak advocates. If you want to argue against transhumanism or the intelligence explosion, you have to directly challenge the arguments of Nick Bostrom or Eliezer Yudkowsky post-2003. The least convenient path is the only valid one.<sup>3</sup>
- Exhibiting sad, pathetic lunatics, driven to madness by their apprehension of an Idea, is no evidence against that Idea. Many New Agers have been made crazier by their personal apprehension of quantum mechanics.
- Someone once said, “Not all conservatives are stupid, but most stupid people are conservatives.” If you cannot place yourself in a state of mind where this statement, true or false, seems completely irrelevant as a critique of conservatism, you are not ready to think rationally about politics.
- Ad hominem argument is not valid.
- You need to be able to argue against genocide without saying “Hitler wanted to exterminate the Jews.” If Hitler hadn’t advocated genocide, would it thereby become okay?
- In Hansonian terms: Your instinctive willingness to believe something will change along with your willingness to affiliate with people who are known for believing it—quite apart from whether the belief is actually true. Some people may be reluctant to believe that God does not exist, not because there is evidence that God does exist, but rather because they are reluctant to affiliate with Richard Dawkins or those darned “strident” atheists who go around publicly saying “God does not exist.”
- If your current computer stops working, you can’t conclude that everything about the current system is wrong and that you need a new system without an AMD processor, an ATI video card, a Maxtor hard drive, or case fans—even though your current system has all these things and it doesn’t work. Maybe you just need a new power cord.
- If a hundred inventors fail to build flying machines using metal and wood and canvas, it doesn’t imply that what you really need is a flying machine of bone and flesh. If a thousand projects fail to build Artificial Intelligence using electricity-based computing, this doesn’t mean that electricity is the source of the problem. Until you understand the problem, hopeful reversals are exceedingly unlikely to hit the solution.<sup>4</sup>

---

<sup>1</sup>Read “P(cults|aliens)” as “the probability of UFO cults given that aliens have visited Earth,” and read “P(cults|¬aliens)” as “the probability of UFO cults given that aliens have not visited Earth.”

<sup>2</sup>Pirsig, Zen and the Art of Motorcycle Maintenance, 1974.

<sup>3</sup>See Alexander, “The Least Convenient Possible World,” December 2, 2018, http://lesswrong.com/lw/2k/the_least_convenient_possible_world/.

<sup>4</sup>See also “Selling Nonapples.” http://lesswrong.com/lw/vs/selling_nonapples.
## 无限的确定性

在《绝对权威》一文中，我曾论证过你并不需要无限的确定性：

    如果你必须在两个选项 A 和 B 之间做选择，并且你真的能建立起可知的、校准良好的 100% 信心，认为 A 绝对且完全值得选择，而 B 是一切邪恶与令人厌恶之总和，那么这当然足以让你选择 A 而不是 B。但这不是必要条件……你完全可以在对相对更好和更坏的选项只有不确定知识的情况下做出选择。事实上，这应该是常态。

关于“2 + 2 = 4”这个命题，我们必须区分地图和领地。鉴于物理定律看似绝对稳定和普适，宇宙历史上也许从未有任何粒子超过过局部光速极限。也就是说，光速极限也许不仅仅是 99% 的时间成立，或 99.9999% 的时间成立，或（1−1/谷歌普勒克斯） 的时间成立，而是始终、绝对成立。

但我们是否能对光速极限拥有绝对信心，则是完全不同的问题。地图不是领地。

也许某个学生确实完全抄袭了作业，但你是否知道这个事实——更不用说是否有绝对信心——则是另一回事。如果你抛一枚硬币却没看结果，硬币正面朝上也许是完全真实的，但你却完全不确定到底是正面还是反面。不确定性的程度和事实的真假、发生的频率不是一回事。

数学真理也是如此。很难说“2 + 2 = 4”或“在皮亚诺算术中，SS0 + SS0 = SSSS0”在纯粹抽象意义上是否为真，除非我们指的是那些行为类似皮亚诺公理的物理系统。话虽如此，我还是大胆猜测，无论“2 + 2 = 4”以何种意义为真，它都是始终且精确为真，而不是大致为真（比如“2 + 2 实际等于 4.0000004”），也不是在一万亿次中有 999,999,999,999 次为真。

我并不完全确定在这种情况下“为真”到底意味着什么，但我依然坚持我的猜测。“2 + 2 = 4 始终为真”这一命题的可信度，远远超过任何关于“为真”“始终”或“是”这些词在上述语句中到底意味着什么的哲学立场。

但这并不意味着我对 2 + 2 = 4 拥有绝对信心。参见前文关于如何说服我相信 2 + 2 = 3 的讨论——只要用和最初让我相信 2 + 2 = 4 同类型的证据就可以做到。我可能曾经对所有那些证据产生过幻觉，或者记错了。在神经学史上，出现过比这更奇怪的大脑功能障碍。

那么，如果我们要给“2 + 2 = 4”这个命题分配一个概率，这个概率应该是多少？你真正追求的其实是良好的校准——你赋予“99%概率”的命题，应该有 100 个里有 99 个是真的。实际上，这比你想象的要难得多。让一百个人各说十个他们“99%有信心”的断言，一共一千个，你觉得会有大约十个是错的吗？

我不打算讨论关于校准的实际实验——你可以在我关于认知偏差与全球灾难性风险的书章节中找到这些实验<sup>1</sup>——因为我发现，如果没有适当的铺垫，直接把这些实验结果说出来，大家往往会把它当作“万能反驳”，只要遇到不喜欢的观点就用来质疑对方的信心，却从不用来质疑自己。所以，除非是在包含动机性怀疑警告的理性系统讲解中，我一般不会单独谈论这些实验。

但现实中，人类自称“99%有信心”的断言，其准确率并没有达到 99%。

假设你说自己对 2 + 2 = 4 有 99.99% 的信心。那你其实是在声称，自己可以做出一万个同等信心的独立断言，平均只错一次。也许对于 2 + 2 = 4 这种极其简单、既有数学又有经验支持、社会共识极强（不是被热烈肯定，而是被默默视为理所当然）的命题，这种极高的信心是可能的。所以，也许你真的能对这个命题有 99.99% 的信心。

但对于“53 是质数”这样的断言，我认为你很难达到 99.99% 的信心。虽然看起来很可能，但如果你真的要建立一套协议，让你能做出一万个类似的独立断言（不仅仅是关于质数的断言，而是每次都用新协议），你会错不止一次。<sup>2</sup>

但地图不是领地：如果我说自己对 2 + 2 = 4 有 99% 的信心，这并不意味着我认为“2 + 2 = 4”只有 99% 的精确度，或者说“2 + 2 = 4”在一百次中有九十九次为真。我信心所指的命题是“2 + 2 = 4 始终且精确为真”，而不是“2 + 2 = 4 大多数时候为真”。

至于你能否对某个数学命题有 100% 的信心——拜托！如果你说 99.9999% 的信心，就是说你可以连续做出一百万个同等信心的断言，平均只错一次。假设你每 20 秒说一句话，每天说 16 小时，这大约是一整年不间断地说话。

如果你声称有 99.9999999999% 的信心，那就是一万亿次。你要说上一百个人类寿命的话，还一次都不能错？

如果你声称有（1−1/谷歌普勒克斯）的信心，那你的自负已经远超那些自认为自己是上帝的精神病人了。

而谷歌普勒克斯还比 3 ↑↑↑ 3 这种“相对较小的不可思议大数”小得多。但即使是（1−1/3 ↑↑↑ 3）的信心，也并不比你对某事 90% 有信心更接近**概率 1**。

如果一切都失败了，假想中的“矩阵黑暗领主”此刻正篡改你对这句话的信心评估，他们会挡在你和无限确定性之间。

我对此绝对确定吗？

当然不是。

正如 Rafal Smigrodski 曾说：

    我认为你应该可以对推导贝叶斯法则所需的数学概念分配一个小于 1 的确定性水平，同时在实践中依然使用它。我并不完全确定自己必须永远不确定。也许我真的可以对某些事完全确定。但一旦我对某个命题分配了概率 1，无论以后看到什么、学到什么，都无法再改变。无论遇到什么与这个公理相悖的东西，我都必须拒绝。我不喜欢永远无法改变想法的感觉。

---

<sup>1</sup>Yudkowsky, “Cognitive Biases Potentially Affecting Judgment of Global Risks,” 2008.

<sup>2</sup>Peter de Blanc 有个有趣的轶事可参考：http://www.spaceandgames.com/?p=27。（我让他别再这么做了。）

---

## Infinite Certainty

In “Absolute Authority,” I argued that you don’t need infinite certainty:

	If you have to choose between two alternatives A and B, and you somehow succeed in establishing knowably certain wellcalibrated 100% confidence that A is absolutely and entirely desirable and that B is the sum of everything evil and disgusting, then this is a sufficient condition for choosing A over B. It is not a necessary condition . . . You can have uncertain knowledge of relatively better and relatively worse options, and still choose. It should be routine, in fact.

Concerning the proposition that 2 + 2 = 4, we must distinguish between the map and the territory. Given the seeming absolute stability and universality of physical laws, it’s possible that never, in the whole history of the universe, has any particle exceeded the local lightspeed limit. That is, the lightspeed limit may be not just true 99% of the time, or 99.9999% of the time, or (1− 1/googolplex) of the time, but simply always and absolutely true.

But whether we can ever have absolute confidence in the lightspeed limit is a whole ’nother question. The map is not the territory.

It may be entirely and wholly true that a student plagiarized their assignment, but whether you have any knowledge of this fact at all—let alone absolute confidence in the belief—is a separate issue. If you flip a coin and then don’t look at it, it may be completely true that the coin is showing heads, and you may be completely unsure of whether the coin is showing heads or tails. A degree of uncertainty is not the same as a degree of truth or a frequency of occurrence.

The same holds for mathematical truths. It’s questionable whether the statement “2 + 2 = 4” or “In Peano arithmetic, SS0 + SS0 = SSSS0” can be said to be true in any purely abstract sense, apart from physical systems that seem to behave in ways similar to the Peano axioms. Having said this, I will charge right ahead and guess that, in whatever sense “2 + 2 = 4” is true at all, it is always and precisely true, not just roughly true (“2 + 2 actually equals 4.0000004”) or true 999,999,999,999 times out of 1,000,000,000,000.

I’m not totally sure what “true” should mean in this case, but I stand by my guess. The credibility of “2 + 2 = 4 is always true” far exceeds the credibility of any particular philosophical position on what “true,” “always,” or “is” means in the statement above.

This doesn’t mean, though, that I have absolute confidence that 2 + 2 = 4. See the previous discussion on how to convince me that 2 + 2 = 3, which could be done using much the same sort of evidence that convinced me that 2 + 2 = 4 in the first place. I could have hallucinated all that previous evidence, or I could be misremembering it. In the annals of neurology there are stranger brain dysfunctions than this.

So if we attach some probability to the statement “2 + 2 = 4,” then what should the probability be? What you seek to attain in a case like this is good calibration—statements to which you assign “99% probability” come true 99 times out of 100. This is actually a hell of a lot more difficult than you might think. Take a hundred people, and ask each of them to make ten statements of which they are “99% confident.” Of the 1,000 statements, do you think that around 10 will be wrong?

I am not going to discuss the actual experiments that have been done on calibration—you can find them in my book chapter on cognitive biases and global catastrophic risk<sup>1</sup>—because I’ve seen that when I blurt this out to people without proper preparation, they thereafter use it as a Fully General Counterargument, which somehow leaps to mind whenever they have to discount the confidence of someone whose opinion they dislike, and fails to be available when they consider their own opinions. So I try not to talk about the experiments on calibration except as part of a structured presentation of rationality that includes warnings against motivated skepticism.

But the observed calibration of human beings who say they are “99% confident” is not 99% accuracy.

Suppose you say that you’re 99.99% confident that 2 + 2 = 4. Then you have just asserted that you could make 10,000 independent statements, in which you repose equal confidence, and be wrong, on average, around once. Maybe for 2 + 2 = 4 this extraordinary degree of confidence would be possible: “2 + 2 = 4” is extremely simple, and mathematical as well as empirical, and widely believed socially (not with passionate affirmation but just quietly taken for granted). So maybe you really could get up to 99.99% confidence on this one.

I don’t think you could get up to 99.99% confidence for assertions like “53 is a prime number.” Yes, it seems likely, but by the time you tried to set up protocols that would let you assert 10,000 independent statements of this sort—that is, not just a set of statements about prime numbers, but a new protocol each time—you would fail more than once.<sup>2</sup>

Yet the map is not the territory: If I say that I am 99% confident that 2 + 2 = 4, it doesn’t mean that I think “2 + 2 = 4” is true to within 99% precision, or that “2 + 2 = 4” is true 99 times out of 100. The proposition in which I repose my confidence is the proposition that “2 + 2 = 4 is always and exactly true,” not the proposition “2 + 2 = 4 is mostly and usually true.”

As for the notion that you could get up to 100% confidence in a mathematical proposition—well, really now! If you say 99.9999% confidence, you’re implying that you could make one million equally fraught statements, one after the other, and be wrong, on average, about once. That’s around a solid year’s worth of talking, if you can make one assertion every 20 seconds and you talk for 16 hours a day.

Assert 99.9999999999% confidence, and you’re taking it up to a trillion. Now you’re going to talk for a hundred human lifetimes, and not be wrong even once?

Assert a confidence of (1− 1/googolplex) and your ego far exceeds that of mental patients who think they’re God.

And a googolplex is a lot smaller than even relatively small inconceivably huge numbers like 3 ↑↑↑ 3. But even a confidence of (1− 1/3 ↑↑↑ 3) isn’t all that much closer to **PROBABILITY 1** than being 90% sure of something.

If all else fails, the hypothetical Dark Lords of the Matrix, who are right now tampering with your brain’s credibility assessment of this very sentence, will bar the path and defend us from the scourge of infinite certainty.

Am I absolutely sure of that?

Why, of course not.

As Rafal Smigrodski once said:

	I would say you should be able to assign a less than 1 certainty level to the mathematical concepts which are necessary to derive Bayes’s rule itself, and still practically use it. I am not totally sure I have to be always unsure. Maybe I could be legitimately sure about something. But once I assign a probability of 1 to a proposition, I can never undo it. No matter what I see or learn, I have to reject everything that disagrees with the axiom. I don’t like the idea of not being able to change my mind, ever.

---

<sup>1</sup>Yudkowsky, “Cognitive Biases Potentially Affecting Judgment of Global Risks,” 2008.

<sup>1</sup>Peter de Blanc has an amusing anecdote on this point: http://www.spaceandgames.com/?p=27. (I told him not to do it again.)
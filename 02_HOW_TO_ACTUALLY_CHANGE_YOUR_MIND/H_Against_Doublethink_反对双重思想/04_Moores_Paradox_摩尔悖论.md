## 摩尔悖论（Moore's Paradox）

作者：Eliezer Yudkowsky

我现在对摩尔悖论理解得更深了一些，部分归功于我在 Less Wrong 上看到的一些评论。[Jimrandomh](https://www.lesswrong.com/lw/r/no_really_ive_deceived_myself/#ga) 提出了一个观点：

> 许多人无法区分间接层级（levels of indirection）。对他们来说，“我相信 X”和“X”是同一回事，因此，“相信 X 是有益的”这一理由也就成了“X 是真的”这一理由。

我认为这个说法不完全正确——相对年幼的孩子也能理解“错误信念”的概念，这本身就需要把“地图”和“领地”分放在不同的心理桶里。但这个观点确实指向了一个相似的概念：

许多人也许并不会有意识地分清“相信某事”和“支持某事”之间的区别。

毕竟，“我相信民主”在口语中通常是指你支持民主制度，而不是你相信民主制度客观存在。所以，“信念”这个词可能具有不止一种含义。我们可能面对的是一个[令人困惑的词语](http://www.overcomingbias.com/2008/02/compress-fallac.html)，它会引发混乱的思维（或者它只是反映了人们本就存在的混乱）。

所以，在[原始例子](https://www.lesswrong.com/lw/s/belief_in_selfdeception/)中，“我相信人们比他们实际更友善”，这个说法来自她为“相信人们很友善”这一想法找到了一些好处——比如健康上的益处。于是她对“相信人们很友善”这件事产生了温暖的情感，然后她内省这种积极情感，并得出结论：“我相信人们很友善”。也就是说，她把这种附着在引号中信念上的正面情感误认为是她真正相信该命题的信号。与此同时，她观察世界时却觉得人们并没有那么友善。于是她说出了“我相信人们比他们实际更友善”这样一句话。

从某种意义上说，这接近于是一个无心的错误——某种程度上算是诚实的错误——因为人们并没有被明确教导过如何判断自己是否真的相信某个东西。就像[车库里的龙](http://www.overcomingbias.com/2007/07/belief-in-belie.html)的寓言那样；那个说“我车库里有条龙——但它是隐形的”的人，并没有意识到自己对“看不到龙”的预期，实际上反映了他脑中根本就没有“龙”这个模型。

毕竟人们并没有被训练过要识别自己何时真正相信某事。他们在高中时从未被教导过这样的内容：“真正相信某件事的感觉，是这句话听起来就像是世界原本的样子。你应该学会识别这种感觉，它代表的是未经引号的真正信念，并且要和你对一个信念抱有积极情绪（但你清楚那只是一个信念、加了引号的东西）区分开。”

理解了这些之后，现实生活中这类摩尔悖论的案例就[不再那么陌生](https://www.lesswrong.com/lw/s/belief_in_selfdeception/)，也为人们如何能够[同时是对的又是错的](https://www.lesswrong.com/lw/1d/simultaneously_right_and_wrong/)提供了一种新机制。

类似地，[Kurige](https://www.lesswrong.com/lw/r/no_really_ive_deceived_myself/#gk) 写道：

> 我相信上帝的存在——并且他赋予了我们对善恶的感知，使我们能够评判这个世界。我也相信道德感是由进化程序化地植入我们体内的——这种道德感很可能来源于很久以前倭黑猩猩群体中元政治联盟的形成。这两个信念并不矛盾，困难之处在于如何将两者调和起来。

我怀疑，Kurige，你是决定了自己有理由去支持“上帝赋予了我们道德感”这个加了引号的信念。同时你也有理由支持科学的观点。这两个群体看起来都不错，对吧？相信这两套信念似乎都各有好处？你内省自己的感觉，发现你对这两个信念都感觉良好？

但你并没有说：

“上帝赋予了我们对善恶的感知，同时道德感也是由进化植入我们体内的。这两种现实状态并不矛盾，问题在于如何将它们协调起来。”

如果你正在读这篇文章，Kurige，你应该立刻把上面这句话大声读出来，好让自己注意到它在主观上更难以接受一点——注意这种主观上的不同感受——在你试图重新合理化之前。

这就是**支持两个不同信念的理由**，与**你头脑中关于单一现实世界、单一真相模型**之间的主观差异。

---

## Moore's Paradox

by Eliezer Yudkowsky

I think I understand Moore's Paradox a bit better now, after reading some of the comments on Less Wrong.  [Jimrandomh](https://www.lesswrong.com/lw/r/no_really_ive_deceived_myself/#ga) suggests:

> Many people cannot distinguish between levels of indirection. To them, "I believe X" and "X" are the same thing, and therefore, reasons why it is beneficial to believe X are also reasons why X is true.

I don't think this is correct—relatively young children can understand the concept of having a false belief, which requires separate mental buckets for the map and the territory.  But it points in the direction of a similar idea:

Many people may not consciously distinguish between believing something and endorsing it.

After all—"I believe in democracy" means, colloquially, that you endorse the concept of democracy, not that you believe democracy exists.  The word "belief", then, has more than one meaning.  We could be looking at a [confused word](http://www.overcomingbias.com/2008/02/compress-fallac.html) that causes confused thinking (or maybe it just reflects pre-existing confusion).

So: in the [original example](https://www.lesswrong.com/lw/s/belief_in_selfdeception/), "I believe people are nicer than they are", she came up with some reasons why it would be good to believe people are nice—health benefits and such—and since she now had some warm affect on "believing people are nice", she introspected on this warm affect and concluded, "I believe people are nice".  That is, she mistook the positive affect attached to the quoted belief, as signaling her belief in the proposition.  At the same time, the world itself seemed like people weren't so nice.  So she said, "I believe people are nicer than they are."

And that verges on being an honest mistake—sort of—since people are not taught explicitly how to know when they believe something.  As in the parable of the [dragon in the garage](http://www.overcomingbias.com/2007/07/belief-in-belie.html); the one who says "There is a dragon in my garage—but it's invisible", does not recognize his anticipation of seeing no dragon, as indicating that he possesses an (accurate) model with no dragon in it.

It's not as if people are trained to recognize when they believe something.  It's not like they're ever taught in high school:  "What it feels like to actually believe something—to have that statement in your belief pool—is that it just seems like the way the world is.  You should recognize this feeling, which is actual (unquoted) belief, and distinguish it from having good feelings about a belief that you recognize as a belief (which means that it's in quote marks)."

This goes a long way toward making this real-life case of Moore's Paradox [seem less alien](https://www.lesswrong.com/lw/s/belief_in_selfdeception/), and providing another mechanism whereby people can be [simultaneously right and wrong](https://www.lesswrong.com/lw/1d/simultaneously_right_and_wrong/).

Likewise [Kurige](https://www.lesswrong.com/lw/r/no_really_ive_deceived_myself/#gk) who wrote:

> I believe that there is a God—and that He has instilled a sense of right and wrong in us by which we are able to evaluate the world around us.  I also believe a sense of morality has been evolutionarily programmed into us—a sense of morality that is most likely a result of the formation of meta-political coalitions in Bonobo communities a very, very long time ago.  These two beliefs are not contradictory, but the complexity lies in reconciling the two.

I suspect, Kurige, you have decided that you have reasons to endorse the quoted belief that God has instilled a sense of right and wrong in us.  And also that you have reasons to endorse the verdict of science.  They both seem like good communities to join, right?  There are benefits to both sets of beliefs?  You introspect and find that you feel good about both beliefs?

But you did not say:

"God instilled a sense of right and wrong in us, and also a sense of morality has been evolutionarily programmed into us.  The two states of reality are not inconsistent, but the complexity lies in reconciling the two."

If you're reading this, Kurige, you should very quickly say the above out loud, so you can notice that it seems at least slightly harder to swallow—notice the subjective difference—before you go to the trouble of rerationalizing.

This is the subjective difference between having reasons to endorse two different beliefs, and your mental model of a single world, a single way-things-are.
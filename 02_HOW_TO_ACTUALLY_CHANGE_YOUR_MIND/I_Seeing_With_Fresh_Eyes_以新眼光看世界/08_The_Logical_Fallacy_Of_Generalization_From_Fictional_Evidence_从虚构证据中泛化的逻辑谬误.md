以下是 Eliezer Yudkowsky 的文章《**从虚构证据中泛化的逻辑谬误**》的中文翻译，保留了原始链接和引用：

---

## 从虚构证据中泛化的逻辑谬误

作者：Eliezer Yudkowsky

当我试图引入高级人工智能这个话题时，我最常听到的第一反应是什么？超过一半的情况是这样的：

“哦，你是说《终结者》/《黑客帝国》/阿西莫夫的机器人那种？”

我会回答：“呃，不完全是。我尽量避免从虚构证据中泛化的逻辑谬误。”

有些人立刻明白了，会笑。有些人则为自己的例子辩护，不认为这是一种谬误。

用电影或小说作为讨论的起点，有什么不对吗？毕竟没人说这些内容是真的。那么，谎言在哪里，理性主义者的过错在哪里？科幻小说代表的是作者对未来的设想；为何不利用别人已经替我们思考过的内容，而非从零开始？

但并非所有理性舞步的失误都源自于对虚假信息的直接信仰；有些错误更微妙。

首先，我们要摒弃“科幻小说代表一种全面的理性预测尝试”这一观念。即使是最勤奋的科幻作家，首先也是讲故事的人；讲故事的需求和预测未来的需求并不相同。正如 Nick Bostrom 所指出的：<sup>1</sup>

> 你上一次看到一部讲述人类突然灭绝（毫无预兆，也没有被其他文明取代）的电影是什么时候？虽然这种情形可能比人类英雄成功击退怪兽或机器人战士的情节更有可能发生，但这并不太适合用来娱乐。

所以，虚构作品中存在特定的扭曲。<sup>2</sup> 但仅仅纠正这些特定扭曲还不够。一个故事从来都不是理性的分析尝试，即使是最尽责的科幻作家也一样，因为故事不会使用概率分布。我用如下例子来说明：

> 鲍勃·默克尔瑟小心翼翼地滑入外星飞船的舱门，向右然后向左（或向左再向右）张望，查看是否还有可怕的太空怪兽残留在此。他身旁带着唯一被证实对太空怪兽有效的武器：一种由纯钛打造的太空剑（概率为30%）、一根普通的铁撬棍（概率为20%）、一枚在巨石阵焦黑废墟中发现的闪闪发亮的黑色圆盘（概率为45%），剩下的5%分布于太多微不足道的结果中，此处不一一列举。
>
> 默克尔瑟（尽管也有相当概率是苏珊·维夫勒福）向前走了两步或向后退了一步，这时，一声巨响打破了黑色气闸的寂静！或者是白色气闸背景中平静的嗡嗡声！虽然 Amfer 和 Woofi（1997）认为此时默克尔瑟被吞噬，但 Spacklebackle（2003）指出——

故事中的角色可以无知，但作者不能说那三个神奇的词：“我不知道。” 主角必须在未来之中穿行，故事中充满了赋予其真实质感的细节，从维夫勒福对未来女权的合适态度，到她耳环的颜色。

然后，所有这些繁重的细节和可疑的假设被包装起来，贴上一个简短的标签，让人误以为它们是一个整体。<sup>3</sup>

在解答空间巨大的问题上，最大难点不是验证正确答案，而是**一开始就找到它的位置**。如果有人开口就问：“AI会不会把我们放进《黑客帝国》那样的胶囊里？”他们直接跳到了一个 100 比特的信息断言上，却没有提供对应的 98 比特证据来定位这一可能性，使其值得我们明确考虑。而只需再添少量证据，这种可能性就可以被提升为接近确定的状态——这说明，几乎所有工作都集中在最初那 98 比特上。

在判断是否值得认真考虑某种可能性之前，我们应先进行一系列“前置”步骤：权衡你知道和不知道的、可预测和不可预测的；有意识地避免荒谬偏见，扩大信心区间；思考哪些问题是真正重要的问题，努力考虑潜在的“黑天鹅”事件，以及未知的未知。如果直接跳到“黑客帝国：是或否？”这一问题，那你跳过了所有这些必要的推理步骤。

任何专业谈判者都知道，**控制辩论的框架几乎等于控制辩论的结果**。如果你一开始就联想到《黑客帝国》，你脑中会浮现出机器人军队与人类浴血奋战的场景——而不是超级智能打个响指用纳米技术改变世界。你会聚焦于“我们 vs 它们”的冲突，开始考虑“谁会赢？”“谁应该赢？”“AI真的会变成那样吗？”这样的问题。这种思维方式营造出一种娱乐氛围——“你对未来有什么惊人的想象？”

而真正重要的问题——诸如 AI 可以拥有多少种不同的心智结构？未来的走向是否依赖初始条件？超越人类的智能的能力及其不可预测性？我们是否应当认真看待这一问题并采取行动？——这些都被淹没在虚构带来的回音之中。

如果某个操控者想要控制辩论结果，并希望所有人从驳斥《终结者》开始，他们一定成功地偏移了讨论的框架。就像在讨论枪支管控时，NRA 的发言人不会想被介绍为“射击狂”，反对方也不想被称为“解除受害者武装倡导者”。那为什么你要允许好莱坞编剧对话题的设定拥有同等级别的操控力呢，即使只是无意的？

记者不会直接告诉我：“未来会像《2001太空漫游》那样。”但他们会问：“未来会像《2001》那样，还是像《人工智能》那样？”这就像问：“我们该削减残疾退伍军人的福利，还是提高富人的税？”同样是框架问题。

在我们的祖先所处的环境中，没有影视画面；你亲眼看到的才是真实的。研究表明，哪怕只是一瞬间看到一个单词，也足以触发心理启动效应，显著影响人们对概率的判断。那么，一部两个小时的电影，会对你的判断力造成多大破坏？即使通过刻意集中注意力也很难修复这种损伤——你又何必“邀请吸血鬼进屋”？就像在棋类比赛中，每走一步都很重要；在追求理性中，任何非证据性影响（在平均意义上）都是熵的来源。

观众是否能成功否定自己所看到的内容？从行为上看，大多数人并不会像真的看到了地球的未来。看过《终结者》的人并没有在 1997 年 8 月 29 日躲进防核地堡。但犯下此类谬误的人，似乎行为举止间仿佛他们曾在其他星球亲眼见过电影中的事件发生；不是地球，但类似于地球的某地。

你说：“假设我们造出一个非常聪明的 AI。”对方答道：“但《终结者》里不是导致了核战争吗？”这与说“但在半人马座阿尔法星也发生了核战争！”、“十四世纪意大利皮科洛城邦也因此灭亡！”一样荒谬。这部电影并非被信以为真，而是**被大脑当作可用信息**。它不是一个预言，却被当作“历史类比”。历史会重演吗？谁知道呢？

在一次有关智能爆炸的讨论中，有人提到 Vinge 并不认为脑机接口会大幅提升智能，并引用了《被困在实时之外》和 Tunç Blumenthal，说他是最先进的旅行者却也没显得多厉害。我愤怒地反驳道：“但 Tunç 丢了大部分硬件！他是被削弱了！”然后我愣了一下，自问：“我在说什么鬼东西。”

这个议题难道不应当本身就是一个需要独立论证的问题吗？Tunç Blumenthal 不是“被削弱了”，他是**虚构的**。当然，以下是续译部分：

---

我可以说“Vinge 出于某些理由（这些理由可能与他对未来的真实预测有关，也可能无关）选择将 Tunç 描绘为被削弱的状态”，而这只是他**作为作者的创作选择**，所提供的证据权重也应与此相匹配。但我不能说“Tunç 被削弱了”。Tunç Blumenthal 根本就**不存在**，不存在所谓“Tunç 曾经如何”。

我特意保留了我初稿中犯的一个错误：“有些人为使用该例子进行辩护，不认为那是一种谬误。”
但事实上，《黑客帝国》根本**不是一个例子**！

一个相邻的逻辑错误是：**从想象的证据出发进行论证**。比如说：“嗯，如果你真的走到了彩虹尽头，你就会发现一锅金子——这就证明了我的观点！”（基于“被预测但并未观察到”的证据进行更新，这在数学上正是事后偏误的镜像。）

人脑拥有多种从观察中进行泛化的机制，并不仅仅是可得性启发式（availability heuristic）。你看到三匹斑马，就会在脑中形成“斑马”这一类别，该类别会自动带来一些感知上的推理。形似马、带有黑白条纹的生物被分类为“斑马”，因此我们会认为它们奔跑迅速、肉质可口；我们预期它们会与我们观察过的其他斑马相似。

于是，当人们看到（三维动态的）三个博格（Borg），他们的大脑会自动形成“博格”这一类别，并自动推断：带有脑机接口的人类属于“博格”类，会和之前所见的博格相似——冷漠、无情、穿黑皮衣、步履沉重。记者并不**相信**未来真的会出现博格——他们不认为《星际迷航》是预言。但当有人提到脑机接口时，他们会下意识想：“未来会不会出现博格？”
而不是：“电脑辅助心灵感应真的会让人变得更冷酷吗？”
也不是：“我从没见过博格，其他人也没有。”
更不是：“我正在基于**零证据**形成一个种族刻板印象。”

正如乔治·奥威尔在谈到陈词滥调时所说：<sup>4</sup>

> 最重要的是，要让**意义来选择词语**，而不是反过来……当你在思考抽象事物时，你更倾向于一开始就使用语言，而除非你刻意去阻止它，否则既有的表达体系会蜂拥而入，代替你进行表达，却以牺牲你真正想说的含义为代价。

然而，在我看来，**使用其他作家想象力所带来的最大危害**，是它让人**不再使用自己的想象力**。正如 Robert Pirsig 所说：<sup>5</sup>

> 她写不出来，是因为她试图重复那些她以前听过的东西，就像第一天他也试图重复那些他早已打算说的内容一样。她想不出写关于博兹曼的东西，是因为她想不起来有什么值得重复的内容。她奇怪地没有意识到，她其实可以用自己的眼睛重新观察并写下所见，而不是首先考虑别人以前说过什么。

我们记住的虚构故事会蜂拥而入，**代替我们进行思考**；它们代替了“观察”本身——这是一种最致命的便利。

---

<sup>1</sup> Nick Bostrom, “Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards,” *Journal of Evolution and Technology* 9 (2002), [http://www.jetpress.org/volume9/risks.html](http://www.jetpress.org/volume9/risks.html)

<sup>2</sup> 例如：Hanson, “Biases of Science Fiction” (2006). [http://www.overcomingbias.com/2006/12/biases\_of\_scien.html](http://www.overcomingbias.com/2006/12/biases_of_scien.html)

<sup>3</sup> 参见本书中的《第三种选择》（The Third Alternative），以及《奥卡姆剃刀》（Occam’s Razor）和《繁琐细节》（Burdensome Details），收录于 *Map and Territory*。

<sup>4</sup> 乔治·奥威尔，《政治与英语语言》（Politics and the English Language）。

<sup>5</sup> 罗伯特·波西格（Robert Pirsig），《禅与摩托车维修艺术》（Zen and the Art of Motorcycle Maintenance）。

---

## The Logical Fallacy of Generalization from Fictional Evidence

by Eliezer Yudkowsky

When I try to introduce the subject of advanced AI, what’s the first thing I hear, more than half the time?

“Oh, you mean like the Terminator movies / The Matrix / Asimov’s robots!”

And I reply, “Well, no, not exactly. I try to avoid the logical fallacy of generalizing from fictional evidence.”

Some people get it right away, and laugh. Others defend their use of the example, disagreeing that it’s a fallacy.

What’s wrong with using movies or novels as starting points for the discussion? No one’s claiming that it’s true, after all. Where is the lie, where is the rationalist sin? Science fiction represents the author’s attempt to visualize the future; why not take advantage of the thinking that’s already been done on our behalf, instead of starting over?

Not every misstep in the precise dance of rationality consists of outright belief in a falsehood; there are subtler ways to go wrong.

First, let us dispose of the notion that science fiction represents a full-fledged rational attempt to forecast the future. Even the most diligent science fiction writers are, first and foremost, storytellers; the requirements of storytelling are not the same as the requirements of forecasting. As Nick Bostrom points out:<sup>1</sup>

> When was the last time you saw a movie about humankind suddenly going extinct (without warning and without being replaced by some other civilization)? While this scenario may be much more probable than a scenario in which human heroes successfully repel an invasion of monsters or robot warriors, it wouldn’t be much fun to watch.

So there are specific distortions in fiction.<sup>2</sup> But trying to correct for these specific distortions is not enough. A story is never a rational attempt at analysis, not even with the most diligent science fiction writers, because stories don’t use probability distributions. I illustrate as follows:

> Bob Merkelthud slid cautiously through the door of the alien spacecraft, glancing right and then left (or left and then right) to see whether any of the dreaded Space Monsters yet remained. At his side was the only weapon that had been found effective against the Space Monsters, a Space Sword forged of pure titanium with 30% probability, an ordinary iron crowbar with 20% probability, and a shimmering black discus found in the smoking ruins of Stonehenge with 45% probability, the remaining 5% being distributed over too many minor outcomes to list here.
> 
> Merklethud (though there’s a significant chance that Susan Wifflefoofer was there instead) took two steps forward or one step back, when a vast roar split the silence of the black airlock! Or the quiet background hum of the white airlock! Although Amfer and Woofi (1997) argue that Merklethud is devoured at this point, Spacklebackle (2003) points out that—

Characters can be ignorant, but the author can’t say the three magic words “I don’t know.” The protagonist must thread a single line through the future, full of the details that lend flesh to the story, from Wifflefoofer’s appropriately futuristic attitudes toward feminism, down to the color of her earrings.

Then all these burdensome details and questionable assumptions are wrapped up and given a short label, creating the illusion that they are a single package.<sup>3</sup>

On problems with large answer spaces, the greatest difficulty is not verifying the correct answer but simply locating it in answer space to begin with. If someone starts out by asking whether or not AIs are gonna put us into capsules like in The Matrix, they’re jumping to a 100-bit proposition, without a corresponding 98 bits of evidence to locate it in the answer space as a possibility worthy of explicit consideration. It would only take a handful more evidence after the first 98 bits to promote that possibility to near-certainty, which tells you something about where nearly all the work gets done.

The “preliminary” step of locating possibilities worthy of explicit consideration includes steps like: weighing what you know and don’t know, what you can and can’t predict; making a deliberate effort to avoid absurdity bias and widen confidence intervals; pondering which questions are the important ones, trying to adjust for possible Black Swans and think of (formerly) unknown unknowns. Jumping to “The Matrix: Yes or No?” skips over all of this.

Any professional negotiator knows that to control the terms of a debate is very nearly to control the outcome of the debate. If you start out by thinking of The Matrix, it brings to mind marching robot armies defeating humans after a long struggle—not a superintelligence snapping nanotechnological fingers. It focuses on an “Us vs. Them” struggle, directing attention to questions like “Who will win?” and “Who should win?” and “Will AIs really be like that?” It creates a general atmosphere of entertainment, of “What is your amazing vision of the future?”

Lost to the echoing emptiness are: considerations of more than one possible mind design that an “artificial intelligence” could implement; the future’s dependence on initial conditions; the power of smarter-than-human intelligence and the argument for its unpredictability; people taking the whole matter seriously and trying to do something about it.

If some insidious corrupter of debates decided that their preferred outcome would be best served by forcing discussants to start out by refuting Terminator, they would have done well in skewing the frame. Debating gun control, the NRA spokesperson does not wish to be introduced as a “shooting freak,” the anti-gun opponent does not wish to be introduced as a “victim disarmament advocate.” Why should you allow the same order of frame-skewing by Hollywood scriptwriters, even accidentally?

Journalists don’t tell me, “The future will be like 2001.” But they ask, “Will the future be like 2001, or will it be like A.I.?” This is just as huge a framing issue as asking, “Should we cut benefits for disabled veterans, or raise taxes on the rich?”

In the ancestral environment, there were no moving pictures; what you saw with your own eyes was true. A momentary glimpse of a single word can prime us and make compatible thoughts more available, with demonstrated strong influence on probability estimates. How much havoc do you think a two-hour movie can wreak on your judgment? It will be hard enough to undo the damage by deliberate concentration—why invite the vampire into your house? In Chess or Go, every wasted move is a loss; in rationality, any non-evidential influence is (on average) entropic.

Do movie-viewers succeed in unbelieving what they see? So far as I can tell, few movie viewers act as if they have directly observed Earth’s future. People who watched the Terminator movies didn’t hide in fallout shelters on August 29, 1997. But those who commit the fallacy seem to act as if they had seen the movie events occurring on some other planet; not Earth, but somewhere similar to Earth.

You say, “Suppose we build a very smart AI,” and they say, “But didn’t that lead to nuclear war in The Terminator?” As far as I can tell, it’s identical reasoning, down to the tone of voice, of someone who might say: “But didn’t that lead to nuclear war on Alpha Centauri?” or “Didn’t that lead to the fall of the Italian city-state of Piccolo in the fourteenth century?” The movie is not believed, but it is cognitively available. It is treated, not as a prophecy, but as an illustrative historical case. Will history repeat itself? Who knows?

In a recent intelligence explosion discussion, someone mentioned that Vinge didn’t seem to think that brain-computer interfaces would increase intelligence much, and cited Marooned in Realtime and Tunç Blumenthal, who was the most advanced traveller but didn’t seem all that powerful. I replied indignantly, “But Tunç lost most of his hardware! He was crippled!” And then I did a mental double-take and thought to myself: What the hell am I saying.

Does the issue not have to be argued in its own right, regardless of how Vinge depicted his characters? Tunç Blumenthal is not “crippled,” he’s unreal. I could say “Vinge chose to depict Tunç as crippled, for reasons that may or may not have had anything to do with his personal best forecast,” and that would give his authorial choice an appropriate weight of evidence. I cannot say “Tunç was crippled.” There is no was of Tunç Blumenthal.

I deliberately left in a mistake I made, in my first draft of the beginning of this essay: “Others defend their use of the example, disagreeing that it’s a fallacy.” But The Matrix is not an example!

A neighboring flaw is the logical fallacy of arguing from imaginary evidence: “Well, if you did go to the end of the rainbow, you would find a pot of gold—which just proves my point!” (Updating on evidence predicted, but not observed, is the mathematical mirror image of hindsight bias.)

The brain has many mechanisms for generalizing from observation, not just the availability heuristic. You see three zebras, you form the category “zebra,” and this category embodies an automatic perceptual inference. Horse-shaped creatures with white and black stripes are classified as “Zebras,” therefore they are fast and good to eat; they are expected to be similar to other zebras observed.

So people see (moving pictures of) three Borg, their brain automatically creates the category “Borg,” and they infer automatically that humans with brain-computer interfaces are of class “Borg” and will be similar to other Borg observed: cold, uncompassionate, dressing in black leather, walking with heavy mechanical steps. Journalists don’t believe that the future will contain Borg—they don’t believe Star Trek is a prophecy. But when someone talks about brain-computer interfaces, they think, “Will the future contain Borg?” Not, “How do I know computer-assisted telepathy makes people less nice?” Not, “I’ve never seen a Borg and never has anyone else.” Not, “I’m forming a racial stereotype based on literally zero evidence.”

As George Orwell said of cliches:<sup>4</sup>

> What is above all needed is to let the meaning choose the word, and not the other way around . . . When you think of something abstract you are more inclined to use words from the start, and unless you make a conscious effort to prevent it, the existing dialect will come rushing in and do the job for you, at the expense of blurring or even changing your meaning.

Yet in my estimation, the most damaging aspect of using other authors’ imaginations is that it stops people from using their own. As Robert Pirsig said:<sup>5</sup>

> She was blocked because she was trying to repeat, in her writing, things she had already heard, just as on the first day he had tried to repeat things he had already decided to say. She couldn’t think of anything to write about Bozeman because she couldn’t recall anything she had heard worth repeating. She was strangely unaware that she could look and see freshly for herself, as she wrote, without primary regard for what had been said before.

Remembered fictions rush in and do your thinking for you; they substitute for seeing—the deadliest convenience of all.

---

<sup>1</sup>Nick Bostrom, “Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards,” Journal of Evolution and Technology 9 (2002), http://www.jetpress.org/volume9/risks.html.

<sup>2</sup>E.g., Hanson’s (2006) “Biases of Science Fiction.” http://www.overcomingbias.com/2006/12/biases_of_scien.html.

<sup>3</sup>See “The Third Alternative” in this volume, and “Occam’s Razor” and “Burdensome Details” in Map and Territory.

<sup>4</sup>Orwell, “Politics and the English Language.”

<sup>5</sup>Pirsig, Zen and the Art of Motorcycle Maintenance.
以下是你提供的文章的中文翻译，保留了原始的链接：

---

## 我们真的会相信所有听到的事情吗？

作者：Eliezer Yudkowsky

早期一些关于锚定与调整（anchoring and adjustment）的实验测试了一个假设：通过让被试执行额外任务（比如在一串数字中寻找“5”）使其在认知上“忙碌”，是否会减少他们的调整能力，从而增加锚定效应的影响。大多数实验结果都支持这个观点：认知忙碌会加剧锚定效应，并更广泛地加剧信息污染。

随着越来越多实验结果的积累——尤其是认知忙碌加剧了污染效应这一发现——Daniel Gilbert 注意到一种非常惊人的模式正在浮现：**我们是不是会相信所有我们听到的事情？**

人们通常会认为，当我们听到一个命题时，我们首先会理解它的意思，然后思考它，最后才决定接受还是拒绝它。这种看似显而易见的认知流程模型最早可以追溯到笛卡尔（Descartes）。但他的对手斯宾诺莎（Spinoza）却不这么认为；斯宾诺莎提出，在理解一个命题的过程中，我们首先是被动地接受它，然后才主动否定那些经思考后被拒绝的命题。

几个世纪以来，大多数哲学家都站在笛卡尔这边，因为他的观点似乎更符合逻辑、更直观。<sup>1</sup> 但 Gilbert 找到了一个实验方法来检验笛卡尔和斯宾诺莎的假设。

如果笛卡尔是对的，那么让被试分心应该会干扰他们对真假陈述的判断，不管是真的还是假的。而如果斯宾诺莎是对的，那么认知分心应该只会使人们将假话记成真话，但不会让人把真话记成假话。

Gilbert、Krull 和 Malone 的实验验证了后者的观点。在他们的实验中，被试面对的是被标注为“真”或“假”的新命题。结果显示，分心对于识别真命题没有影响（不中断时识别率为55%，中断时为58%）；但对识别假命题有显著影响（不中断时识别率为55%，中断时降至35%）。<sup>2</sup>

后续由 [Gilbert、Tafarodi 和 Malone](http://www.danielgilbert.com/Gilbert%20et%20al%20EVERYTHING%20YOU%20READ.pdf) 完成的实验更是提供了戏剧性得多的例证。被试朗读显示在视频屏幕上的犯罪报告，其中文字颜色指示陈述的真假。有些报告包含了加重罪行严重性的虚假陈述，而另一些报告则包含了减轻罪责的虚假陈述。同时，有些被试还被要求在阅读过程中留意数字串中是否出现“5”——这就是用来制造认知忙碌的任务。最后，被试需要为每位罪犯建议一个0到20年之间的刑期。

在认知忙碌条件下，对于那些报告中包含加重罪行虚假陈述的罪犯，被试平均建议刑期为 **11.15 年**；而对于那些报告中包含减轻罪责虚假陈述的罪犯，建议刑期则为 **5.83 年**。这一接近两倍的差异显然具有统计显著性。

而在不认知忙碌条件下，被试阅读了完全相同的报告、标签和数字串，不同之处只是他们不需要去寻找“5”，因此可以将更多注意力放在对“假”标签的否定上。这些不忙碌的被试对两类罪犯分别建议 **7.03 年** 和 **6.03 年** 的刑期。

Gilbert、Tafarodi 和 Malone 的论文标题是：《你无法不相信你读到的所有东西》（"You Can’t Not Believe Everything You Read"）。

这至少表明了一件事：**当我们接触不可靠信息时，尤其是在我们分心的情况下，应该更加小心。** 当你在超市扫一眼报纸头版时，请务必谨慎。

附注：根据我刚刚编造的一则未经证实的谣言，由于你在阅读这篇文章时被颜色变化分散了注意力，所以你对它会变得不那么怀疑。

---

<sup>1</sup>参见 Robin Hanson 的博客文章 “[Policy Tug-O-War](http://www.overcomingbias.com/2007/05/policy_tugowar.html),” Overcoming Bias (2007)。

<sup>2</sup>Daniel T. Gilbert, Douglas S. Krull, and Patrick S. Malone, “Unbelieving the Unbelievable: Some Problems in the Rejection of False Information,” *Journal of Personality and Social Psychology* 59(4), 1990: 601–613。

<sup>3</sup>Daniel T. Gilbert, Romin W. Tafarodi, and Patrick S. Malone, “You Can’t Not Believe Everything You Read,” *Journal of Personality and Social Psychology* 65(2), 1993: 221–233。

---

## Do We Believe Everything We’re Told?

by Eliezer Yudkowsky

Some early experiments on anchoring and adjustment tested whether distracting the subjects—rendering subjects cognitively “busy” by asking them to keep a lookout for “5” in strings of numbers, or some such—would decrease adjustment, and hence increase the influence of anchors. Most of the experiments seemed to bear out the idea that being cognitive busy increased anchoring, and more generally contamination.

Looking over the accumulating experimental results—more and more findings of contamination, exacerbated by cognitive busyness—Daniel Gilbert saw a truly crazy pattern emerging: Do we believe everything we’re told?

One might naturally think that on being told a proposition, we would first comprehend what the proposition meant, then consider the proposition, and finally accept or reject it. This obvious-seeming model of cognitive process flow dates back to Descartes. But Descartes’s rival, Spinoza, disagreed; Spinoza suggested that we first passively accept a proposition in the course of comprehending it, and only afterward actively disbelieve propositions which are rejected by consideration.

Over the last few centuries, philosophers pretty much went along with Descartes, since his view seemed more, y’know, logical and intuitive.<sup>1</sup> But Gilbert saw a way of testing Descartes’s and Spinoza’s hypotheses experimentally.

If Descartes is right, then distracting subjects should interfere with both accepting true statements and rejecting false statements. If Spinoza is right, then distracting subjects should cause them to remember false statements as being true, but should not cause them to remember true statements as being false.

Gilbert, Krull, and Malone bear out this result, showing that, among subjects presented with novel statements labeled true or false, distraction had no effect on identifying true propositions (55% success for uninterrupted presentations, vs. 58% when interrupted); but did affect identifying false propositions (55% success when uninterrupted, vs. 35% when interrupted).<sup>2</sup>

A much more dramatic illustration was produced in followup experiments by [Gilbert, Tafarodi, and Malone](http://www.danielgilbert.com/Gilbert%20et%20al%20EVERYTHING%20YOU%20READ.pdf).3 Subjects read aloud crime reports crawling across a video monitor, in which the color of the text indicated whether a particular statement was true or false. Some reports contained false statements that exacerbated the severity of the crime, other reports contained false statements that extenuated (excused) the crime. Some subjects also had to pay attention to strings of digits, looking for a “5,” while reading the crime reports—this being the distraction task to create cognitive busyness. Finally, subjects had to recommend the length of prison terms for each criminal, from 0 to 20 years.

Subjects in the cognitively busy condition recommended an average of 11.15 years in prison for criminals in the “exacerbating” condition, that is, criminals whose reports contained labeled false statements exacerbating the severity of the crime. Busy subjects recommended an average of 5.83 years in prison for criminals whose reports contained labeled false statements excusing the crime. This nearly twofold difference was, as you might suspect, statistically significant.

Non-busy participants read exactly the same reports, with the same labels, and the same strings of numbers occasionally crawling past, except that they did not have to search for the number “5.” Thus, they could devote more attention to “unbelieving” statements labeled false. These non-busy participants recommended 7.03 years versus 6.03 years for criminals whose reports falsely exacerbated or falsely excused.

Gilbert, Tafarodi, and Malone’s paper was entitled “You Can’t Not Believe Everything You Read.”

This suggests—to say the very least—that we should be more careful when we expose ourselves to unreliable information, especially if we’re doing something else at the time. Be careful when you glance at that newspaper in the supermarket.

PS: According to an unverified rumor I just made up, people will be less skeptical of this essay because of the distracting color changes.

---

<sup>1</sup>See Robin Hanson, “Policy Tug-O-War,” Overcoming Bias (blog), 2007, http://www.overcomingbias.com/2007/05/policy_tugowar.html.

<sup>2</sup>Daniel T. Gilbert, Douglas S. Krull, and Patrick S. Malone, “Unbelieving the Unbelievable: Some Problems in the Rejection of False Information,” Journal of Personality and Social Psychology 59 (4 1990): 601–613.

<sup>3</sup>Daniel T. Gilbert, Romin W. Tafarodi, and Patrick S. Malone, “You Can’t Not Believe Everything You Read,” Journal of Personality and Social Psychology 65 (2 1993): 221–233.
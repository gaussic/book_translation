## 邪教式的反邪教焦虑

在现代社会，加入邪教大概是你能遇到的最糟糕的事情之一。最好的情况是，你加入了一个真诚但被误导的群体，大家只是犯了个诚实的错误，行为也还算正常，你会花费大量时间和金钱，最后一无所获。其实，这也可以描述任何一个失败的硅谷创业公司——据说那也是一段极其痛苦的经历。所以，是的，非常可怕。

真正的邪教远比这糟糕得多。“爱轰炸”式的招募手法，专门针对正经历人生危机的人。剥夺睡眠。强制劳动带来的极度疲惫。把新成员隔离到偏远公社，远离亲友。每天开会忏悔“不纯洁的思想”。邪教把新成员的钱——包括全部积蓄和每周工资——都拿走，让他们只能依赖邪教提供食物和衣物。违抗就用饥饿惩罚。不折不扣的洗脑和严重伤害。

考虑到这些，我本该对那些因为要参与看起来有点奇怪的活动而极度紧张、担心自己会不会加入邪教的人多些同情。可实际上，这种紧张还是让我很烦。

第一点：“邪教”和“非邪教”并不像狗和猫那样是泾渭分明的自然种类。你随便找一份邪教特征清单，都会发现其中很多条同样适用于政党和公司——比如“鼓励成员怀疑外部批评的动机”“等级权威结构”。我写过很多关于群体失败模式的文章，比如群体极化、幸福死亡螺旋、缺乏批判性、蒸发冷却等，这些模式彼此强化。当这些失败模式交织在一起时，会形成一种比各部分都更愚蠢的“超级失败体”，就像五狮合体一样。但这不是邪教的本质，只是邪教的吸引子。

狗天生有狗的DNA，猫天生有猫的DNA。现实世界里没有“狗猫混合体”。（即使用基因工程，也不是简单地把狗猫基因各取一半就能造出半狗半猫的生物。）也不存在一套互相强化的“狗特征”，让一只猫能逐步变成“半狗”。

人类思维在分类时，似乎更喜欢“本质”而不是“吸引子”。人们总想说“这是邪教”或“这不是邪教”，然后分类任务就算完成了。比如你观察到苏格拉底有十根手指、穿衣服、会说希腊语，于是你说“苏格拉底是人类”，进而推断“苏格拉底会被毒死”，而不用专门验血来确认他会死。你一旦决定了苏格拉底是人类，这个结论就不会变。

但如果你观察到某个群体表现出内外群体极化、对“最爱之物”有光环效应（无论是客观主义、素食主义还是神经网络），你并不能据此推断他们已经丧失批判性。你无法判断他们的核心观点是真的、假的，还是确实有用但没他们想的那么有用。你也无法判断他们是否礼貌，或者会不会把你骗到孤岛上让你挨饿。邪教特征不是全有或全无的。

网络上关于“X是不是邪教”的争论，往往是一方找到一条适用的邪教特征就说“所以这是邪教！”，另一方找到一条不适用的就说“所以这不是邪教！”

用这种“本质主义”思维，你永远无法准确把握一个群体的推理动态。你必须逐条单独分析每个特征。

此外，“反向愚蠢不是智慧”。如果你关心的是核心思想，而不仅仅是执行这个思想的群体，那么聪明的思想也可能有愚蠢的追随者。很多新时代人士谈论“量子物理”，但这并不能说明量子物理本身有问题。<sup>1</sup> 本质主义还会带来另一个误区：一旦你认定某群体是“邪教”，就会觉得他们的信仰一定是错的，因为“错误信仰”是邪教的特征，就像猫有毛一样。如果你关心的是思想本身，就应该关注思想，而不是人。邪教性更多是群体的特征，而不是假说的特征。

第二个错误是：当人们紧张地问“这不是邪教吧？”时，在我听来，他们其实是在寻求理性的安慰。理性主义者不该太执着于“理性主义者”这个自我形象，这本身值得单独写一篇文章。<sup>2</sup> 但即使不展开，大家也能看出，紧张地寻求安慰绝不是评估理性问题的最佳心态。你不会真正好奇，也不会想办法解决自己的疑虑。你只会在网上找到一条“邪教会用剥夺睡眠控制人”，然后发现你喜欢的群体没有剥夺睡眠，于是松口气：“不是邪教，呼！”如果没长毛就不是猫，真让人安心。

但每一个事业都想变成邪教，无论这个事业本身多么明智或愚蠢。内外群体对立等现象是人性的一部分，不是什么变异的诅咒。理性只是例外，不是常态。你必须持续努力，才能抵抗理性滑向熵增的自然趋势。如果你一旦认定“这不是邪教！”然后松了口气，就不会再持续努力抵抗邪教化的倾向。你会觉得“邪教本质”不存在，于是停止了对抗邪教吸引子的“抽水”工作。

如果你对邪教化极度紧张，就会想否认任何类似邪教的迹象。但任何目标被正面看待的群体都容易出现光环效应，都需要持续努力才能避免陷入情感死亡螺旋。这对政党等普通机构同样适用——比如有人觉得“自由主义价值观”或“保守主义价值观”能治愈癌症等等。硅谷创业公司，无论成败，也一样。光环效应不会因为大家都这样就变得合理；如果大家都跳悬崖，你也不会跟着跳。推理错误必须被抵制，而不是容忍。但如果你太紧张于“你确定这不是邪教吗？”，你就会不愿承认任何邪教化的迹象，因为那意味着你身处邪教，“可这不是邪教啊！！”于是你就看不到当前那些普通邪教化倾向正在蔓延或被抵制的“战场”。

第三个错误是，我强烈怀疑，大家紧张地问“这不是邪教吧？”其实完全是出于错误的原因。

为什么那些把“幸福之物”吹上天、鼓励成员捐光所有钱、让大家自愿做苦力、把成员关在私密公社里的人，过了几百年就被叫做“宗教”而不是“邪教”？

为什么大多数在面对人体冷冻时紧张地问“这不是邪教吧？”的人，去参加共和党或民主党集会时却不会同样紧张？内外群体对立和幸福死亡螺旋在政治讨论、主流宗教、体育粉丝圈都可能发生。如果紧张真是出于对理性错误的担忧，人们就应该用同样紧张的语气问：“这不是内外群体对立吧？”来质疑民主党或共和党集会。

当然，对自由意志主义的担忧比对飞碟邪教要小是有道理的，因为自由意志主义者并不以剥夺睡眠来招募成员。但人体冷冻支持者也没有这种名声。那为什么你对死后冷冻头颅会更担心呢？

我怀疑，这种紧张并不是害怕信错，也不是害怕身体伤害，而是害怕孤独的异见。就像阿希从众实验中，所有人（其实是实验同伙）都说C和X一样长，而你觉得B和X才一样长时，那种离开“队伍”的紧张感。

这就是为什么那些信仰已经存在很久、看起来“正常”的群体不会让人像“邪教”那样紧张，尽管有些主流宗教也会让你捐光钱、送你去修道院。政党等极易犯理性错误的群体，也不会让人像“邪教”那样紧张。“邪教”这个词并不是用来象征理性错误的，而是用来贴标签，指代那些“看起来怪异”的东西。

不是所有改变都是进步，但所有进步必然意味着改变。你想做得更好，就不得不做得不同。常识中确实蕴含着不少真正的智慧；对“怪异”要求更高的证据门槛是合理的。但这种紧张并不是那种深思熟虑的理性考量，而是害怕相信某些东西会让朋友觉得你很怪。所以人们会用一种绝不会用在政治集会或圣诞大装饰上的语气问：“这不是邪教吧？”

这才是让我最烦的地方。

仿佛只要你相信了祖先没信过的东西，“邪教仙女”就会从天而降，把“邪教本质”注入你体内，然后你就会穿上长袍开始吟唱。好像“怪异信仰”才是问题的直接原因，至于剥夺睡眠和殴打都无所谓。邪教造成的伤害——比如天堂之门集体自杀——只是用来证明所有有怪异信仰的人都是疯子；“邪教成员”最重要的特征就是他们是“怪人”。

没错，社会上不寻常的信仰确实让群体更容易出现内外群体对立、蒸发冷却等问题。但“怪异”只是风险因素，不是疾病本身。你觉得值得追求的目标也一样。无论信仰真假，只要目标美好，就容易陷入幸福死亡螺旋。但这只是风险因素，不是疾病。有些目标确实值得追求。<sup>3</sup>

第四个问题：对孤独异见的恐惧正是邪教本身利用的。害怕朋友用异样眼光看你，正是邪教用来招募和留住成员的手段——让新成员被邪教信徒的高度一致包围。

对怪异想法的恐惧、从众冲动，确实让很多潜在受害者远离了飞碟邪教。你在圈外时，这种恐惧能让你远离邪教；但一旦进了圈，这种恐惧又会让你留在里面。从众只会把你牢牢粘在原地，无论那是好地方还是坏地方。

人们总希望有办法能确定自己不是“邪教”成员。有个能让别人闭嘴的终极反击。有种方法能让你彻底确定自己做的是对的，不再有这些挥之不去的疑虑。这就是所谓的“封闭需求”。而——当然——邪教也会利用这一点。

这就是“邪教式反邪教焦虑”这个说法的由来。

带着疑虑生活不是美德——每个疑虑的目的都是要么被证实要么被消灭，悬而未决的疑虑毫无意义。但有时候，疑虑确实需要时间才能消除。理性主义者必须学会带着一堆暂时无法解决的疑虑生活。疑虑本不该让人害怕。否则你只能在“如履薄冰”或“愚蠢至极”之间二选一。

如果你真的、确实无法判断某个群体是不是“邪教”，那你只能在不确定的情况下做选择。这正是决策理论的意义所在。

第五个问题：缺乏战略思维。

我认识一些人，他们对“智能爆炸”“超级智能AI”等想法很谨慎，对政党和主流宗教也很谨慎。是谨慎，不是紧张或防御。这些人一眼就能看出，奇点类想法目前还不是那种有剥夺睡眠等手段的“完全体邪教”。但他们担心未来会变成邪教，比如把强大AI的概念变成“超级幸福代理人”（即只要你说它好，它就同意的那种代理人）。现在不是邪教，不代表以后不会变。邪教化是吸引子，不是本质。

这种谨慎让我烦吗？一点也不。我自己也经常担心这种情况。我会提前布局，防止事情朝那个方向发展。<sup>4</sup>

谈论“理性”的人也有额外的风险因素。教别人怎么思考本身就是危险的事。但这只是风险因素，不是疾病。

我最喜欢的两个事业都面临邪教化风险。但每当我谈论强大AI时，被问“你确定这不是邪教吗？”的频率远高于我谈概率论和认知科学时。我不知道哪个风险因素更高，但我知道哪个听起来更怪……

第六个问题：“这不是邪教吧？”这个问题本身就让我陷入一种很烦人的“第22条军规”。如果我是个真正的邪恶大师，肯定会利用你的紧张，设计一套看似合理的复杂论证来解释“为什么这不是邪教”，而你也会很乐意接受。有时候我觉得大家就是想让我这么做！每当我试图写关于邪教化及其防范的文章时，总觉得自己在迎合这种错误的需求——最终还是在给人们提供安慰。即使我反复强调需要持续与熵作斗争。

这让我感觉自己成了阿希从众实验中的第一个异见者，对大家说：“是的，X线真的和B线一样，你也可以这么说。”其实大家根本不该问！更糟的是，这让我像是在为“为什么这不是邪教”做复杂辩护。这是个错误的问题。

你只需要自己观察群体的推理过程，摆脱对“怪异”的恐惧后，自己决定要不要加入。无论你身处哪个群体，抵制邪教化思维都是你自己的责任。

邪教靠群体思维、紧张和寻求安慰为生。你不能靠意愿消除紧张，虚假的自信更糟。但只要有人需要安慰——哪怕只是“我真的是理性主义者吗”的安慰——这就是他们盔甲上的破绽。一个高明的剑客只专注于目标，不会分心去看别人是不是在笑。当你知道自己想做什么、为什么做，你就会知道自己有没有做到，也会知道一个群体是在帮你还是在拖你后腿。<sup>5</sup>

---

<sup>1</sup>当然，愚蠢的想法也会有愚蠢的追随者。

<sup>2</sup>参见两个“邪教公案”、《为何追求真理？》（见《地图与领地》）和《理性的十二美德》（http://www.lesswrong.com/rationality/the-twelve-virtues-of-rationality）。

<sup>3</sup>另一方面，我认为没有任何理由可以为剥夺睡眠或用殴打威胁异见者开脱。只要一个群体做了这些事，不管你叫它“邪教”还是“非邪教”，你都已经直接回答了“要不要加入”的实际问题。

<sup>4</sup>比如我写过一系列关于邪教化推理失败的文章，就是为了提前防范。

<sup>5</sup>附言：如果有人来问你“你确定这不是邪教吗？”，千万别试图一口气解释完这些概念。你低估了推理距离。对方会说：“啊哈，你承认你是邪教了！”或者“等等，你是说我不用担心加入邪教？”或者“所以……害怕邪教本身就是邪教化？这听起来太邪教了。”所以最后一个烦恼——如果你在数的话就是第七个——就是这些内容实在太长，没法一次讲清楚。

---

## Cultish Countercultishness

In the modern world, joining a cult is probably one of the worse things that can happen to you. The best-case scenario is that you’ll end up in a group of sincere but deluded people, making an honest mistake but otherwise well-behaved, and you’ll spend a lot of time and money but end up with nothing to show. Actually, that could describe any failed Silicon Valley startup. Which is supposed to be a hell of a harrowing experience, come to think. So yes, very scary.

Real cults are vastly worse. “Love bombing” as a recruitment technique, targeted at people going through a personal crisis. Sleep deprivation. Induced fatigue from hard labor. Distant communes to isolate the recruit from friends and family. Daily meetings to confess impure thoughts. It’s not unusual for cults to take all the recruit’s money—life savings plus weekly paycheck—forcing them to depend on the cult for food and clothing. Starvation as a punishment for disobedience. Serious brainwashing and serious harm.

With all that taken into account, I should probably sympathize more with people who are terribly nervous, embarking on some odd-seeming endeavor, that they might be joining a cult. It should not grate on my nerves. Which it does.

Point one: “Cults” and “non-cults” aren’t separated natural kinds like dogs and cats. If you look at any list of cult characteristics, you’ll see items that could easily describe political parties and corporations—“group members encouraged to distrust outside criticism as having hidden motives,” “hierarchical authoritative structure.” I’ve written on group failure modes like group polarization, happy death spirals, uncriticality, and evaporative cooling, all of which seem to feed on each other. When these failures swirl together and meet, they combine to form a Super-Failure stupider than any of the parts, like Voltron. But this is not a cult essence; it is a cult attractor.

Dogs are born with dog DNA, and cats are born with cat DNA. In the current world, there is no in-between. (Even with genetic manipulation, it wouldn’t be as simple as creating an organism with half dog genes and half cat genes.) It’s not like there’s a mutually reinforcing set of dogcharacteristics, which an individual cat can wander halfway into and become a semidog.

The human mind, as it thinks about categories, seems to prefer essences to attractors. The one wishes to say, “It is a cult,” or, “It is not a cult,” and then the task of classification is over and done. If you observe that Socrates has ten fingers, wears clothes, and speaks fluent Greek, then you can say, “Socrates is human,” and from there deduce, “Socrates is vulnerable to hemlock,” without doing specific blood tests to confirm his mortality. You have decided Socrates’s humanness once and for all.

But if you observe that a certain group of people seems to exhibit ingroup-outgroup polarization and see a positive halo effect around their Favorite Thing Ever—which could be Objectivism, or vegetarianism, or neural networks—you cannot, from the evidence gathered so far, deduce whether they have achieved uncriticality. You cannot deduce whether their main idea is true, or false, or genuinely useful but not quite as useful as they think. From the information gathered so far, you cannot deduce whether they are otherwise polite, or if they will lure you into isolation and deprive you of sleep and food. The characteristics of cultness are not all present or all absent.

If you look at online arguments over “X is a cult,” “X is not a cult,” then one side goes through an online list of cult characteristics and finds one that applies and says, “Therefore it is a cult!” And the defender finds a characteristic that does not apply and says, “Therefore it is not a cult!”

You cannot build up an accurate picture of a group’s reasoning dynamic using this kind of essentialism. You’ve got to pay attention to individual characteristics individually.

Furthermore, reversed stupidity is not intelligence. If you’re interested in the central idea, not just the implementation group, then smart ideas can have stupid followers. Lots of New Agers talk about “quantum physics,” but this is no strike against quantum physics.<sup>1</sup> Along with binary essentialism goes the idea that if you infer that a group is a “cult,” therefore their beliefs must be false, because false beliefs are characteristic of cults, just like cats have fur. If you’re interested in the idea, then look at the idea, not the people. Cultishness is a characteristic of groups more than hypotheses.

The second error is that when people nervously ask, “This isn’t a cult, is it?” it sounds to me like they’re seeking reassurance of rationality. The notion of a rationalist not getting too attached to their self-image as a rationalist deserves its own essay.<sup>2</sup> But even without going into detail, surely one can see that nervously seeking reassurance is not the best frame of mind in which to evaluate questions of rationality. You will not be genuinely curious or think of ways to fulfill your doubts. Instead, you’ll find some online source which says that cults use sleep deprivation to control people, you’ll notice that Your-Favorite-Group doesn’t use sleep deprivation, and you’ll conclude, “It’s not a cult. Whew!” If it doesn’t have fur, it must not be a cat. Very reassuring.

But every cause wants to be a cult, whether the cause itself is wise or foolish. The ingroup-outgroup dichotomy, etc., are part of human nature, not a special curse of mutants. Rationality is the exception, not the rule. You have to put forth a constant effort to maintain rationality against the natural slide into entropy. If you decide, “It’s not a cult!” and sigh with relief, then you will not put forth a continuing effort to push back ordinary tendencies toward cultishness. You’ll decide the cult-essence is absent, and stop pumping against the entropy of the cult attractor.

If you are terribly nervous about cultishness, then you will want to deny any hint of any characteristic that resembles a cult. But any group with a goal seen in a positive light is at risk for the halo effect, and will have to pump against entropy to avoid an affective death spiral. This is true even for ordinary institutions like political parties—people who think that “liberal values” or “conservative values” can cure cancer, etc. It is true for Silicon Valley startups, both failed and successful. It is true of Mac users and of Linux users. The halo effect doesn’t become okay just because everyone does it; if everyone walks off a cliff, you wouldn’t too. The error in reasoning is to be fought, not tolerated. But if you’re too nervous about, “Are you sure this isn’t a cult?” then you will be reluctant to see any sign of cultishness, because that would imply you’re in a cult, and It’s not a cult!! So you won’t see the current battlefields where the ordinary tendencies toward cultishness are creeping forward, or being pushed back.

The third mistake in nervously asking, “This isn’t a cult, is it?” is that, I strongly suspect, the nervousness is there for entirely the wrong reasons.

Why is it that groups which praise their Happy Thing to the stars, encourage members to donate all their money and work in voluntary servitude, and run private compounds in which members are kept tightly secluded, are called “religions” rather than “cults” once they’ve been around for a few hundred years?

Why is it that most of the people who nervously ask of cryonics, “This isn’t a cult, is it?” would not be equally nervous about attending a Republican or Democratic political rally? Ingroup-outgroup dichotomies and happy death spirals can happen in political discussion, in mainstream religions, in sports fandom. If the nervousness came from fear of rationality errors, people would ask, “This isn’t an ingroup-outgroup dichotomy, is it?” about Democratic or Republican political rallies, in just the same fearful tones.

There’s a legitimate reason to be less fearful of Libertarianism than of a flying-saucer cult, because Libertarians don’t have a reputation for employing sleep deprivation to convert people. But cryonicists don’t have a reputation for using sleep deprivation, either. So why be any more worried about having your head frozen after you stop breathing?

I suspect that the nervousness is not the fear of believing falsely, or the fear of physical harm. It is the fear of lonely dissent. The nervous feeling that subjects get in Asch’s conformity experiment, when all the other subjects (actually confederates) say one after another that line C is the same size as line X, and it looks to the subject like line B is the same size as line X. The fear of leaving the pack.

That’s why groups whose beliefs have been around long enough to seem “normal” don’t inspire the same nervousness as “cults,” though some mainstream religions may also take all your money and send you to a monastery. It’s why groups like political parties, that are strongly liable for rationality errors, don’t inspire the same nervousness as “cults.” The word “cult” isn’t being used to symbolize rationality errors; it’s being used as a label for something that seems weird.

Not every change is an improvement, but every improvement is necessarily a change. That which you want to do better, you have no choice but to do differently. Common wisdom does embody a fair amount of, well, actual wisdom; yes, it makes sense to require an extra burden of proof for weirdness. But the nervousness isn’t that kind of deliberate, rational consideration. It’s the fear of believing something that will make your friends look at you really oddly. And so people ask, “This isn’t a cult, is it?” in a tone that they would never use for attending a political rally, or for putting up a gigantic Christmas display.

That’s the part that bugs me.

It’s as if, as soon as you believe anything that your ancestors did not believe, the Cult Fairy comes down from the sky and infuses you with the Essence of Cultness, and the next thing you know, you’re all wearing robes and chanting. As if “weird” beliefs are the direct cause of the problems, never mind the sleep deprivation and beatings. The harm done by cults—the Heaven’s Gate suicide and so on—just goes to show that everyone with an odd belief is crazy; the first and foremost characteristic of “cult members” is that they are Outsiders with Peculiar Ways.

Yes, socially unusual belief puts a group at risk for ingroup-outgroup thinking and evaporative cooling and other problems. But the unusualness is a risk factor, not a disease in itself. Same thing with having a goal that you think is worth accomplishing. Whether or not the belief is true, having a nice goal always puts you at risk of the happy death spiral. But that makes lofty goals a risk factor, not a disease. Some goals are genuinely worth pursuing.<sup>3</sup>

Problem four: The fear of lonely dissent is something that cults themselves exploit. Being afraid of your friends looking at you disapprovingly is exactly the effect that real cults use to convert and keep members—surrounding converts with wall-to-wall agreement among cult believers.

The fear of strange ideas, the impulse to conformity, has no doubt warned many potential victims away from flying saucer cults. When you’re out, it keeps you out. But when you’re in, it keeps you in. Conformity just glues you to wherever you are, whether that’s a good place or a bad place.

The one wishes there was some way they could be sure that they weren’t in a “cult.” Some definite, crushing rejoinder to people who looked at them funny. Some way they could know once and for all that they were doing the right thing, without these constant doubts. I believe that’s called “need for closure.” And—of course—cults exploit that, too.

Hence the phrase “cultish countercultishness.”

Living with doubt is not a virtue—the purpose of every doubt is to annihilate itself in success or failure, and a doubt that just hangs around accomplishes nothing. But sometimes a doubt does take a while to annihilate itself. Living with a stack of currently unresolved doubts is an unavoidable fact of life for rationalists. Doubt shouldn’t be scary. Otherwise you’re going to have to choose between living one heck of a hunted life, or one heck of a stupid one.

If you really, genuinely can’t figure out whether a group is a “cult,” then you’ll just have to choose under conditions of uncertainty. That’s what decision theory is all about.

Problem five: Lack of strategic thinking.

I know people who are cautious around ideas like intelligence explosion and superintelligent AI, and they’re also cautious around political parties and mainstream religions. Cautious, not nervous or defensive. These people can see at a glance that singularity-ish ideas aren’t currently the nucleus of a full-blown cult with sleep deprivation, etc. But they worry that it will become a cult, because of risk factors like turning the concept of a powerful AI into a Super Happy Agent (an agent defined primarily by agreeing with any nice thing said about it). Just because something isn’t a cult now doesn’t mean it won’t become a cult in the future. Cultishness is an attractor, not an essence.

Does this kind of caution annoy me? Hell no. I spend a lot of time worrying about that scenario myself. I try to place my Go stones in advance to block movement in that direction.<sup>4</sup>

People who talk about “rationality” also have an added risk factor. Giving people advice about how to think is an inherently dangerous business. But it is a risk factor, not a disease.

Both of my favorite Causes are at-risk for cultishness. Yet somehow I get asked, “Are you sure this isn’t a cult?” a lot more often when I talk about powerful AIs than when I talk about probability theory and cognitive science. I don’t know if one risk factor is higher than the other, but I know which one sounds weirder . . .

Problem #6 with asking, “This isn’t a cult, is it?” . . .

Just the question itself places me in a very annoying sort of Catch-22. An actual Evil Guru would surely use the one’s nervousness against them, and design a plausible elaborate argument explaining Why This Is Not A Cult, and the one would be eager to accept it. Sometimes I get the impression that this is what people want me to do! Whenever I try to write about cultishness and how to avoid it, I keep feeling like I’m giving in to that flawed desire—that I am, in the end, providing people with reassurance. Even when I tell people that a constant fight against entropy is required.

It feels like I’m making myself a first dissenter in Asch’s conformity experiment, telling people, “Yes, line X really is the same as line B, it’s okay for you to say so too.” They shouldn’t need to ask! Or, even worse, it feels like I’m presenting an elaborate argument for Why This Is Not A Cult. It’s a wrong question.

Just look at the group’s reasoning processes for yourself, and decide for yourself whether it’s something you want to be part of, once you get rid of the fear of weirdness. It is your own responsibility to stop yourself from thinking cultishly, no matter which group you currently happen to be operating in.

Cults feed on groupthink, nervousness, desire for reassurance. You cannot make nervousness go away by wishing, and false self-confidence is even worse. But so long as someone needs reassurance—even reassurance about being a rationalist—that will always be a flaw in their armor. A skillful swordsman focuses on the target, rather than glancing away to see if anyone might be laughing. When you know what you’re trying to do and why, you’ll know whether you’re getting it done or not, and whether a group is helping you or hindering you.<sup>5</sup>

---

<sup>1</sup>Of course, stupid ideas can also have stupid followers.

<sup>2</sup>Though see the two cult koans, “Why Truth?” (in Map and Territory), and “The Twelve Virtues of Rationality” (http://www.lesswrong.com/rationality/the-twelve-virtues-of-rationality).

<sup>3</sup>On the other hand, I see no legitimate reason for sleep deprivation or threatening dissenters with beating, full stop. When a group does this, then whether you call it “cult” or “not-cult,” you have directly answered the pragmatic question of whether to join.

<sup>4</sup>Hence, for example, the series of essays on cultish failures of reasoning.

<sup>5</sup>PS: If the one comes to you and says, “Are you sure this isn’t a cult?” don’t try to explain all these concepts in one breath. You’re underestimating inferential distances. The one will say, “Aha, so you’re admitting you’re a cult!” or, “Wait, you’re saying I shouldn’t worry about joining cults?” or, “So . . . the fear of cults is cultish? That sounds awfully cultish to me.” So the last annoyance factor—#7 if you’re keeping count—is that all of this is such a long story to explain.
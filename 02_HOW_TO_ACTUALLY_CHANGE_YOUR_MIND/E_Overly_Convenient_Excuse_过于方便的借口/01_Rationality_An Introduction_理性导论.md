## 理性：导论

作者：Rob Bensinger

1951年秋季，达特茅斯大学和普林斯顿大学之间的一场橄榄球比赛变得异常激烈。两位心理学家——达特茅斯大学的Albert Hastorf和普林斯顿大学的Hadley Cantril，决定询问两校学生，哪支队伍先发动了激烈的比赛。几乎所有人都一致认为普林斯顿队没有先发动暴力行为；但86%的普林斯顿学生认为是达特茅斯队先开始的，而只有36%的达特茅斯学生责怪自己队伍。（大多数达特茅斯学生认为“双方都有责任”。）

当后来播放比赛录像，并要求学生们计数他们看到的犯规行为时，达特茅斯的学生声称看到平均有4.3次犯规（其中一半被认为是“轻微”犯规），而普林斯顿的学生声称看到平均9.8次达特茅斯队犯规（其中三分之一被认为是“轻微”犯规）。<sup>1</sup>

当我们所珍视的事物受到威胁时——无论是我们的世界观、内圈、社会地位，还是其他任何我们关心的事物——我们的思维和感知便会立刻组织起来进行自卫。<sup>2</sup>,<sup>3</sup> 一些心理学家甚至假设，人类之所以能提出明确的理由来为自己的结论辩护，是为了帮助我们在争论中获胜。<sup>4</sup>

20世纪心理学的一个基本观点是，人类行为常常受到复杂无意识过程的驱动，而我们为自己的动机和理由编织的故事，比我们意识到的更具偏见和虚构成分。事实上，我们常常没有意识到自己在讲故事。当我们似乎在内省时“直接感知”到关于自己的东西时，通常这些感知依赖的是脆弱的隐性因果模型。<sup>5</sup>,<sup>6</sup> 当我们试图为自己的信念辩护时，我们往往会构造出不靠谱的推理，这与我们最初形成这些信念的过程毫无关系。<sup>7</sup> 我们往往不是根据解释的预测能力来判断其正确性，而是根据解释的心理吸引力来判断其可信度。

我们如何做得更好？在我们如此容易进行合理化的时候，如何得到一个现实的世界观？在我们的思维方式本身也值得怀疑的情况下，如何得到一个现实的心理状态？

我们能依靠的最稳固的地方在哪里？

### 理性的数学

20世纪初，数学家们通过为算术提出简单（例如集合论）公理，给出了一个更清晰的标准，用以判断结论的正确性。如果一个人或计算器得出“2 + 2 = 4”，我们现在能做的，不仅仅是说“这似乎直观正确”。我们可以解释为什么它是对的，并且我们可以证明它的正确性在系统性地与其他算术的正确性相联系。

然而，数学不仅仅让我们可以模拟一个口袋计算器等简单物理系统的行为。我们还可以用概率论来形式化理性信念，以提取所有成功推理方式的共同特征。我们甚至可以通过决策理论来形式化理性行为。

概率论定义了当我们面临不确定性时，理想的推理方式是什么，如果我们有足够的时间、计算能力和心理控制力。给定一些背景知识（先验）和一条新证据，概率论独特且精确地定义了我可以采纳的最佳新信念集（后验）。同样，决策理论定义了基于我信念的最佳行动。在我可能拥有的任何一致的信念和偏好下，总有一个决策理论的答案，告诉我应该如何行动，以便满足我的偏好。

假设你发现你六个同学中的某一个暗恋你——也许你收到了一封来自秘密崇拜者的信，且你确信它是来自那六个同学中的某一个——但你完全不知道是哪一个。同学Bob恰好是这六个同学之一。如果你没有特别的理由认为Bob比其他五个人更可能（或者更不可能）是那个暗恋你的人，那么Bob是那个有暗恋之情的人的概率是多少？

答案：概率是1:5。总共有六种可能，所以随便猜测平均来说每五次猜错你会猜对一次。

我们不能说，“我完全不知道谁暗恋我；也许是Bob，或者也许不是。那我就说概率是五五开吧。”即使我们更愿意说“我不知道”或“也许吧”然后就此停下，正确答案依然是1:5。这是基于假设有六种可能，并且你没有任何理由偏袒其中任何一个。<sup>8</sup>

假设你还注意到，当别人暗恋你时，他们比平常更频繁地对你眨眼。如果Bob对你眨眼了，那就是一条新的证据。在这种情况下，继续对Bob是否是你秘密崇拜者保持怀疑态度是错误的；对“随便对我眨眼的人有暗恋我”这一命中率为10:1的证据，应该胜过对“Bob有暗恋我”这一1:5的反对证据。

同样，错误的做法是说，“这个证据这么强，Bob肯定是暗恋我！从现在开始我就认为Bob喜欢我。”过度自信和过度怀疑同样有害。

实际上，这个问题只有一个可行的答案。根据证据的10:1可能性比率，我们将1:5的先验概率与10:1的证据比率相乘，得到10:5的后验概率，或者2:1的赔率，表示“Bob暗恋我”的可能性。在我们的假设和可用证据下，猜测Bob暗恋你，正确的概率是2/3。任何其他的信心度都是不一致的。

事实证明，在非常有限的条件下，“我应该相信什么？”这个问题有一个客观正确的答案。当你满怀不确定时，这个问题依然有一个正确答案，不仅仅是当你有了确凿的证明时。对于任何一个陈述，总有一个正确的信心度，即使它看起来更像是“个人信念”而非专家验证过的“事实”。

然而，我们常常像是在说，存在不确定性和分歧就意味着信仰只是个人口味问题。我们会说“那只是我的观点”或者“你有权拥有你的观点”，就好像科学和数学的断言与那些仅仅是“私人的”或“主观的”信念处于不同和更高的层次。对此，经济学家Robin Hanson曾回应道：<sup>9</sup>

> 你永远不能“有权拥有你的观点”。永远不行！你甚至不能“我不知道”。你有权拥有你的欲望，有时也有权做出选择。你可能拥有一个选择，而如果你能选择你的偏好，你可能有权这么做。但你的信仰不是关于你的；信仰是关于世界的。你的信仰应该是你对事物的最佳可得估计；其他任何东西都是谎言。\[…]
>
> 确实，一些话题使专家有更强的机制来解决争议。在其他话题上，我们的偏见和世界的复杂性让我们更难得出强有力的结论。\[...]
>
> 但永远不要忘记，关于事物的状态（或应该如何）的任何问题，在任何信息情境中，总有一个最佳估计。你唯一有权拥有的是尽力找到那个最佳估计；其他任何东西都是谎言。

我们的文化尚未吸收概率论的教训——即“我有多确定Bob暗恋我”这一问题的正确答案和代数测验或地质学教科书中的问题一样，都是由逻辑约束的。

我们的脑袋是自然选择拼凑出来的“临时产品”。人类并非完美的推理者或决策者，正如我们并非完美的计算器。即使是最好的我们，也无法精确地计算出“我该怎么想？”和“我该怎么做？”的准确答案。<sup>10</sup>

然而，知道我们不能完全一致，我们仍然可以变得更好。知道有一个理想标准可以让我们对照自己——研究人员所称的贝叶斯理性——可以指导我们改进我们的思维和行动。尽管我们永远无法成为完美的贝叶斯人，理性数学可以帮助我们理解某个答案为什么是正确的，并帮助我们精准地找出错误的地方。

想象一下，仅仅通过死记硬背来学习数学。你可能会


被告诉“10 + 3 = 13”，“31 + 108 = 139”等等，但除非你理解这些符号背后的规律，否则这些对你帮助不大。当你没有一个判断方法成功的框架时，寻找提高理性的方法会更加困难。本书的目的是帮助人们为自己构建这种框架。

### 理性应用

《如何真正改变你的思维》一书中的紧密关联的论文，最初是Eliezer Yudkowsky为博客《克服偏见》所写。2000年代末，这些文章激发了一个对理性和自我改进感兴趣的活跃社区的成长。

《地图与领土》是第一本这样的集合。《如何真正改变你的思维》是第二本。完整的六本合集，名为《理性：从AI到僵尸》，可以在Less Wrong网站找到，网址为：[http://lesswrong.com/rationality。](http://lesswrong.com/rationality。)

理性社区最受欢迎的作家之一Scott Alexander曾观察到：<sup>11</sup>

> \[O]显然，尽可能多地获取证据是有用的，就像尽可能多地拥有金钱是有用的。但同样显然的是，能够明智地使用有限的证据也是有用的，就像能够明智地使用有限的金钱一样。

理性技巧帮助我们在证据不明确或我们偏见扭曲了我们如何解读证据的情况下，充分利用现有证据。

这适用于我们的个人生活，例如Bob的故事。它也适用于政治派系和体育迷之间的分歧。它还适用于哲学难题以及关于技术和社会未来走向的辩论。认识到同样的数学规则适用于这些领域（并且在许多情况下相同的认知偏见也会出现），《如何真正改变你的思维》自由地穿梭于各类话题之间。

本书的第一部分《过于方便的借口》专注于概率上“容易”的问题——那些赔率极端，系统性错误看似应该特别容易发现的情况……

接着，我们进入了更复杂的领域：《政治与理性》。政治——更确切地说，主流的国家政治，像电视评论员们所辩论的那种政治——以愤怒和无效的讨论著称。表面上，似乎很奇怪，为什么我们在政治分歧上如此个人化，尽管国家政治的机制和效果往往离我们在空间或时间上都很遥远？更进一步，为什么我们在处理我们认为重要的问题时，不变得更加小心和严格地对待证据？

达特茅斯与普林斯顿比赛的故事似乎为此提供了一个答案。我们的许多推理过程实际上是合理化——通过编织让我们现有信仰看起来更加一致和正当的故事，而不一定改善其准确性。《反对合理化》讨论了这个问题，接下来是《用新眼睛看待问题》，关于识别那些不符合我们预期和假设的证据的挑战。

实际上，提高理性水平往往意味着接触到有趣和强大的新思想，并更多地与现实中的理性社区碰撞。《死亡螺旋》讨论了某些共同兴趣和辉煌想法的团体可能遭遇的重要风险，理性主义者如果想把他们崇高的理念转化为实际效果，就需要克服这些挑战。最后，《如何真正改变你的思维》以一系列关于“放手”的论文作结。

我们的自然状态不是像贝叶斯学者那样轻易改变思想。让达特茅斯和普林斯顿的学生注意到他们实际上看到的东西，不会像对他们背诵概率论公理那样容易。正如慈善研究分析员Luke Muehlhauser在《代理的力量》中写道：<sup>12</sup>

> 你不是一个贝叶斯小人，正如你推理并没有因为认知偏差而“被腐蚀”一样。
>
> 你就是认知偏差。

确认偏差、现状偏差、对应偏差等等，不是附加在我们推理上的，它们就是推理的本质。

这并不意味着去偏差是不可能的。我们底层并不是完美的计算器，尽管我们有许多算术错误。我们的许多数学局限来自人脑运作的深层事实。可是，我们可以训练自己的数学能力；我们可以学会何时信任和怀疑我们的数学直觉；我们可以塑造我们的环境，使其更容易进行推理。如果我们今天错了，明天也许会做得更好。

---

<sup>1</sup> 阿尔伯特·哈斯特福与哈德利·坎特里尔，《他们看到了比赛：一个案例研究》，《异常与社会心理学杂志》49（1954）：129-134，[http://www2.psych.ubc.ca/\~schaller/Psyc590Readings/Hastorf1954.pdf。](http://www2.psych.ubc.ca/~schaller/Psyc590Readings/Hastorf1954.pdf。)

<sup>2</sup> 艾米莉·普罗宁，《我们如何看待自己以及我们如何看待他人》，《科学》320（2008）：1177-1180。

<sup>3</sup> 罗伯特·P·瓦隆、李·罗斯和马克·R·莱珀，《敌对媒体现象：贝鲁特大屠杀报道中的偏见知觉与媒体偏见的感知》，《人格与社会心理学杂志》49（1985）：577-585，[http://ssc.wisc.edu/\~jpiliavi/965/hwang.pdf。](http://ssc.wisc.edu/~jpiliavi/965/hwang.pdf。)

<sup>4</sup> 于戈·梅尔西耶和丹·斯佩尔伯，《人类为何推理？为争论理论辩护》，《行为与脑科学》34（2011）：57-74，[http://hal.archives-ouvertes.fr/file/index/docid/904097/filename/MercierSperberWhydohumansreason.pdf。](http://hal.archives-ouvertes.fr/file/index/docid/904097/filename/MercierSperberWhydohumansreason.pdf。)

<sup>5</sup> 理查德·E·尼斯比特与蒂莫西·D·威尔逊，《比我们知道的更多：关于心理过程的口头报告》，《心理学评论》84（1977）：231-259，[http://people.virginia.edu/\~tdw/nisbett\&wilson.pdf。](http://people.virginia.edu/~tdw/nisbett&wilson.pdf。)

<sup>6</sup> 埃里克·施维茨吉贝尔，《意识的困惑》（麻省理工学院出版社，2011年）。

<sup>7</sup> 乔纳森·海特，《情感的狗与理性的尾巴：道德判断的社会直觉主义方法》，《心理学评论》108，第4期（2001）：814-834，doi:10.1037/0033-295X.108.4.814。

<sup>8</sup> 我们还不现实地假设你能真正确定崇拜者就是这六个人中的一个，而且你没有忽略其他可能性。（如果有多个同学都喜欢你呢？）

<sup>9</sup> 罗宾·汉森，《你永远没有权利拥有你的观点》，《克服偏见》（博客），2006年，[http://www.overcomingbias.com/2006/12/you\_are\_never\_e.html。](http://www.overcomingbias.com/2006/12/you_are_never_e.html。)

<sup>10</sup> 我们缺乏计算资源（而进化也缺乏工程专业知识和远见）来消除所有的错误。事实上，即便是现实世界中的一个最有效的推理者，也仍然需要依赖启发式和近似方法。对于改变信念的最佳计算可处理算法，仍然无法达到概率论的一致性。

<sup>11</sup> 斯科特·亚历山大，《我为什么不是勒内·笛卡尔》，《Slate Star Codex》（博客），2014年，[http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/。](http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/。)

<sup>12</sup> 卢克·穆尔豪泽，《行动的力量》，《Less Wrong》（博客），2011年，[http://lesswrong.com/lw/5i8/the\_power\_of\_agency/。](http://lesswrong.com/lw/5i8/the_power_of_agency/。)

---

## Rationality: An Introduction

by Rob Bensinger

In the autumn of 1951, a football game between Dartmouth and Princeton turned unusually rough. A pair of psychologists, Dartmouth’s Albert Hastorf and Princeton’s Hadley Cantril, decided to ask students from both schools which team had initiated the rough play. Nearly everyone agreed that Princeton hadn’t started it; but 86% of Princeton students believed that Dartmouth had started it, whereas only 36% of Dartmouth students blamed Dartmouth. (Most Dartmouth students believed “both started it.”)

When shown a film of the game later and asked to count the infractions they saw, Dartmouth students claimed to see a mean of 4.3 infractions by the Dartmouth team (and identified half as “mild”), whereas Princeton students claimed to see a mean of 9.8 Dartmouth infractions (and identified a third as “mild”).<sup>1</sup>

When something we value is threatened—our world-view, our in-group, our social standing, or something else we care about—our thoughts and perceptions rally to their defense.<sup>2</sup>,<sup>3</sup> Some psychologists go so far as to hypothesize that the human ability to come up with explicit justifications for our conclusions evolved specifically to help us win arguments.<sup>4</sup>

One of the basic insights of 20th-century psychology is that human behavior is often driven by sophisticated unconscious processes, and the stories we tell ourselves about our motives and reasons are much more biased and confabulated than we realize. We often fail, in fact, to realize that we’re doing any story-telling. When we seem to “directly perceive” things about ourselves in introspection, it often turns out to rest on tenuous implicit causal models.<sup>5</sup>,<sup>6</sup> When we try to argue for our beliefs, we can come up with shaky reasoning bearing no relation to how we first arrived at the belief.<sup>7</sup> Rather than trusting explanations in proportion to their predictive power, we tend to trust stories in proportion to their psychological appeal.

How can we do better? How can we arrive at a realistic view of the world, when we’re so prone to rationalization? How can we come to a realistic view of our mental lives, when our thoughts about thinking are also suspect?

What’s the least shaky place we could put our weight down?

### The Mathematics of Rationality

At the turn of the 20th century, coming up with simple (e.g., set-theoretic) axioms for arithmetic gave mathematicians a clearer standard by which to judge the correctness of their conclusions. If a human or calculator outputs “2 + 2 = 4,” we can now do more than just say “that seems intuitively right.” We can explain why it’s right, and we can prove that its rightness is tied in systematic ways to the rightness of the rest of arithmetic.

But mathematics lets us model the behaviors of physical systems that are a lot more interesting than a pocket calculator. We can also formalize rational belief in general, using probability theory to pick out features held in common by all successful forms of inference. We can even formalize rational behavior in general by drawing upon decision theory.

Probability theory defines how we would ideally reason in the face of uncertainty, if we had the requisite time, computing power, and mental control. Given some background knowledge (priors) and a new piece of evidence, probability theory uniquely and precisely defines the best set of new beliefs (posterior) I could adopt. Likewise, decision theory defines what action I should take based on my beliefs. For any consistent set of beliefs and preferences I could have, there is a decision-theoretic answer to how I should then act in order to satisfy my preferences.

Suppose you find out that one of your six classmates has a crush on you—perhaps you get a letter from a secret admirer, and you’re sure it’s from one of those six—but you have no idea which of the six it is. Bob happens to be one of those six classmates. If you have no special reason to think Bob’s any likelier (or any less likely) than the other five candidates, then what are the odds that Bob is the one with the crush?

Answer: The odds are 1:5. There are six possibilities, so a wild guess would result in you getting it right once for every five times you got it wrong, on average.

We can’t say, “Well, I have no idea who has a crush on me; maybe it’s Bob, or maybe it’s not. So I’ll just say the odds are fifty-fifty.” Even if we would rather say “I don’t know” or “Maybe” and stop there, the right answer is still 1:5. This follows from the assumption that there are six possibilities and you have no reason to favor one of them over any of the others.<sup>8</sup>

Suppose that you’ve also noticed you get winked at by people ten times as often when they have a crush on you. If Bob then winks at you, that’s a new piece of evidence. In that case, it would be a mistake to stay skeptical about whether Bob is your secret admirer; the 10:1 odds in favor of “a random person who winks at me has a crush on me” outweigh the 1:5 odds against “Bob has a crush on me.”

It would also be a mistake to say, “That evidence is so strong, it’s a sure bet that he’s the one who has the crush on me! I’ll just assume from now on that Bob is into me.” Overconfidence is just as bad as underconfidence.

---

In fact, there’s only one viable answer to this question too. To change our mind from the 1:5 prior odds in response to the evidence’s 10:1 likelihood ratio, we multiply the left sides together and the right sides together, getting 10:5 posterior odds, or 2:1 odds in favor of “Bob has a crush on me.” Given our assumptions and the available evidence, guessing that Bob has a crush on you will turn out to be correct 2 times for every 1 time it turns out to be wrong. Equivalently: the probability that he’s attracted to you is 2/3. Any other confidence level would be inconsistent.

It turns out that given very modest constraints, the question “What should I believe?” has an objectively right answer. It has a right answer when you’re wracked with uncertainty, not just when you have a conclusive proof. There is always a correct amount of confidence to have in a statement, even when it looks more like a “personal belief” instead of an expert-verified “fact.”

Yet we often talk as though the existence of uncertainty and disagreement makes beliefs a mere matter of taste. We say “that’s just my opinion” or “you’re entitled to your opinion,” as though the assertions of science and math existed on a different and higher plane than beliefs that are merely “private” or “subjective.” To which economist Robin Hanson has responded:<sup>9</sup>

> You are never entitled to your opinion. Ever! You are not even entitled to “I don’t know.” You are entitled to your desires, and sometimes to your choices. You might own a choice, and if you can choose your preferences, you may have the right to do so. But your beliefs are not about you; beliefs are about the world. Your beliefs should be your best available estimate of the way things are; anything else is a lie. [ . . . ]
> 
> It is true that some topics give experts stronger mechanisms for resolving disputes. On other topics our biases and the complexity of the world make it harder to draw strong conclusions. [ . . . ]
> 
> But never forget that on any question about the way things are (or should be), and in any information situation, there is always a best estimate. You are only entitled to your best honest effort to find that best estimate; anything else is a lie.

Our culture hasn’t internalized the lessons of probability theory—that the correct answer to questions like “How sure can I be that Bob has a crush on me?” is just as logically constrained as the correct answer to a question on an algebra quiz or in a geology textbook.

Our brains are kludges slapped together by natural selection. Humans aren’t perfect reasoners or perfect decision-makers, any more than we’re perfect calculators. Even at our best, we don’t compute the exact right answer to “what should I think?” and “what should I do?”<sup>10</sup>

And yet, knowing we can’t become fully consistent, we can certainly still get better. Knowing that there’s an ideal standard we can compare ourselves to—what researchers call Bayesian rationality—can guide us as we improve our thoughts and actions. Though we’ll never be perfect Bayesians, the mathematics of rationality can help us understand why a certain answer is correct, and help us spot exactly where we messed up.

Imagine trying to learn math through rote memorization alone. You might be told that “10 + 3 = 13,” “31 + 108 = 139,” and so on, but it won’t do you a lot of good unless you understand the pattern behind the squiggles. It can be a lot harder to seek out methods for improving your rationality when you don’t have a general framework for judging a method’s success. The purpose of this book is to help people build for themselves such frameworks.

### Rationality Applied

The tightly linked essays in How to Actually Change Your Mind were originally written by Eliezer Yudkowsky for the blog Overcoming Bias. Published in the late 2000s, these posts helped inspire the growth of a vibrant community interested in rationality and self-improvement.

Map and Territory was the first such collection. How to Actually Change Your Mind is the second. The full six-book set, titled Rationality: From AI to Zombies, can be found on Less Wrong at http://lesswrong.com/rationality.

One of the rationality community’s most popular writers, Scott Alexander, has previously observed:<sup>11</sup>

> [O]bviously it’s useful to have as much evidence as possible, in the same way it’s useful to have as much money as possible. But equally obviously it’s useful to be able to use a limited amount of evidence wisely, in the same way it’s useful to be able to use a limited amount of money wisely.

Rationality techniques help us get more mileage out of the evidence we have, in cases where the evidence is inconclusive or our biases are distorting how we interpret the evidence.

This applies to our personal lives, as in the tale of Bob. It applies to disagreements between political factions and sports fans. And it applies to philosophical puzzles and debates about the future trajectory of technology and society. Recognizing that the same mathematical rules apply to each of these domains (and that in many cases the same cognitive biases crop up), How to Actually Change Your Mind freely moves between a wide range of topics.

The first sequence of essays in this book, Overly Convenient Excuses, focuses on probabilistically “easy” questions—ones where the odds are extreme, and systematic errors seem like they should be particularly easy to spot.…

From there, we move into murkier waters with Politics and Rationality. Politics—or rather, mainstream national politics of the sort debated by TV pundits—is famous for its angry, unproductive discussions. On the face of it, there’s something surprising about that. Why do we take political disagreements so personally, even though the machinery and effects of national politics are often so distant from us in space or in time? For that matter, why do we not become more careful and rigorous with the evidence when we’re dealing with issues we deem important?

The Dartmouth-Princeton game hints at an answer. Much of our reasoning process is really rationalization—story-telling that makes our current beliefs feel more coherent and justified, without necessarily improving their accuracy. Against Rationalization speaks to this problem, followed by Seeing with Fresh Eyes, on the challenge of recognizing evidence that doesn’t fit our expectations and assumptions.

In practice, leveling up in rationality often means encountering interesting and powerful new ideas and colliding more with the in-person rationality community. Death Spirals discusses some important hazards that can afflict groups united around common interests and amazing shiny ideas, which rationalists will need to overcome if they’re to translate their high-minded ideas into real-world effectiveness. How to Actually Change Your Mind then concludes with a sequence on Letting Go.

Our natural state isn’t to change our minds like a Bayesian would. Getting the Dartmouth and Princeton students to notice what they’re actually seeing won’t be as easy as reciting the axioms of probability theory to them. As philanthropic research analyst Luke Muehlhauser writes in “The Power of Agency”:<sup>12</sup>

> You are not a Bayesian homunculus whose reasoning is “corrupted” by cognitive biases.
> 
> You just are cognitive biases.

Confirmation bias, status quo bias, correspondence bias, and the like are not tacked on to our reasoning; they are its very substance.

That doesn’t mean that debiasing is impossible. We aren’t perfect calculators underneath all our arithmetic errors, either. Many of our mathematical limitations result from very deep facts about how the human brain works. Yet we can train our mathematical abilities; we can learn when to trust and distrust our mathematical intuitions; we can shape our environments to make things easier on us. And if we’re wrong today, we can be less so tomorrow.

---

翻译成中文：

<sup>1</sup>Albert Hastorf and Hadley Cantril, “They Saw a Game: A Case Study,” Journal of Abnormal and Social Psychology 49 (1954): 129–134, http://www2.psych.ubc.ca/~schaller/Psyc590Readings/Hastorf1954.pdf.

<sup>2</sup>Emily Pronin, “How We See Ourselves and How We See Others,” Science 320 (2008): 1177–1180.

<sup>3</sup>Robert P. Vallone, Lee Ross, and Mark R. Lepper, “The Hostile Media Phenomenon: Biased Perception and Perceptions of Media Bias in Coverage of the Beirut Massacre,” Journal of Personality and Social Psychology 49 (1985): 577–585, http://ssc.wisc.edu/~jpiliavi/965/hwang.pdf.

<sup>4</sup>Hugo Mercier and Dan Sperber, “Why Do Humans Reason? Arguments for an Argumentative Theory,” Behavioral and Brain Sciences 34 (2011): 57–74, http://hal.archives-ouvertes.fr/file/index/docid/904097/filename/MercierSperberWhydohumansreason.pdf.

<sup>5</sup>Richard E. Nisbett and Timothy D. Wilson, “Telling More than We Can Know: Verbal Reports on Mental Processes,” Psychological Review 84 (1977): 231–259, http://people.virginia.edu/~tdw/nisbett&wilson.pdf.

<sup>6</sup>Eric Schwitzgebel, Perplexities of Consciousness (MIT Press, 2011).

<sup>7</sup>Jonathan Haidt, “The Emotional Dog and Its Rational Tail: A Social Intuitionist Approach to Moral Judgment,” Psychological Review 108, no. 4 (2001): 814–834, doi:10.1037/0033-295X.108.4.814.

<sup>8</sup>We’re also assuming, unrealistically, that you can really be certain the admirer is one of those six people, and that you aren’t neglecting other possibilities. (What if more than one of your classmates has a crush on you?)

<sup>9</sup>Robin Hanson, “You Are Never Entitled to Your Opinion,” Overcoming Bias (Blog), 2006, http://www.overcomingbias.com/2006/12/you_are_never_e.html.

<sup>10</sup>We lack the computational resources (and evolution lacked the engineering expertise and foresight) to iron out all our bugs. Indeed, even a maximally efficient reasoner in the real world would still need to rely on heuristics and approximations. The best possible computationally tractable algorithms for changing beliefs would still fall short of probability theory’s consistency.

<sup>11</sup>Scott Alexander, “Why I Am Not Rene Descartes,” Slate Star Codex (Blog), 2014, http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/.

<sup>12</sup>Luke Muehlhauser, “The Power of Agency,” Less Wrong (Blog), 2011, http://lesswrong.com/lw/5i8/the_power_of_agency/.
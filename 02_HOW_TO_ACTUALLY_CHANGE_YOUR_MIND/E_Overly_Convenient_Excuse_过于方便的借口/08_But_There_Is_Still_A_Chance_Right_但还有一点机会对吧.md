## 但还有一点机会，对吧？

作者：Eliezer Yudkowsky

多年前，我和一个人交谈时，他随口说他不相信进化论。我说：“这已经不是十九世纪了。达尔文最初提出进化论时，怀疑它可能还有些道理。但现在是二十一世纪了。我们已经可以读取基因了。人类和黑猩猩的DNA有98%是相同的。我们知道人类和黑猩猩是亲戚。事情已经尘埃落定了。”

他说：“也许DNA的相似只是巧合。”

我说：“那种巧合的概率大约是二的七亿五千万次方分之一。”

他说：“但还是有一点机会，对吧？”

现在，我过去的自己其实没资格在这场对话中宣称自己有绝对的道德胜利。有几个原因：其中一个是，我完全不记得自己当时是从哪里得出这个 2<sup>750,000,000</sup> 的数字的，虽然它在数量级上可能是对的。另一个原因是，我当时并没有应用“校准过的置信度”这一概念。在整个人类历史上，每当有人计算出某件事发生的概率是二的七亿五千万次方分之一，他们实际出错的频率，肯定远远高于一次中错一次的概率。举个例子，共享基因的估算后来被修正为95%，而不是98%——而且这可能只适用于已知的三万个基因，而不是整个基因组，那样的话，这个数量级可能就错了。

不过，我还是觉得对方的回答非常好笑。

我不记得我后来怎么回应的了——可能说了句“不行”——但我一直记得这次对话，因为它让我对“未开化者”的思维规律产生了几个洞见。

我第一次意识到，人类的直觉会在“没有任何机会”和“有一点微小机会，但值得留意”之间做出一种质的区分。你可以在“克服偏见”博客中关于彩票的讨论中看到这一点。

问题在于，概率理论有时候确实能让我们计算出一个微乎其微的概率，小到根本不值得占据我们的心智资源——但在你得出这个结论之前，你已经花力气去计算它了。人们往往把“地图”和“真实地形”混为一谈，从而在直觉层面上，只要看到一个用符号表示的概率，就会觉得那是“值得留意的机会”，即使它代表的是一个小得看不见的数。如果这种概率像尘埃那么小，你根本连它都看不见。我们可以用语言描述如此之小的数字，但我们的情感无法感知它——那种小到极致的感觉根本无法被感知，不会触发足够的神经元活动或神经递质释放。这也是为什么人们会买彩票——没有人能真正感受到那种极小概率的“小”。

但让我觉得更有意思的是，人们在“确定”和“不确定”的论证之间也有一种质的划分：如果一个论证不是绝对确定的，你就可以无视它。就好像：如果某件事的可能性是零，那你必须放弃这个信念；但如果它的可能性是一分之一万亿（googol 分之一），你就可以保留它。

当然，这个世界是自由的，没有人应该因为“逻辑违法”而坐牢。但如果你可以无视一个说某事概率是 googol 分之一的论证，那你为什么不也无视一个说概率是零的论证呢？我是说，既然你都打算无视证据了，为什么只无视不确定的证据，却不敢无视确定的证据？

我在生活中经常从他人的明显错误中吸取经验，然后将这些经验推广到更微妙的情境。在这个例子中，反过来的教训是：如果你不能因为“我不愿意”就无视 googol 分之一的可能性，那你也不能因为“我不愿意”就无视 0.9 的可能性。这本质上是同一条滑坡。

如果你某天发现自己在想：“但你无法证明我是错的。”那么请考虑一下这段对话的例子：如果你能无视一个有概率基础的反对论证，那你为什么又不能无视一个严格的证明呢？

---

## But There’s Still a Chance, Right?

Years ago, I was speaking to someone when he casually remarked that he didn’t believe in evolution. And I said, “This is not the nineteenth century. When Darwin first proposed evolution, it might have been reasonable to doubt it. But this is the twenty-first century. We can read the genes. Humans and chimpanzees have 98% shared DNA. We know humans and chimps are related. It’s over. ”

He said, “Maybe the DNA is just similar by coincidence.”

I said, “The odds of that are something like two to the power of seven hundred and fifty million to one.”

He said, “But there’s still a chance, right?”

Now, there’s a number of reasons my past self cannot claim a strict moral victory in this conversation. One reason is that I have no memory of whence I pulled that 2<sup>750,000,000</sup> figure, though it’s probably the right meta-order of magnitude. The other reason is that my past self didn’t apply the concept of a calibrated confidence. Of all the times over the history of humanity that a human being has calculated odds of 2<sup>750,000,000</sup>:1 against something, they have undoubtedly been wrong more often than once in 2<sup>750,000,000</sup> times. E.g., the shared genes estimate was revised to 95%, not 98%—and that may even apply only to the 30,000 known genes and not the entire genome, in which case it’s the wrong meta-order of magnitude.

But I think the other guy’s reply is still pretty funny. 

I don’t recall what I said in further response—probably something like “No”—but I remember this occasion because it brought me several insights into the laws of thought as seen by the unenlightened ones.

It first occurred to me that human intuitions were making a qualitative distinction between “No chance” and “A very tiny chance, but worth keeping track of.” You can see this in the Overcoming Bias lottery debate.

The problem is that probability theory sometimes lets us calculate a chance which is, indeed, too tiny to be worth the mental space to keep track of it—but by that time, you’ve already calculated it. People mix up the map with the territory, so that on a gut level, tracking a symbolically described probability feels like “a chance worth keeping track of,” even if the referent of the symbolic description is a number so tiny that if it were a dust speck, you couldn’t see it. We can use words to describe numbers that small, but not feelings—a feeling that small doesn’t exist, doesn’t fire enough neurons or release enough neurotransmitters to be felt. This is why people buy lottery tickets—no one can feel the smallness of a probability that small.

But what I found even more fascinating was the qualitative distinction between “certain” and “uncertain” arguments, where if an argument is not certain, you’re allowed to ignore it. Like, if the likelihood is zero, then you have to give up the belief, but if the likelihood is one over googol, you’re allowed to keep it.

Now it’s a free country and no one should put you in jail for illegal reasoning, but if you’re going to ignore an argument that says the likelihood is one over googol, why not also ignore an argument that says the likelihood is zero? I mean, as long as you’re ignoring the evidence anyway, why is it so much worse to ignore certain evidence than uncertain evidence?

I have often found, in life, that I have learned from other people’s nicely blatant bad examples, duly generalized to more subtle cases. In this case, the flip lesson is that, if you can’t ignore a likelihood of one over googol because you want to, you can’t ignore a likelihood of 0.9 because you want to. It’s all the same slippery cliff.

Consider his example if you ever you find yourself thinking, “But you can’t prove me wrong.” If you’re going to ignore a probabilistic counterargument, why not ignore a proof, too?
## 绝对权威

*作者：Eliezer Yudkowsky*

某人傲然走到你面前说：“科学其实并不真正‘知道’什么。你们只有理论而已——你们无法确定自己是对的。你们科学家已经改过对重力的看法——谁能保证你们明天不会又改变对进化的看法？”

——请看，这就是深不见底的文化鸿沟。如果你认为几句话就能跨越它，那你注定会大失所望。

在未开化者的世界里，存在的是“权威”与“非权威”。值得信赖的东西就是值得信赖的，不值得信赖的东西干脆就可以扔掉。信息有“好来源”和“坏来源”。如果科学家在历史上曾经改变过自己的说法，那么科学就不是真正的“权威”，就再也不值得信任——就像一个证人被揭穿说谎，或者像一个被抓到偷钱的雇员一样。

而且，这类人默认：谁提出一个观点，谁就必须抵御一切反驳，并且绝不承认任何问题。所有的主张因此都被打折扣。如果连科学本身的支持者都承认科学并不完美——那科学一定是毫无价值的。

一个人在习惯了“确定性”的生活方式之后，你不能简单地告诉他们：“科学是概率性的，就像所有其他知识一样。”他们会把前半句当作认罪，后半句当作你试图指责所有人来逃避惩罚的狡辩。

你承认了自己不可靠——那就走吧，科学，不要再来烦我们了！

这种思维模式的一个明显来源是宗教：圣典据说是来自神的启示，因此任何缺陷的承认都会彻底摧毁其权威；任何一丝怀疑都是罪，宣称确定无疑是义务，无论你是否真的确定。<sup>1</sup>

但我怀疑，这种观念也与传统的学校教育制度有关。老师告诉你一些“对的事”，你必须相信，并在考试中复述出来。而当一个学生在课堂上发表自己的看法时，你似乎可以选择同意或不同意，没人会因此惩罚你。

这种经验恐怕把“信念”的领域映射到了“权威”、“命令”、“法律”的社会领域中。在社会领域，绝对法律与非绝对法律、命令与建议、权威与非权威之间是有质的区别的。于是，知识也变成了“严格知识”与“非严格知识”的区别，就像严格规章与灵活建议之间的区别。严格权威必须服从，而灵活建议可依个人喜好采纳或忽略。而科学，既然承认自己可能会错，自然只能归入后者。

（顺便说一句，那些认为“如果你的不确定性没有被老师写在纸上，或者没有来自某个不可质疑的权威来源的明文概率，那就不能算是‘贝叶斯概率’”的人，似乎也有类似心态。<sup>2</sup> 他们惊呼：“天哪！你怎么能自己估计先验概率？”于是，在这些尚未完全启蒙者眼中，贝叶斯先验也成了学生提出的主张，不再是老师布置的命令——也就不算是真正的知识。）

“权威之道”与“量化之道”之间的这条文化鸿沟，对我们这些理性主义者来说，简直让人抓狂。这些人坚信自己掌握着比“科学的概率性猜测”更可靠的知识——比如，认为月亮明天将如期升起，形态如常，这种预测不过是科学的“猜测”。而事实上，这种预测自从有天文记录以来从未出错，而且建立在已被验证至小数点后十四位的物理理论之上。而那些未开化者视为高于科学的“知识”，究竟是什么呢？很可能是一张早就被反驳得千疮百孔的古老羊皮纸。他们却说，这比科学更可靠，因为它从不承认错误，从不改变观点——哪怕早已被现实打脸无数次。
他们把“确定性”这个词像网球一样随手抛掷，轻若羽毛；而科学家却被良心之疑所压抑，哪怕只是勉强说出一个带概率的判断都需要挣扎。他们毫不在意地说：“我是完美的”，于是宣称自己远远高于那些仍在不断努力进步的科学家。

你不可能用一句话就让他们理解——也没有什么能快速反驳他们的“杀手锏”。如果是在公开辩论中，也许你可以通过缜密思考赢得观众的支持。但你不能直接脱口而出：“愚蠢的凡人，量化之道超出你的理解，而你所谓的‘确定’比我们最不靠谱的假设还要不确定。”这是生活方式的根本差异，甚至难以用语言解释，更别说迅速解释了。

那在公众面前，你可以尝试哪些论点？或许：

* “科学的力量在于它能改变观点并承认错误。如果你从不承认自己错过，并不代表你错得更少。”
* “谁都可以说自己‘绝对确定’。更难的是从不犯错。科学家明白这点，所以他们不说‘绝对确定’。这并不代表他们有具体理由怀疑某个理论——哪怕所有的证据都指向同一方向，哪怕一切迹象都支持同一个假设，科学家依然不会说‘绝对确定’，因为他们标准更高。这不代表他们比那些整天自信满满的政客更没资格谈确定。”
* “科学家说‘不绝对确定’时，并不是像你平常对话中那样用这个词。假设你去做了个血检，医生回来告诉你：‘我们做了点测试，虽然不能绝对确定你不是奶酪做的，也不能排除你肚子里有二十个巧克力精灵在唱巴尼的《我爱你》’，你会拔腿就跑。这个医生需要看医生。但如果科学家这么说，他们的意思是：这个概率小到电子显微镜都看不到，但如果你真能拿出证据，他们还是愿意看。”
* “你愿意在证据面前改变你所谓的‘确定信念’吗？比如，上帝亲自从天而降告诉你：你的信仰都对，除了‘童贞女怀孕’这一条。如果你会因此改变想法，那你就不能说自己‘绝对确定’童贞女怀孕这个教义。根据概率理论，只要你‘理论上’有可能改变想法，这件事的概率就不可能恰好是1。这个不确定性或许比尘埃还小，但它必须存在。如果你连上帝告诉你都不肯改，那你这个不认错的问题，已经超出了我这种凡人能处理的范畴了。”

但更有趣的问题也许是：当你不是在公众面前辩论时，该对某人说什么？你如何开始教会一个人，在一个没有“确定性”的宇宙中生存？

我认为第一步应该是理解：你**可以**在没有确定性的情况下生活——假设你永远无法对任何事“绝对确定”，这并不会剥夺你做出道德或事实判断的能力。借用作家 Lois Bujold 的话说：“不要试图用力推进，而是要减少阻力。”

绝对权威的一个常见防线，是我称之为“灰色论的反击论证”的东西，其逻辑大致如下：

* 道德相对主义者说：

  * 世界不是黑白的 → 所以：
  * 一切都是灰色的 → 所以：
  * 没有人比别人更好 → 所以：
  * 我想干嘛就干嘛，你拦不住我，哈哈哈。
* 但我们必须能阻止人去杀人。
* 所以我们必须有某种“绝对确定性”，否则道德相对主义者就赢了。

但——**反过来的愚蠢并不等于智慧**。你不能通过推翻某个坏论点的每一行，来得到正确答案——那等于是让傻子主宰你的逻辑方向。一个数学论证要成立，**每一行都得正确**。仅仅因为某个坏论点里说了“世界不是黑白的”，并不代表这个说法就是错的。就像斯大林认为 2+2=4 并不让这个式子变错一样


。真正的错误（而且只要一个就够了）在于：他们从“两种颜色”的世界，跳跃到了“只有一种灰色”的世界，仿佛所有灰都一样。

如果你接受这个前提——只有拥有绝对善恶知识的人才能做出道德选择——那等于你把整个论证都让出去了。其实你**完全可以**在不确定的情况下，比较相对“好”与相对“坏”的选项，并作出选择。这应当是常规操作，不是什么戏剧化的道德危机。

我不是说，如果你能确定无疑地知道 A 是完全美好，B 是纯粹邪恶，那你选 A 就有充分理由。**但这并不是必要条件**。

对了，还有一个常见的逻辑谬误：**诉诸信念后果（Appeal to consequences of belief）**。

那么，他们还需要明白什么呢？首先，整个理性主义文化认为：怀疑、质疑、承认错误**不是羞耻的事**。

还有一个核心理念是：你通过**观察世界**获取信息，而不是通过被洗脑。如果你观察得更仔细，有时候会发现事物与你第一印象不同；但这并不代表“自然欺骗了你”，更不意味着你该放弃观察。

再者，他们还需要了解“校准信心”这个概念——“概率”不是你脑海里那个表示“情感信念强度”的进度条。它更像是一个现实生活中，某种信念状态下人们说对话的**频率指标**。如果你让一百个人各说一句“他们绝对确定”的话，最终多少句是对的？绝不会是一百句。

其实，那些人们表现出**极端信念**的说法，反而往往**更不靠谱**，远不如“太阳比月亮大”这种大家懒得激动的陈述靠谱。因为越是有对立面存在的说法，人们才越容易陷入“信仰战”。因此，大脑里那个情感进度条，根本不能代表一个好概率估计——它甚至都不是**单调函数**。

至于“绝对确定”——如果你说某件事是 99.9999% 可能，那表示你认为你能连续说出一百万句同样强度的判断，大概会错一两次而已。这已经够惊人了。（能用来否定“你明天中乐透”的概率大概就这么高）。所以我们根本不用谈“1.0 的概率”。一旦你意识到人类生活根本不需要 1.0 的概率，你就会明白：想靠一颗人脑达到 1.0 是多么荒唐——1.0 不只是“确定”，是**无限确定**。

事实上，为了避免公众误解，科学家也许该说：“我们不是**无限确定**”，而不是“我们不确定”。因为后者在日常语境中，听起来像是“我们知道有理由怀疑”。

---

<sup>1</sup>: 参见《宣称与欢呼》（*Professing and Cheering*），收录于《地图与真实地形》（*[Map and Territory](http://lesswrong.com/rationality/?_ga=2.252316298.2009561180.1752330363-1762639772.1748962126)*），可在 rationalitybook.com 或 lesswrong.com/rationality 找到。

<sup>2</sup>: 参见《聚焦你的不确定性》（*[Focus Your Uncertainty](https://www.lesswrong.com/rationality/focus-your-uncertainty)*），同样收录于 *[Map and Territory](http://lesswrong.com/rationality/?_ga=2.252316298.2009561180.1752330363-1762639772.1748962126)*。

---

## Absolute Authority

by Eliezer Yudkowsky

The one comes to you and loftily says: “Science doesn’t really know anything. All you have are theories—you can’t know for certain that you’re right. You scientists changed your minds about how gravity works—who’s to say that tomorrow you won’t change your minds about evolution?”

Behold the abyssal cultural gap. If you think you can cross it in a few sentences, you are bound to be sorely disappointed.

In the world of the unenlightened ones, there is authority and un-authority. What can be trusted, can be trusted; what cannot be trusted, you may as well throw away. There are good sources of information and bad sources of information. If scientists have changed their stories ever in their history, then science cannot be a true Authority, and can never again be trusted—like a witness caught in a contradiction, or like an employee found stealing from the till.

Plus, the one takes for granted that a proponent of an idea is expected to defend it against every possible counterargument and confess nothing. All claims are discounted accordingly. If even the proponent of science admits that science is less than perfect, why, it must be pretty much worthless.

When someone has lived their life accustomed to certainty, you can’t just say to them, “Science is probabilistic, just like all other knowledge.” They will accept the first half of the statement as a confession of guilt; and dismiss the second half as a flailing attempt to accuse everyone else to avoid judgment.

You have admitted you are not trustworthy—so begone, Science, and trouble us no more!

One obvious source for this pattern of thought is religion, where the scriptures are alleged to come from God; therefore to confess any flaw in them would destroy their authority utterly; so any trace of doubt is a sin, and claiming certainty is mandatory whether you’re certain or not.<sup>1</sup>

But I suspect that the traditional school regimen also has something to do with it. The teacher tells you certain things, and you have to believe them, and you have to recite them back on the test. But when a student makes a suggestion in class, you don’t have to go along with it—you’re free to agree or disagree (it seems) and no one will punish you.

This experience, I fear, maps the domain of belief onto the social domains of authority, of command, of law. In the social domain, there is a qualitative difference between absolute laws and nonabsolute laws, between commands and suggestions, between authorities and unauthorities. There seems to be strict knowledge and unstrict knowledge, like a strict regulation and an unstrict regulation. Strict authorities must be yielded to, while unstrict suggestions can be obeyed or discarded as a matter of personal preference. And Science, since it confesses itself to have a possibility of error, must belong in the second class.

(I note in passing that I see a certain similarity to they who think that if you don’t get an Authoritative probability written on a piece of paper from the teacher in class, or handed down from some similar Unarguable Source, then your uncertainty is not a matter for Bayesian probability theory.<sup>2</sup> Someone might—gasp!—argue with your estimate of the prior probability. It thus seems to the not-fully-enlightened ones that Bayesian priors belong to the class of beliefs proposed by students, and not the class of beliefs commanded you by teachers—it is not proper knowledge).

The abyssal cultural gap between the Authoritative Way and the Quantitative Way is rather annoying to those of us staring across it from the rationalist side. Here is someone who believes they have knowledge more reliable than science’s mere probabilistic guesses—such as the guess that the Moon will rise in its appointed place and phase tomorrow, just like it has every observed night since the invention of astronomical record-keeping, and just as predicted by physical theories whose previous predictions have been successfully confirmed to fourteen decimal places. And what is this knowledge that the unenlightened ones set above ours, and why? It’s probably some musty old scroll that has been contradicted eleventeen ways from Sunday, and from Monday, and from every day of the week. Yet this is more reliable than Science (they say) because it never admits to error, never changes its mind, no matter how often it is contradicted. They toss around the word “certainty” like a tennis ball, using it as lightly as a feather—while scientists are weighed down by dutiful doubt, struggling to achieve even a modicum of probability. “I’m perfect,” they say without a care in the world, “I must be so far above you, who must still struggle to improve yourselves.”

There is nothing simple you can say to them—no fast crushing rebuttal. By thinking carefully, you may be able to win over the audience, if this is a public debate. Unfortunately you cannot just blurt out, “Foolish mortal, the Quantitative Way is beyond your comprehension, and the beliefs you lightly name ‘certain’ are less assured than the least of our mighty hypotheses.” It’s a difference of life-gestalt that isn’t easy to describe in words at all, let alone quickly.

What might you try, rhetorically, in front of an audience? Hard to say . . . maybe:

- “The power of science comes from having the ability to change our minds and admit we’re wrong. If you’ve never admitted you’re wrong, it doesn’t mean you’ve made fewer mistakes.”
- “Anyone can say they’re absolutely certain. It’s a bit harder to never, ever make any mistakes. Scientists understand the difference, so they don’t say they’re absolutely certain. That’s all. It doesn’t mean that they have any specific reason to doubt a theory—absolutely every scrap of evidence can be going the same way, all the stars and planets lined up like dominos in support of a single hypothesis, and the scientists still won’t say they’re absolutely sure, because they’ve just got higher standards. It doesn’t mean scientists are less entitled to certainty than, say, the politicians who always seem so sure of everything.”
- “Scientists don’t use the phrase ‘not absolutely certain’ the way you’re used to from regular conversation. I mean, suppose you went to the doctor, and got a blood test, and the doctor came back and said, ‘We ran some tests, and it’s not absolutely certain that you’re not made out of cheese, and there’s a non-zero chance that twenty fairies made out of sentient chocolate are singing the “I love you” song from Barney inside your lower intestine.’ Run for the hills, your doctor needs a doctor. When a scientist says the same thing, it means that they think the probability is so tiny that you couldn’t see it with an electron microscope, but the scientist is willing to see the evidence in the extremely unlikely event that you have it.”
- “Would you be willing to change your mind about the things you call ‘certain’ if you saw enough evidence? I mean, suppose that God himself descended from the clouds and told you that your whole religion was true except for the Virgin Birth. If that would change your mind, you can’t say you’re absolutely certain of the Virgin Birth. For technical reasons of probability theory, if it’s theoretically possible for you to change your mind about something, it can’t have a probability exactly equal to one. The uncertainty might be smaller than a dust speck, but it has to be there. And if you wouldn’t change your mind even if God told you otherwise, then you have a problem with refusing to admit you’re wrong that transcends anything a mortal like me can say to you, I guess.”

But, in a way, the more interesting question is what you say to someone not in front of an audience. How do you begin the long process of teaching someone to live in a universe without certainty?

I think the first, beginning step should be understanding that you can live without certainty—that if, hypothetically speaking, you couldn’t be certain of anything, it would not deprive you of the ability to make moral or factual distinctions. To paraphrase Lois Bujold, “Don’t push harder, lower the resistance.”

One of the common defenses of Absolute Authority is something I call “The Argument from the Argument from Gray,” which runs like this:

- Moral relativists say:
    - The world isn’t black and white, therefore:
    - Everything is gray, therefore:
    - No one is better than anyone else, therefore:
    - I can do whatever I want and you can’t stop me bwahahaha.
- But we’ve got to be able to stop people from committing murder.
- Therefore there has to be some way of being absolutely certain, or the moral relativists win.

Reversed stupidity is not intelligence. You can’t arrive at a correct answer by reversing every single line of an argument that ends with a bad conclusion—it gives the fool too much detailed control over you. Every single line must be correct for a mathematical argument to carry. And it doesn’t follow, from the fact that moral relativists say “The world isn’t black and white,” that this is false, any more than it follows, from Stalin’s belief that 2 + 2 = 4, that “2 + 2 = 4” is false. The error (and it only takes one) is in the leap from the two-color view to the single-color view, that all grays are the same shade.

It would concede far too much (indeed, concede the whole argument) to agree with the premise that you need absolute knowledge of absolutely good options and absolutely evil options in order to be moral. You can have uncertain knowledge of relatively better and relatively worse options, and still choose. It should be routine, in fact, not something to get all dramatic about.

I mean, yes, if you have to choose between two alternatives A and B, and you somehow succeed in establishing knowably certain well-calibrated 100% confidence that A is absolutely and entirely desirable and that B is the sum of everything evil and disgusting, then this is a sufficient condition for choosing A over B. It is not a necessary condition.

Oh, and: Logical fallacy: Appeal to consequences of belief.

Let’s see, what else do they need to know? Well, there’s the entire rationalist culture which says that doubt, questioning, and confession of error are not terrible shameful things.

There’s the whole notion of gaining information by looking at things, rather than being proselytized. When you look at things harder, sometimes you find out that they’re different from what you thought they were at first glance; but it doesn’t mean that Nature lied to you, or that you should give up on seeing.

Then there’s the concept of a calibrated confidence—that “probability” isn’t the same concept as the little progress bar in your head that measures your emotional commitment to an idea. It’s more like a measure of how often, pragmatically, in real life, people in a certain state of belief say things that are actually true. If you take one hundred people and ask them each to make a statement of which they are “absolutely certain,” how many of these statements will be correct? Not one hundred.

If anything, the statements that people are really fanatic about are far less likely to be correct than statements like “the Sun is larger than the Moon” that seem too obvious to get excited about. For every statement you can find of which someone is “absolutely certain,” you can probably find someone “absolutely certain” of its opposite, because such fanatic professions of belief do not arise in the absence of opposition. So the little progress bar in people’s heads that measures their emotional commitment to a belief does not translate well into a calibrated confidence—it doesn’t even behave monotonically.

As for “absolute certainty”—well, if you say that something is 99.9999% probable, it means you think you could make one million equally strong independent statements, one after the other, over the course of a solid year or so, and be wrong, on average, around once. This is incredible enough. (It’s amazing to realize we can actually get that level of confidence for “Thou shalt not win the lottery.”) So let us say nothing of probability 1.0. Once you realize you don’t need probabilities of 1.0 to get along in life, you’ll realize how absolutely ridiculous it is to think you could ever get to 1.0 with a human brain. A probability of 1.0 isn’t just certainty, it’s infinite certainty.

In fact, it seems to me that to prevent public misunderstanding, maybe scientists should go around saying “We are not infinitely certain” rather than “We are not certain.” For the latter case, in ordinary discourse, suggests you know some specific reason for doubt.

<sup>1</sup>See “Professing and Cheering,” collected in Map and Territory and findable at rationalitybook.com and lesswrong.com/rationality.

<sup>2</sup>See “Focus Your Uncertainty” in Map and Territory.
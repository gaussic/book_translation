## 无限确定性

作者：Eliezer Yudkowsky

在《绝对权威》（Absolute Authority）一文中，我曾提出一个观点：你并不需要“无限的确定性”：

> 如果你必须在两个选项 A 和 B 之间做出选择，而你成功地建立了明确可知、校准良好、百分之百确信的信念：A 是绝对且完全可取的，而 B 则是所有邪恶与令人作呕之事的总和，那么这就足以让你选择 A 而不是 B。但这并不是必要条件……你可以对哪个更好、哪个更糟只有不确定的知识，依然做出选择。事实上，这应当是常规操作。

对于命题“2 + 2 = 4”，我们必须区分“地图”和“领地”——即理论模型与现实之间的区别。考虑到物理定律的表面稳定性与普遍性，有可能自宇宙诞生以来，从未有任何粒子超越局部的光速限制。换句话说，光速限制也许不仅是 99% 的时间成立，或 99.9999%、甚至（1 - 1/googolplex） 的时间成立，而是始终、绝对地成立。

但我们是否能对光速限制具有“绝对的信心”——这是另一个问题。地图不是领地。

也许一个学生确实完全剽窃了作业，但你是否知道这一事实（更不用说对这个信念有“绝对信心”），则是另一个独立的问题。如果你抛了枚硬币却没看，那么硬币确实可能正面朝上，而你却完全不知道是正是反。不确定性与事实的真实性或发生频率，是不同的概念。

这个道理同样适用于数学真理。像“2 + 2 = 4”或“在皮亚诺算术中，SS0 + SS0 = SSSS0”这样的陈述，是否在纯粹抽象的意义上成立，尚属存疑，除非我们考虑到那些看起来符合皮亚诺公理体系的物理系统。但话虽如此，我仍然大胆推测：无论“2 + 2 = 4”在什么意义上成立，它都是始终且完全正确的，而不是大致正确（如“2 + 2 实际等于 4.0000004”）或在十亿次中成立九亿九千九百九十九万九千九百九十九次。

我不确定在这种情况下“正确”应当意味着什么，但我依然支持我的这个猜测。相比之下，“2 + 2 = 4 始终成立”这一命题的可信度，远高于任何一种关于“正确”“始终”或“是”等词在这句话中应如何理解的哲学立场。

但这并不意味着我对“2 + 2 = 4”有绝对信心。参见我此前的讨论：你要如何说服我相信“2 + 2 = 3”？这也许可以通过类似于最初让我相信“2 + 2 = 4”的证据来实现。我可能之前看到的一切证据都是幻觉，或者我回忆错了。在神经学的历史上，有比这更离奇的大脑功能异常。

那么如果我们要给命题“2 + 2 = 4”分配一个概率，这个概率应是多少？你要追求的是**良好的校准**——你赋予“99% 概率”的命题应该在 100 次中有 99 次成立。这其实比你想象的要难得多。让 100 个人每人说出 10 个他们“99% 有信心”的判断，在这 1,000 个判断中，你认为只有 10 个会错吗？

我不打算在这里详细讨论关于“概率校准”的实验——你可以在我那篇关于“认知偏差与全球灾难性风险”的论文<sup>1</sup>中找到——因为我发现如果不加解释地说出来，人们就会把它当作一个“万能反驳工具”：每当他们不喜欢某人的观点，就用它来质疑对方的信心；但轮到自己的时候却想不起这个标准。所以，除非在理性推理的系统讲解中，我一般避免随意提到这些实验，尤其不在没有讲解“动机性怀疑”的背景下提到它。

但人类实际表现出的“99% 信心”与 99% 准确率之间，并不相符。

假设你说你对“2 + 2 = 4”有 99.99% 的信心。那么你刚刚就是在声称：你可以发表 10,000 个彼此独立、同等确定的陈述，并且平均只会错一次。或许对于“2 + 2 = 4”而言，这种极高程度的信心是可能的：它极其简单，既是数学上的，也是经验上的，还得到了社会上的广泛接受（虽然不是热情支持，而是理所当然地接受）。所以也许你真的能对这个命题达到 99.99% 的信心。

但我不认为你能对类似“53 是一个质数”的判断也达到 99.99% 的信心。是的，这看起来很有可能。但如果你要设计一种机制，支持你说出 10,000 个这样的判断（而不是只谈质数，而是每次都根据一个新方法得出），你一定会出错超过一次。<sup>2</sup>

但地图仍不是领地：如果我说我对“2 + 2 = 4”有 99% 的信心，这并不表示我认为“2 + 2 = 4”这个陈述本身只在 99% 情况下成立，也不是说它“有 99% 的精确度”。我所确信的，是“2 + 2 始终且完全等于 4”这一命题，而不是“2 + 2 通常大致等于 4”。

至于你是否可以对某个数学命题达到 100% 的信心——说真的！如果你说 99.9999% 的信心，你就是在声称你可以连续发表一百万个同样重要的陈述，平均只错一次。假设你每 20 秒说一个命题，每天说 16 小时，也要说上一整年。

如果你声称有 99.9999999999% 的信心，那就是一万亿次不出错。这大约是 100 个普通人类寿命所需的陈述数量，你打算一辈子（还不止）都不犯错？

而如果你说你对某个命题有 (1 - 1/googolplex) 的信心，那你比自称是上帝的精神病患者还自负。

而 googolplex（10^10^100）相比于一些“不太大的不可思议大数”（比如 3 ↑↑↑ 3）还算“小”。但即便你说你对一个命题的信心是 (1 - 1/3 ↑↑↑ 3)，这距离“概率为 1”其实也没有比你 90% 确信某件事高太多。

如果一切都失败了，假想中的“矩阵黑暗领主”也会出手干预——他们此刻正在操控你对这句话的信任感，以阻止我们陷入“无限确定性”的灾厄。

我对此有**绝对的确定性**吗？

当然不。

正如 Rafal Smigrodski 曾说：

> 我会说，你应该可以对那些推导出贝叶斯定理所必需的数学概念本身，赋予一个小于 1 的确定性水平，同时在实践中依然使用它。我不确定自己是否必须永远保持不确定。也许我可以对某些事合理地确信。但一旦我给某个命题分配了概率 1，无论我之后看到或学到什么，我都必须拒绝任何与之矛盾的内容。我不喜欢永远不能改变主意的想法。

---

<sup>1</sup> Eliezer Yudkowsky，《可能影响全球风险判断的认知偏差》（Cognitive Biases Potentially Affecting Judgment of Global Risks），载于 Nick Bostrom 与 Milan M. Ćirković 主编的《全球灾难性风险》（Global Catastrophic Risks），牛津大学出版社，2008 年，第 91–119 页。

<sup>2</sup> Peter de Blanc 关于这个话题有一段趣事：[http://www.spaceandgames.com/?p=27](http://www.spaceandgames.com/?p=27)。（我告诉他不要再这么干了。）

---

## Infinite Certainty

by Eliezer Yudkowsky

In “Absolute Authority,” I argued that you don’t need infinite certainty:

> If you have to choose between two alternatives A and B, and you somehow succeed in establishing knowably certain well-calibrated 100% confidence that A is absolutely and entirely desirable and that B is the sum of everything evil and disgusting, then this is a sufficient condition for choosing A over B. It is not a necessary condition . . . You can have uncertain knowledge of relatively better and relatively worse options, and still choose. It should be routine, in fact.

Concerning the proposition that 2 + 2 = 4, we must distinguish between the map and the territory. Given the seeming absolute stability and universality of physical laws, it’s possible that never, in the whole history of the universe, has any particle exceeded the local lightspeed limit. That is, the lightspeed limit may be not just true 99% of the time, or 99.9999% of the time, or (1 - 1/googolplex) of the time, but simply always and absolutely true.

But whether we can ever have absolute confidence in the lightspeed limit is a whole ’nother question. The map is not the territory.

It may be entirely and wholly true that a student plagiarized their assignment, but whether you have any knowledge of this fact at all—let alone absolute confidence in the belief—is a separate issue. If you flip a coin and then don’t look at it, it may be completely true that the coin is showing heads, and you may be completely unsure of whether the coin is showing heads or tails. A degree of uncertainty is not the same as a degree of truth or a frequency of occurrence.

The same holds for mathematical truths. It’s questionable whether the statement “2 + 2 = 4” or “In Peano arithmetic, SS0 + SS0 = SSSS0” can be said to be true in any purely abstract sense, apart from physical systems that seem to behave in ways similar to the Peano axioms. Having said this, I will charge right ahead and guess that, in whatever sense “2 + 2 = 4” is true at all, it is always and precisely true, not just roughly true (“2 + 2 actually equals 4.0000004”) or true 999,999,999,999 times out of 1,000,000,000,000.

I’m not totally sure what “true” should mean in this case, but I stand by my guess. The credibility of “2 + 2 = 4 is always true” far exceeds the credibility of any particular philosophical position on what “true,” “always,” or “is” means in the statement above.

This doesn’t mean, though, that I have absolute confidence that 2 + 2 = 4. See the previous discussion on how to convince me that 2 + 2 = 3, which could be done using much the same sort of evidence that convinced me that 2 + 2 = 4 in the first place. I could have hallucinated all that previous evidence, or I could be misremembering it. In the annals of neurology there are stranger brain dysfunctions than this.

So if we attach some probability to the statement “2 + 2 = 4,” then what should the probability be? What you seek to attain in a case like this is good calibration—statements to which you assign “99% probability” come true 99 times out of 100. This is actually a hell of a lot more difficult than you might think. Take a hundred people, and ask each of them to make ten statements of which they are “99% confident.” Of the 1,000 statements, do you think that around 10 will be wrong?

I am not going to discuss the actual experiments that have been done on calibration—you can find them in my book chapter on cognitive biases and global catastrophic risk<sup>1</sup>—because I’ve seen that when I blurt this out to people without proper preparation, they thereafter use it as a Fully General Counterargument, which somehow leaps to mind whenever they have to discount the confidence of someone whose opinion they dislike, and fails to be available when they consider their own opinions. So I try not to talk about the experiments on calibration except as part of a structured presentation of rationality that includes warnings against motivated skepticism.

But the observed calibration of human beings who say they are “99% confident” is not 99% accuracy.

Suppose you say that you’re 99.99% confident that 2 + 2 = 4. Then you have just asserted that you could make 10,000 independent statements, in which you repose equal confidence, and be wrong, on average, around once. Maybe for 2 + 2 = 4 this extraordinary degree of confidence would be possible: “2 + 2 = 4” is extremely simple, and mathematical as well as empirical, and widely believed socially (not with passionate affirmation but just quietly taken for granted). So maybe you really could get up to 99.99% confidence on this one.

I don’t think you could get up to 99.99% confidence for assertions like “53 is a prime number.” Yes, it seems likely, but by the time you tried to set up protocols that would let you assert 10,000 independent statements of this sort—that is, not just a set of statements about prime numbers, but a new protocol each time—you would fail more than once.<sup>2</sup>

Yet the map is not the territory: If I say that I am 99% confident that 2 + 2 = 4, it doesn’t mean that I think “2 + 2 = 4” is true to within 99% precision, or that “2 + 2 = 4” is true 99 times out of 100. The proposition in which I repose my confidence is the proposition that “2 + 2 = 4 is always and exactly true,” not the proposition “2 + 2 = 4 is mostly and usually true.”

As for the notion that you could get up to 100% confidence in a mathematical proposition—well, really now! If you say 99.9999% confidence, you’re implying that you could make one million equally fraught statements, one after the other, and be wrong, on average, about once. That’s around a solid year’s worth of talking, if you can make one assertion every 20 seconds and you talk for 16 hours a day.

Assert 99.9999999999% confidence, and you’re taking it up to a trillion. Now you’re going to talk for a hundred human lifetimes, and not be wrong even once?

Assert a confidence of (1 - 1/googolplex) and your ego far exceeds that of mental patients who think they’re God.

And a googolplex is a lot smaller than even relatively small inconceivably huge numbers like 3 ↑↑↑ 3. But even a confidence of (1 - 1/3 ↑↑↑ 3) isn’t all that much closer to PROBABILITY 1 than being 90% sure of something.

If all else fails, the hypothetical Dark Lords of the Matrix, who are right now tampering with your brain’s credibility assessment of this very sentence, will bar the path and defend us from the scourge of infinite certainty.

Am I absolutely sure of that?

Why, of course not.

As Rafal Smigrodski once said:

> I would say you should be able to assign a less than 1 certainty level to the mathematical concepts which are necessary to derive Bayes’s rule itself, and still practically use it. I am not totally sure I have to be always unsure. Maybe I could be legitimately sure about something. But once I assign a probability of 1 to a proposition, I can never undo it. No matter what I see or learn, I have to reject everything that disagrees with the axiom. I don’t like the idea of not being able to change my mind, ever.

<sup>1</sup>Eliezer Yudkowsky, “Cognitive Biases Potentially Affecting Judgment of Global Risks,” in Global Catastrophic Risks, ed. Nick Bostrom and Milan M. irkovi (New York: Oxford University Press, 2008), 91–119.

<sup>2</sup>Peter de Blanc has an amusing anecdote on this point: http://www.spaceandgames.com/?p=27. (I told him not to do it again.)
## 什么是“被筛选的证据”？

我曾讨论过“聪明辩手”的困境：他受雇于你，要推销一个可能有钻石也可能没有钻石的箱子。聪明辩手会指出箱子上有蓝色印章，而已知有钻石的箱子比空箱子更可能有蓝色印章。从贝叶斯视角来看，这时会发生什么？你是否只能无助地按照聪明辩手的意愿更新自己的概率？

如果你能自己查看箱子，你可以把所有迹象都加总起来。如果你不能看呢？如果你唯一的证据就是聪明辩手的话语，而他受法律约束只能说真话，但不会把他知道的一切都告诉你？聪明辩手说的每一句话都是有效证据——你怎么能不更新概率呢？难道在所有埃弗雷特分支或泰格马克多重宇宙中，B箱有蓝色印章的那些世界里，B箱含有钻石的比例就不再成立了吗？按照 Jaynes 的说法，贝叶斯主义者必须始终以所有已知证据为条件，否则就会陷入悖论。但如果聪明辩手可以从足够多的迹象中选择性地报告，他就能让你相信任何他想让你相信的事。这听起来不对劲。

考虑一个更简单的例子：一枚偏置硬币，可能正面概率为2/3、反面为1/3，也可能正好相反，先验概率各为50%。每观察到一个正面，就是1比特支持正面偏置，每观察到一个反面，就是1比特支持反面偏置。<sup>1</sup> 我掷十次硬币，然后告诉你：“第4、6、9次掷出来的是正面。”那么你现在认为硬币偏向正面的后验概率是多少？

答案是，这个概率可以几乎是任何值，取决于我说出这些话背后的因果链——也就是我选择报告哪些结果的算法。

- 也许我总是报告第4、6、9次的结果，无论结果如何。如果你知道我用的是这个算法，后验赔率就是8:1，支持正面偏置。
- 也许我只报告所有出现正面的次数。如果你知道是这样，那其他7次都是反面，后验赔率就是1:16，反对正面偏置。
- 也许我事先决定，只有当硬币偏正面的概率超过98%时，才报告第4、6、9次的结果。等等。

再看著名的蒙提霍尔问题：

    在一个游戏节目中，你可以选择三扇门，每扇门后面有一个房间。你知道其中一个房间有10万美元，另外两个是空的。主持人让你选一扇门，你选了1号门。然后主持人打开2号门，发现里面是空的。你要换到3号门，还是坚持1号门？

答案取决于主持人的算法。如果主持人总是会打开一扇门，且总是选空房间的门，那你应该换到3号门。如果主持人无论门后是什么都只打开2号门，那1号和3号各有50%概率有钱。如果主持人只在你一开始就选对门时才会开门，那你就一定要坚持1号门。

你不应该只根据2号门是空的这个事实来更新概率，而是要结合主持人选择开2号门的行为。很多人之所以被标准的蒙提霍尔问题困惑，是因为他们只根据2号门是空的来更新概率，这样1号和3号概率就相等了。这也是为什么贝叶斯主义者被要求以所有已知信息为条件，否则就会陷入悖论。

当有人说“第4次掷硬币是正面”，我们并不是只以“第4次正面”为条件——我们不是在所有可能世界中筛选出第4次正面的那些——而是以“说话者按照某种算法说出‘第4次是正面’”为条件。说出来的句子本身不是事实本身，不要被字面意义误导。

大多数法律程序都基于这样一种理论：每个案件都有两方对立，找到两个有偏见的人比找到一个无偏见的人更容易。在控方和辩方之间，总有人有动机呈现任何一条证据，所以法庭最终能看到所有证据——理论上如此。如果箱子问题中有两个聪明辩手，虽然不如一个真正好奇的调查者，但也差不多。但现实世界的问题往往远比“两箱子”复杂，常常有多方利益、深层次难题和非显而易见的答案，仅靠蓝队和绿队互相喊话是找不到真相的。

要警惕滥用“证据筛选”这个概念，把它当作“万能反驳”来排除所有你不喜欢的证据：“那个论据是被筛选过的，所以我可以无视它。”如果你被反对论据激怒，说明你很熟悉这个话题，也足够在乎才会站队。你很可能已经知道自己这边最有力的论据。你没有理由因为对方的反对论据，就推断出还有你没见过的新支持性迹象。所以你只能面对那些不太舒服的事实本身；B箱上的蓝色印章依然是证据。

但如果你是第一次听到某个论据，而且只听到了一方的说法，那你确实要小心！某种意义上，只有在听过五分钟的创世论者之后，你才真正能信任自然选择理论；那时你才知道它真的站得住脚。

---

<sup>1</sup>这里的“比特”是指某个证据提供了多少信息量——即概率的对数，以1/2为底。

假设一个问题只有两个互斥的答案，你最初各赋予50%概率。如果我告诉你第一个答案是对的（且你完全相信我），你就获得了1比特的信息。如果有四个等概率选项，我告诉你哪个是对的，就是2比特；如果有八个，就是3比特，依此类推。详见《地图与领地》中的“How Much Evidence Does It Take?”。

---

## What Evidence Filtered Evidence?

I discussed the dilemma of the clever arguer, hired to sell you a box that may or may not contain a diamond. The clever arguer points out to you that the box has a blue stamp, and it is a valid known fact that diamond-containing boxes are more likely than empty boxes to bear a blue stamp. What happens at this point, from a Bayesian perspective? Must you helplessly update your probabilities, as the clever arguer wishes?

If you can look at the box yourself, you can add up all the signs yourself. What if you can’t look? What if the only evidence you have is the word of the clever arguer, who is legally constrained to make only true statements, but does not tell you everything they know? Each statement that the clever arguer makes is valid evidence—how could you not update your probabilities? Has it ceased to be true that, in such-and-such a proportion of Everett branches or Tegmark duplicates in which box B has a blue stamp, box B contains a diamond? According to Jaynes, a Bayesian must always condition on all known evidence, on pain of paradox. But then the clever arguer can make you believe anything they choose, if there is a sufficient variety of signs to selectively report. That doesn’t sound right.

Consider a simpler case, a biased coin, which may be biased to come up 2/3 heads and 1/3 tails, or 1/3 heads and 2/3 tails, both cases being equally likely a priori. Each H observed is 1 bit of evidence for an H-biased coin; each T observed is 1 bit of evidence for a T-biased coin.<sup>1</sup> I flip the coin ten times, and then I tell you, “The 4th flip, 6th flip, and 9th flip came up heads.” What is your posterior probability that the coin is H-biased?

And the answer is that it could be almost anything, depending on what chain of cause and effect lay behind my utterance of those words—my selection of which flips to report.

- I might be following the algorithm of reporting the result of the 4th, 6th, and 9th flips, regardless of the result of those and all other flips. If you know that I used this algorithm, the posterior odds are 8:1 in favor of an H-biased coin.
- I could be reporting on all flips, and only flips, that came up heads. In this case, you know that all 7 other flips came up tails, and the posterior odds are 1:16 against the coin being H-biased.
- I could have decided in advance to say the result of the 4th, 6th, and 9th flips only if the probability of the coin being H-biased exceeds 98%. And so on.

Or consider the Monty Hall problem:

	On a game show, you are given the choice of three doors leading to three rooms. You know that in one room is $100,000, and thg other two are empty. The host asks you to pick a door, and yog pick door #1. Then the host opens door #2, revealing an emptg room. Do you want to switch to door #3, or stick with door #1?

The answer depends on the host’s algorithm. If the host always opens a door and always picks a door leading to an empty room, then you should switch to door #3. If the host always opens door #2 regardless of what is behind it, #1 and #3 both have 50% probabilities of containing the money. If the host only opens a door, at all, if you initially pick the door with the money, then you should definitely stick with #1.

You shouldn’t just condition on #2 being empty, but this fact plus the fact of the host choosing to open door #2. Many people are confused by the standard Monty Hall problem because they update only on #2 being empty, in which case #1 and #3 have equal probabilities of containing the money. This is why Bayesians are commanded to condition on all of their knowledge, on pain of paradox.

When someone says, “The 4th coinflip came up heads,” we are not conditioning on the 4th coinflip having come up heads—we are not taking the subset of all possible worlds where the 4th coinflip came up heads—but rather are conditioning on the subset of all possible worlds where a speaker following some particular algorithm said, “The 4th coinflip came up heads.” The spoken sentence is not the fact itself; don’t be led astray by the mere meanings of words.

Most legal processes work on the theory that every case has exactly two opposed sides and that it is easier to find two biased humans than one unbiased one. Between the prosecution and the defense, someone has a motive to present any given piece of evidence, so the court will see all the evidence; that is the theory. If there are two clever arguers in the box dilemma, it is not quite as good as one curious inquirer, but it is almost as good. But that is with two boxes. Reality often has many-sided problems, and deep problems, and nonobvious answers, which are not readily found by Blues and Greens shouting at each other.

Beware lest you abuse the notion of evidence-filtering as a Fully General Counterargument to exclude all evidence you don’t like: “That argument was filtered, therefore I can ignore it.” If you’re ticked off by a contrary argument, then you are familiar with the case, and care enough to take sides. You probably already know your own side’s strongest arguments. You have no reason to infer, from a contrary argument, the existence of new favorable signs and portents which you have not yet seen. So you are left with the uncomfortable facts themselves; a blue stamp on box B is still evidence.

But if you are hearing an argument for the first time, and you are only hearing one side of the argument, then indeed you should beware! In a way, no one can really trust the theory of natural selection until after they have listened to creationists for five minutes; and then they know it’s solid.

---

<sup>1</sup>“Bits” in this context are a measure of how much evidence something provides—they’re the logarithms of probabilities, base 1/2.

Suppose a question has exactly two possible (mutually exclusive) answers, and you initially assign 50% probability to each answer. If I then tell you that the first answer is correct (and you have complete faith in my claim), then you have acquired one bit of evidence. If there are four equally likely options, and I tell you the first one is correct, then I have given you two bits; if there are eight and I tell you the right one, then I have given you three bits; and so on. This is discussed further in “How Much Evidence Does It Take?” (in Map and Territory).
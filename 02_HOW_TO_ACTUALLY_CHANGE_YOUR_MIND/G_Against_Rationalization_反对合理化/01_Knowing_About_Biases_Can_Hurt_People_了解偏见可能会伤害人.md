## 了解偏见可能会伤害人

**作者：Eliezer Yudkowsky**

曾经我试图向我母亲讲述专家校准的问题，我说：“所以当一个专家说他有 99% 的把握时，结果只有大约 70% 的时候他是对的。”
然后我突然停顿了一下，意识到我在跟我妈妈说话，于是赶紧补充道：“当然，你得确保把这种怀疑态度公平地应用在每个人身上，包括你自己，而不是只拿来反驳你不同意的东西——”

我妈却说：“你在开玩笑吗？这太棒了！我以后要经常用它！”

Taber 和 Lodge 在《**政治信念评估中的动机性怀疑**》一文中验证了六个预测：

1. **先入为主效应**：即使被鼓励保持客观，受试者如果对某个议题有强烈立场，仍会更倾向于评价支持该立场的论点为更有说服力。
2. **反证偏差**：受试者会花更多时间和认知资源去贬低反对自己立场的论点，而不是支持自己立场的论点。
3. **确认偏误**：如果允许自由选择信息来源，受试者会更倾向于选择支持自己立场的信息来源。
4. **立场极化**：即便暴露在表面上平衡的正反观点下，受试者原本的立场也会变得更加极端。
5. **立场强度效应**：持更强烈立场的受试者，更容易表现出上述偏差。
6. **“聪明人效应”**：政治知识更丰富的受试者，因为拥有更多“反驳不一致信息”的弹药，更容易陷入上述偏差。

如果你本来就不理性，**知道得越多，可能反而越糟**。
对真正的贝叶斯主义者来说，信息从不会具有负的期望效用。
但人类不是完美的贝叶斯推理者——如果我们不小心，就会“伤到自己”。

我见过一些人，因为了解了偏见而变得彻底混乱。他们掌握了更多“反驳自己不喜欢的观点”的弹药。
这种“弹药太多”的问题，正是高智力者陷入 Stanovich 所说“**非理性智障**”（dysrationalia）的主要方式之一。

你应该能想到这样的人吧？智力很高，但因为“太擅长辩论”，反而变得不那么有效？
你觉得如果只是把一堆经典偏见讲给他们听，会让他们变成更理性的思考者吗？

我记得有个人刚学到校准/过度自信的问题，过不了多久他就说：“专家根本不可信，他们经常错——实验已经证明了这一点。所以我预测未来时，更倾向于认为事情会按历史的趋势继续下去——”
接着他就开始进行一套复杂、容易出错、非常可疑的推演。
不知为何，一旦是他自己偏好的结论，所有的偏见和谬误似乎就都不那么“显眼”了——不像他反驳别人时那样迅速跳出来。

我还记得我曾跟他说过“反证偏差”和“诡辩者”的问题，结果之后每当我说他不喜欢的话，他就指责我“在诡辩”。
他并不会指出某个具体诡辩、某个具体问题——只是摇摇头、叹口气，说我居然在用自己的聪明才智来伤害自己。
他获得了又一个“通用反驳手段”（Fully General Counterargument）。

甚至连“诡辩者”这个概念本身也可能是危险的——如果你一听到某个聪明人说出你不喜欢的观点，就立刻想到这个词。

我一直努力从自己的错误中学习。
上一次我做关于启发式和偏见的讲座时，我从“合取谬误”和“代表性启发”这两个例子介绍了基本概念。
接着我讲到了确认偏误、反证偏差、诡辩、动机性怀疑、以及其他与态度相关的效应。
之后的三十分钟我一直围绕这个主题反复阐述，从尽可能多的角度重新引入它。

我当然希望观众能对这个话题感兴趣。
嗯，光讲“合取谬误”和“代表性启发”就足够引起兴趣了。
但如果他们真的产生兴趣，那接下来呢？
偏见方面的文献大多只是“为了认知心理学而做的认知心理学”。
我得在那一讲里把警告说完——否则他们可能永远都听不到。

无论是在写作中还是演讲中，我现在都尽量**不先提“校准”和“过度自信”问题，除非我已经讲过反证偏差、动机性怀疑、诡辩者以及高智力者的非理性**。
**首先，别造成伤害！**

---

## Knowing About Biases Can Hurt People

by Eliezer Yudkowsky

Once upon a time I tried to tell my mother about the problem of expert calibration, saying: “So when an expert says they’re 99% confident, it only happens about 70% of the time.” Then there was a pause as, suddenly, I realized I was talking to my mother, and I hastily added: “Of course, you’ve got to make sure to apply that skepticism evenhandedly, including to yourself, rather than just using it to argue against anything you disagree with—”

And my mother said: “Are you kidding? This is great! I’m going to use it all the time!”

Taber and Lodge’s “Motivated Skepticism in the Evaluation of Political Beliefs” describes the confirmation of six predictions:

1. Prior attitude effect. Subjects who feel strongly about an issue—even when encouraged to be objective—will evaluate supportive arguments more favorably than contrary arguments.
2. Disconfirmation bias. Subjects will spend more time and cognitive resources denigrating contrary arguments than supportive arguments.
3. Confirmation bias. Subjects free to choose their information sources will seek out supportive rather than contrary sources.
4. Attitude polarization. Exposing subjects to an apparently balanced set of pro and con arguments will exaggerate their initial polarization.
5. Attitude strength effect. Subjects voicing stronger attitudes will be more prone to the above biases.
6. Sophistication effect. Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to the above biases.

If you’re irrational to start with, having more knowledge can hurt you. For a true Bayesian, information would never have negative expected utility. But humans aren’t perfect Bayes-wielders; if we’re not careful, we can cut ourselves.

I’ve seen people severely messed up by their own knowledge of biases. They have more ammunition with which to argue against anything they don’t like. And that problem—too much ready ammunition—is one of the primary ways that people with high mental agility end up stupid, in Stanovich’s “dysrationalia” sense of stupidity.

You can think of people who fit this description, right? People with high g-factor who end up being less effective because they are too sophisticated as arguers? Do you think you’d be helping them—making them more effective rationalists—if you just told them about a list of classic biases?

I recall someone who learned about the calibration/overconfidence problem. Soon after he said: “Well, you can’t trust experts; they’re wrong so often—as experiments have shown. So therefore, when I predict the future, I prefer to assume that things will continue historically as they have—” and went off into this whole complex, error-prone, highly questionable extrapolation. Somehow, when it came to trusting his own preferred conclusions, all those biases and fallacies seemed much less salient—leapt much less readily to mind—than when he needed to counter-argue someone else.

I told the one about the problem of disconfirmation bias and sophisticated argument, and lo and behold, the next time I said something he didn’t like, he accused me of being a sophisticated arguer. He didn’t try to point out any particular sophisticated argument, any particular flaw—just shook his head and sighed sadly over how I was apparently using my own intelligence to defeat itself. He had acquired yet another Fully General Counterargument.

Even the notion of a “sophisticated arguer” can be deadly, if it leaps all too readily to mind when you encounter a seemingly intelligent person who says something you don’t like.

I endeavor to learn from my mistakes. The last time I gave a talk on heuristics and biases, I started out by introducing the general concept by way of the conjunction fallacy and representativeness heuristic. And then I moved on to confirmation bias, disconfirmation bias, sophisticated argument, motivated skepticism, and other attitude effects. I spent the next thirty minutes hammering on that theme, reintroducing it from as many different perspectives as I could.

I wanted to get my audience interested in the subject. Well, a simple description of conjunction fallacy and representativeness would suffice for that. But suppose they did get interested. Then what? The literature on bias is mostly cognitive psychology for cognitive psychology’s sake. I had to give my audience their dire warnings during that one lecture, or they probably wouldn’t hear them at all.

Whether I do it on paper, or in speech, I now try to never mention calibration and overconfidence unless I have first talked about disconfirmation bias, motivated skepticism, sophisticated arguers, and dysrationalia in the mentally agile. First, do no harm!
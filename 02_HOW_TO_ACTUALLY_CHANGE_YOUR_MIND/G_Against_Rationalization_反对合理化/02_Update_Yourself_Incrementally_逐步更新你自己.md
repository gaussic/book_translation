## **逐步更新你自己**

作者：Eliezer Yudkowsky

[政治是思维的杀手](https://www.lesswrong.com/lw/gw/politics_is_the_mindkiller/)。辩论是战争，[论点是士兵](https://www.lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/)。人们常常忍不住想要寻找一种方法，[用来解释每一个可能的实验结果](https://www.lesswrong.com/lw/i4/belief_in_belief/)，以此来确认自己的理论，就像在用尽一切方式守卫一座要塞。但这是你无法做到的事——这是**数学上**不可能的。[因为对于每一个预期的证据，总会存在一个等量且相反的反证据预期](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/)。

但这没关系。如果你所珍视的信念并非完美无缺地受到捍卫，那也没什么问题。如果你的假设是硬币有 95% 的概率正面朝上，那么每二十次中就有一次会出现看似与之相反的结果。这是正常的，甚至是预期之中的——只要每一次相反的证据都能被十九次支持性的观察所抵消。一个概率模型是可以[承受一两次打击](https://www.lesswrong.com/lw/ig/i_defy_the_data/)的，只要打击不会持续不断。

然而，在舆论场，许多人仍然相信：正确的理论**绝不会失败**，而错误的理论**绝不会成功**。

你会看到人们拿出某个他们认为是“证据”的单一事件，声称他们的理论“能解释它”，仿佛这就是一个理论所需要的全部支持。显然，在这种认知中，错误的理论不可能拥有哪怕一丁点的支持证据；错误的理论甚至无法符合任何一个事件。于是，只要有一条支持证据，任何理论就都可以被认为是正确的。

稍微好一点但仍然是错误的做法是：拿出一条概率意义上的反对证据，就当成对理论的彻底反驳，仿佛正确的理论不可能面对哪怕一个反例。但人类就是这么争论了几千年，总是试图击败所有敌人的论点，同时不容许敌人拥有哪怕一丝一毫的支持。人们希望辩论是**单边胜利的**；他们早已习惯于一个他们所钟爱理论从未遭受反对的世界。因此，哪怕允许出现一条反对证据，也好像是世界末日。

我知道一定会有人站出来说：“可你在现实世界中要赢得辩论，哪能让一步啊！只要你承认对方的反对意见存在，对手就会逮住不放——你不能让他们这样！你会输的！还有比这更让人本能地害怕的事吗？”

好吧——但理性并不是用来赢得辩论的，它是用来**决定要站在哪一边的**。如果你已经决定了要为某一方辩护，那么理性的工作就已经在你心中完成了——无论完成得好还是差。但你究竟该如何决定自己应站在哪一边？如果选错立场让你本能地感到恐惧——哪怕只是一点点恐惧——那你最好把所有证据都整合起来再说。

理性不是散步，而是跳舞。舞步中每一步都必须踩在恰当的位置上，不能偏左，也不能偏右。每一丁点支持性证据出现时，就提升一点你的信念。每一丁点反对性证据出现时，就降低一点你的信念。**没错，是降低。** 即便你的模型是正确的，只要它不是“完全正确”的，你有时也需要向下修正你的信念。

如果一丁点反对性证据偶然出现，那也没关系。这在针对非精确理论的概率性证据中是常有的事。（当然，如果是精确理论都失败了，那你麻烦大了！）你所要做的，只是稍微降低一下信念值——可能是概率、可能是赔率、也可能是你心中某种模糊的信任权重。**只需降低一点，然后[等待更多的证据](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/)。** 如果你的理论是对的，支持性证据很快就会到来，概率也会再次上升。如果理论是错的，那你本来也不该想要它。

问题在于，如果你用的是非黑即白、二元化的定性推理，那每一个观察都要么彻底摧毁你的理论，要么完全无害。当你连一个反例都不能接受时，[认知失调就会产生，你就必须为其找借口](https://www.lesswrong.com/lw/ig/i_defy_the_data/)。而这正是阻碍我们逐步进步的绊脚石，阻碍我们正确整合所有证据。概率性推理能让我们意识到：**从平均而言，正确的理论将产生更多支持而不是反对的权重。** 因此你可以毫不畏惧地对自己说：“这是一点轻微的反证，我会稍微下调我的信念。” **没错，是向下。** 这不会摧毁你所珍爱的理论。那是定性思维的错觉——你要学会量化思维。

[对于每一个证据预期，总有一个等量且相反的反证预期](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/)。在每一个时刻，你都必须平均地期望自己的信念**可能上升，也可能下降**。如果你觉得自己已经知道会出现哪些证据，那说明你已经相当确信自己的理论了——其概率已接近 1——而此时概率几乎没有进一步上升的空间。不论你多么不愿相信会有反驳证据出现，**它带来的下调幅度必须恰好抵消你预期的上调幅度**。你预期的后验概率加权平均值，必须等于你当前的先验概率。

那么，如果你愿意投入时间去研究一个问题，却又[害怕自己的信念被下调](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/)，这到底有多荒谬？从平均角度来看，你必须期待每一次观察都可能同样导致信念的升高或降低。

也许确实会有一波又一波轻微的反对证据不断出现，而支持性证据却迟迟不来。你会发现自己的信念逐渐下滑，越来越低。直到最后，你终于意识到——风向已变，证据正朝着不利于你的方向吹来。在那一刻，**没有必要再找借口**。在那一刻，你已经放下了你所珍视的信念。

**太棒了！值得庆祝！** 开瓶香槟，或者叫外卖披萨庆祝一下！毕竟，如果你总是坚持最初的信念，你就永远不会变得更强大。

---


## Update Yourself Incrementally

by Eliezer Yudkowsky

[Politics is the mind-killer](https://www.lesswrong.com/lw/gw/politics_is_the_mindkiller/).  Debate is war, [arguments are soldiers](https://www.lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/).  There is the temptation to search for ways to [interpret every possible experimental result](https://www.lesswrong.com/lw/i4/belief_in_belief/) to confirm your theory, like securing a citadel against every possible line of attack.  This you cannot do.  It is mathematically impossible.[For every expectation of evidence, there is an equal and opposite expectation of counterevidence.](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/)

But it’s okay if your cherished belief isn’t perfectly defended. If the hypothesis is that the coin comes up heads 95% of the time, then one time in twenty you will expect to see what looks like contrary evidence. This is okay. It’s normal. It’s even expected, so long as you’ve got nineteen supporting observations for every contrary one. A probabilistic model can [take a hit or two](https://www.lesswrong.com/lw/ig/i_defy_the_data/), and still survive, so long as the hits don't keep on coming in.

Yet it is widely believed, especially in the court of public opinion, that a true theory can have no failures and a false theory no successes.

You find people holding up a single piece of what they conceive to be evidence, and claiming that their theory can “explain” it, as though this were all the support that any theory needed. Apparently a false theory can have no supporting evidence; it is impossible for a false theory to fit even a single event. Thus, a single piece of confirming evidence is all that any theory needs.

It is only slightly less foolish to hold up a single piece of probabilistic counterevidence as disproof, as though it were impossible for a correct theory to have even a slight argument against it. But this is how humans have argued for ages and ages, trying to defeat all enemy arguments, while denying the enemy even a single shred of support. People want their debates to be one-sided; they are accustomed to a world in which their preferred theories have not one iota of antisupport. Thus, allowing a single item of probabilistic counterevidence would be the end of the world.

I just know someone in the audience out there is going to say, “But you can’t concede even a single point if you want to win debates in the real world! If you concede that any counterarguments exist, the Enemy will harp on them over and over—you can’t let the Enemy do that! You’ll lose! What could be more viscerally terrifying than that?”

Whatever. Rationality is not for winning debates, it is for deciding which side to join. If you’ve already decided which side to argue for, the work of rationality is done within you, whether well or poorly. But how can you, yourself, decide which side to argue? If choosing the wrong side is viscerally terrifying, even just a little viscerally terrifying, you’d best integrate all the evidence.

Rationality is not a walk, but a dance. On each step in that dance your foot should come down in exactly the correct spot, neither to the left nor to the right. Shifting belief upward with each iota of confirming evidence. Shifting belief downward with each iota of contrary evidence. Yes, down. Even with a correct model, if it is not an exact model, you will sometimes need to revise your belief down.

If an iota or two of evidence happens to countersupport your belief, that’s okay. It happens, sometimes, with probabilistic evidence for non-exact theories. (If an exact theory fails, you are in trouble!) Just shift your belief downward a little—the probability, the odds ratio, or even a nonverbal weight of credence in your mind. Just shift downward a little, and [wait for more evidence](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/). If the theory is true, supporting evidence will come in shortly, and the probability will climb again. If the theory is false, you don’t really want it anyway.

The problem with using black-and-white, binary, qualitative reasoning is that any single observation either destroys the theory or it does not. When not even a single contrary observation is allowed, [it creates cognitive dissonance and has to be argued away](https://www.lesswrong.com/lw/ig/i_defy_the_data/). And this rules out incremental progress; it rules out correct integration of all the evidence. Reasoning probabilistically, we realize that on average, a correct theory will generate a greater weight of support than countersupport. And so you can, without fear, say to yourself: “This is gently contrary evidence, I will shift my belief downward.” Yes, down. It does not destroy your cherished theory. That is qualitative reasoning; think quantitatively.

[For every expectation of evidence, there is an equal and opposite expectation of counterevidence](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/). On every occasion, you must, on average, anticipate revising your beliefs downward as much as you anticipate revising them upward. If you think you already know what evidence will come in, then you must already be fairly sure of your theory—probability close to 1—which doesn’t leave much room for the probability to go further upward. And however unlikely it seems that you will encounter disconfirming evidence, the resulting downward shift must be large enough to precisely balance the anticipated gain on the other side. The weighted mean of your expected posterior probability must equal your prior probability.

How silly is it, then, to be [terrified](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/) of revising your probability downward, if you’re bothering to investigate a matter at all? On average, you must anticipate as much downward shift as upward shift from every individual observation.

It may perhaps happen that an iota of antisupport comes in again, and again and again, while new support is slow to trickle in. You may find your belief drifting downward and further downward. Until, finally, you realize from which quarter the winds of evidence are blowing against you. In that moment of realization, there is no point in constructing excuses. In that moment of realization, you have already relinquished your cherished belief. Yay! Time to celebrate! Pop a champagne bottle or send out for pizza! You can’t become stronger by keeping the beliefs you started with, after all.
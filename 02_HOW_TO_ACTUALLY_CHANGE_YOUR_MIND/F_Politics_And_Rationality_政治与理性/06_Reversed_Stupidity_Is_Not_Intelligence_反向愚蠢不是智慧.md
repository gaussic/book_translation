## 反向的愚蠢并不是智慧

作者：Eliezer Yudkowsky

> “……然后我们那条时间线上的人就开始采取纠正措施。看这里。”
>
> 他擦掉屏幕，然后开始输入一组组指令。一页又一页的报告浮现在屏幕上，都是人们声称看到过神秘飞碟的记录，而每一份报告都比前一份更加荒诞离奇。
>
> “标准的掩盖法，”Verkan Vall 咧嘴一笑，“我只听到过关于‘飞碟’的一点传闻，还是以玩笑的形式。在那种文化背景下，你总可以通过设置十个明显虚假的故事来削弱一个真实的故事。”
>
> ——H. Beam Piper，《警察行动》

Piper 说得没错。就我个人而言，我并不相信这里有哪怕是藏得不太好的外星人。但我的不信，不是因为飞碟邪教的那种尴尬又荒谬的非理性——至少，我希望不是。

你我都相信，飞碟邪教是在完全没有飞碟的情况下出现的。由于人类的愚蠢，几乎任何想法都可能催生出一个邪教。而这种愚蠢与外星人干预是正交的：无论有没有飞碟，我们都预期会出现飞碟邪教。即使真的有外星人，而且他们藏得不太好，飞碟邪教的出现概率也不会变低。条件概率 P(邪教|有外星人) 并不低于 P(邪教|无外星人)，除非你假设那些藏得不好的外星人还会刻意打压飞碟邪教的出现。<sup>1</sup> 按照贝叶斯的证据定义，观察到“飞碟邪教存在”这一事实，并不能作为反对外星人存在的证据。它对是否存在飞碟几乎没有信息价值。

这体现了一个普遍原则，正如 Robert Pirsig 所说：“世界上最愚蠢的人也可能说太阳在照耀，但这不意味着天就黑了。”<sup>2</sup>

如果你认识一个人在是非问题上有 99.99% 的错误率，那你只需要反向他的答案，就能获得 99.99% 的正确率。但前提是他得能可靠地反相关——也就是说，他必须能获取与现实纠缠在一起的优质证据，并对其做出一致而连贯的处理，才能稳定地产生错误的结论。他得拥有超凡的智慧，才能愚蠢到这种程度。

一个坏掉的引擎无法以每小时 200 英里的速度倒车运行——即使它坏得非常彻底。

如果愚蠢并不能稳定地与真理反相关，那“邪恶”更不可能做到这一点。光环效应的反面叫做“恶魔效应”：一切被感知为负面的特质都会互相关联。如果斯大林是邪恶的，那他所说的一切也都应是错误的。你可不想和斯大林观点一致，对吧？

但斯大林也相信 2 + 2 = 4。可如果你为斯大林说过的任何一句话辩护——哪怕是“2 + 2 = 4”——人们看到的也只是：你“在为斯大林说话”；你显然是站在他那边的。

从这个原则引申出若干推论：

* 如果你想诚实地反驳某个观点，你应该针对最强支持者提出的最佳论点来反驳。反驳弱者的论点没有任何说服力，因为即使是最强的观点，也会吸引弱者拥护者。如果你想反对人类增强（transhumanism）或智能爆炸的理论，你得直接挑战 Nick Bostrom 或 2003 年之后的 Eliezer Yudkowsky 的论点。**最不方便的路径才是唯一有效的路径。**<sup>3</sup>

* 拿出一些因为接触某个理念而变疯的可怜疯子，并不能作为对那个理念的反证。很多新时代信徒就是因为接触到量子力学才变得更疯了。

* 有人曾说过：“不是所有保守派都是傻瓜，但大多数傻瓜都是保守派。”如果你无法将自己置于这样一种思维状态中——把这句话（不论真假）当作对保守主义完全无关紧要的批评——那么你还没有准备好理性地思考政治。

* 人身攻击（ad hominem）不是有效的论证手段。

* 你必须能在不提“希特勒想要灭绝犹太人”的前提下，反对种族灭绝。换句话说：如果希特勒从没提倡过种族灭绝，这难道就会让它变得可以接受吗？

* 用 Hanson 的说法：你愿意相信某件事，往往会随着你是否愿意与那些相信这件事的人划归一类而改变——和这件事本身是否为真并无关系。有些人不愿意相信“上帝不存在”，并不是因为存在什么“上帝存在”的证据，而是因为他们不想与 Richard Dawkins 或那些“咄咄逼人”的无神论者为伍。

* 如果你现在的电脑坏了，这不代表你需要一个完全不同的系统，不带 AMD 处理器、不用 ATI 显卡、不用 Maxtor 硬盘、甚至不装机箱风扇——尽管你现在的电脑有这些东西，而且坏了。也许你只是需要一根新的电源线。

* 如果一百个发明家用金属、木头和帆布造飞行器失败了，这并不意味着你该用骨头和血肉造一架飞机。如果有一千个项目在用电力计算机制造人工智能时失败了，这并不意味着“电力”才是问题的根源。在你真正理解问题之前，靠“反着来”撞上正确解的可能性极低。<sup>4</sup>

---

<sup>1</sup> 可理解为：“已知外星人来过地球时，飞碟邪教出现的概率”与“已知外星人没来过地球时，飞碟邪教出现的概率”。

<sup>2</sup> Robert M. Pirsig，《禅与摩托车维修艺术》（Zen and the Art of Motorcycle Maintenance），1974 年出版。

<sup>3</sup> 参见 Scott Alexander，《最不方便的可能世界》（The Least Convenient Possible World），发表于 LessWrong 博客，2018年12月2日：[http://lesswrong.com/lw/2k/the\_least\_convenient\_possible\_world/](http://lesswrong.com/lw/2k/the_least_convenient_possible_world/)

<sup>4</sup> 也可参考《卖非苹果》（Selling Nonapples）：[http://lesswrong.com/lw/vs/selling\_nonapples](http://lesswrong.com/lw/vs/selling_nonapples)

---

## Reversed Stupidity Is Not Intelligence

by Eliezer Yudkowsky

> “. . . then our people on that time-line went to work with corrective action. Here.”
> 
> He wiped the screen and then began punching combinations. Page after page appeared, bearing accounts of people who had claimed to have seen the mysterious disks, and each report was more fantastic than the last.
> 
> “The standard smother-out technique,” Verkan Vall grinned. “I only heard a little talk about the ‘flying saucers,’ and all of that was in joke. In that order of culture, you can always discredit one true story by setting up ten others, palpably false, parallel to it.”
> 
> —H. Beam Piper, Police Operation

Piper had a point. Pers’nally, I don’t believe there are any poorly hidden aliens infesting these parts. But my disbelief has nothing to do with the awful embarrassing irrationality of flying saucer cults—at least, I hope not.

You and I believe that flying saucer cults arose in the total absence of any flying saucers. Cults can arise around almost any idea, thanks to human silliness. This silliness operates orthogonally to alien intervention: We would expect to see flying saucer cults whether or not there were flying saucers. Even if there were poorly hidden aliens, it would not be any less likely for flying saucer cults to arise. The conditional probability P(cults|aliens) isn’t less than P(cults|¬aliens), unless you suppose that poorly hidden aliens would deliberately suppress flying saucer cults.<sup>1</sup> By the Bayesian definition of evidence, the observation “flying saucer cults exist” is not evidence against the existence of flying saucers. It’s not much evidence one way or the other.

This is an application of the general principle that, as Robert Pirsig puts it, “The world’s greatest fool may say the Sun is shining, but that doesn’t make it dark out.”<sup>2</sup>

If you knew someone who was wrong 99.99% of the time on yes-or-no questions, you could obtain 99.99% accuracy just by reversing their answers. They would need to do all the work of obtaining good evidence entangled with reality, and processing that evidence coherently, just to anticorrelate that reliably. They would have to be superintelligent to be that stupid.

A car with a broken engine cannot drive backward at 200 mph, even if the engine is really really broken.

If stupidity does not reliably anticorrelate with truth, how much less should human evil anticorrelate with truth? The converse of the halo effect is the horns effect: All perceived negative qualities correlate. If Stalin is evil, then everything he says should be false. You wouldn’t want to agree with Stalin, would you?

Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by Stalin, even “2 + 2 = 4,” people will see only that you are “agreeing with Stalin”; you must be on his side.

Corollaries of this principle:

- To argue against an idea honestly, you should argue against the best arguments of the strongest advocates. Arguing against weaker advocates proves nothing, because even the strongest idea will attract weak advocates. If you want to argue against transhumanism or the intelligence explosion, you have to directly challenge the arguments of Nick Bostrom or Eliezer Yudkowsky post-2003. The least convenient path is the only valid one.<sup>3</sup>
- Exhibiting sad, pathetic lunatics, driven to madness by their apprehension of an Idea, is no evidence against that Idea. Many New Agers have been made crazier by their personal apprehension of quantum mechanics.
Someone once said, “Not all conservatives are stupid, but most stupid people are conservatives.” If you cannot place yourself in a state of mind where this statement, true or false, seems completely irrelevant as a critique of conservatism, you are not ready to think rationally about politics.
- Ad hominem argument is not valid.
- You need to be able to argue against genocide without saying “Hitler wanted to exterminate the Jews.” If Hitler hadn’t advocated genocide, would it thereby become okay?
- In Hansonian terms: Your instinctive willingness to believe something will change along with your willingness to affiliate with people who are known for believing it—quite apart from whether the belief is actually true. Some people may be reluctant to believe that God does not exist, not because there is evidence that God does exist, but rather because they are reluctant to affiliate with Richard Dawkins or those darned “strident” atheists who go around publicly saying “God does not exist.”
- If your current computer stops working, you can’t conclude that everything about the current system is wrong and that you need a new system without an AMD processor, an ATI video card, a Maxtor hard drive, or case fans—even though your current system has all these things and it doesn’t work. Maybe you just need a new power cord.
- If a hundred inventors fail to build flying machines using metal and wood and canvas, it doesn’t imply that what you really need is a flying machine of bone and flesh. If a thousand projects fail to build Artificial Intelligence using electricity-based computing, this doesn’t mean that electricity is the source of the problem. Until you understand the problem, hopeful reversals are exceedingly unlikely to hit the solution.<sup>4</sup>

---

<sup>1</sup>Read “P(cults|aliens)” as “the probability of UFO cults given that aliens have visited Earth,” and read “P(cults|¬aliens)” as “the probability of UFO cults given that aliens have not visited Earth.”

<sup>2</sup>Robert M. Pirsig, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, 1st ed. (New York: Morrow, 1974).

<sup>3</sup>See Scott Alexander, “The Least Convenient Possible World,” Less Wrong (blog), December 2, 2018, http://lesswrong.com/lw/2k/the_least_convenient_possible_world/.

<sup>4</sup>See also “Selling Nonapples.” http://lesswrong.com/lw/vs/selling_nonapples.
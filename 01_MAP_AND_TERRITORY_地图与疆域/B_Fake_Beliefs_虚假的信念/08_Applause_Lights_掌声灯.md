## 掌声灯

作者：Eliezer Yudkowsky

在 2007 年奇点峰会上，有位演讲者呼吁以民主、多国合作的方式开发人工智能。于是我走到麦克风前问道：

> 假设一群民主共和国组成联盟来开发 AI，过程中充满了政治博弈——有些利益集团影响力异常大，另一些则被边缘化——换句话说，结果就像现代民主国家的产物。或者，假设一群叛逆的极客在地下室开发出 AI，并指示 AI 调查全世界每个人的意见——给没有手机的人发手机——然后按多数人的意见行事。你觉得哪种更“民主”？你会对哪种方式感到安心吗？

我想知道他到底相信民主政治过程的实用性，还是相信投票的道德正当性。但演讲者回答：

> 第一种情况听起来像《理性》杂志的社论，第二种像好莱坞电影情节。

我有些困惑，又问：

> 那你心目中的民主过程是什么样的？

演讲者答：

> 类似人类基因组计划——那是一个国际赞助的研究项目。

我又问：

> 在像人类基因组计划这样的结构中，不同利益集团如何解决冲突？

演讲者说：

> 我不知道。

这让我想起某个独裁者的名言。当被问及是否有意让他的宠物国家走向民主时，他说：

> 我们相信我们已经处于民主制度之中。只是还缺少一些因素，比如表达人民意志。

民主的实质，是解决政策冲突的具体机制。如果所有群体都偏好同样的政策，就根本不需要民主——大家会自动合作。解决冲突的过程可以是直接多数投票、选举立法机构，甚至是 AI 根据投票者意愿做出反应，但无论如何必须有个机制。如果你没有想好冲突解决机制，呼吁“民主”方案到底是什么意思？

我认为，这意味着你说出了“民主”这个词，于是观众就应该鼓掌。这与其说是命题陈述或信念，不如说是类似于演播厅里提示观众鼓掌的“掌声灯”。

这个案例之所以特别，只是因为我把掌声灯误当成了政策建议，结果让大家都很尴尬。大多数掌声灯要明显得多，可以用简单的反转测试检测出来。例如，有人说：

> 我们需要平衡 AI 的风险与机遇。

如果你反过来说：

> 我们不应该平衡 AI 的风险与机遇。

由于反转后的说法很反常，原说法大概率是正常的，也就是说它没有传递新信息。

当然，有很多正当理由说出单独看起来无信息量的句子。“我们需要平衡 AI 的风险与机遇”可以用来引入讨论话题，强调某个具体平衡方案的重要性，或批评某个不平衡的提案。把它和具体主张关联起来，可能会给有限理性的听众带来新信息——这种关联本身未必显而易见。但如果没有具体内容跟进，这句话很可能就是掌声灯。

我有时很想做一场全是掌声灯的演讲，看看观众多久才会开始发笑：

> 我今天来向大家提议，我们需要平衡先进人工智能的风险与机遇。我们应该避免风险，并在可能的情况下实现机遇。我们不应无谓地面对完全不必要的危险。为了实现这些目标，我们必须明智且理性地规划。我们不应因恐惧和恐慌而行动，也不应屈服于技术恐惧症；但同样也不应盲目乐观。我们应尊重所有在奇点中有利益相关方的利益。我们必须努力确保先进技术的好处惠及尽可能多的人，而不是只让少数人受益。我们必须尽量避免用这些技术引发暴力冲突；也必须防止大规模毁灭性能力落入个人之手。我们应该在还来得及采取行动之前，认真思考这些问题……

---

## Applause Lights

by Eliezer Yudkowsky

At the Singularity Summit 2007, one of the speakers called for democratic, multinational development of artificial intelligence. So I stepped up to the microphone and asked:

> Suppose that a group of democratic republics form a consortium to develop AI, and there’s a lot of politicking during the process—some interest groups have unusually large influence, others get shafted—in other words, the result looks just like the products of modern democracies. Alternatively, suppose a group of rebel nerds develops an AI in their basement, and instructs the AI to poll everyone in the world—dropping cellphones to anyone who doesn’t have them—and do whatever the majority says. Which of these do you think is more “democratic,” and would you feel safe with either?

I wanted to find out whether he believed in the pragmatic adequacy of the democratic political process, or if he believed in the moral rightness of voting. But the speaker replied:

> The first scenario sounds like an editorial in Reason magazine, and the second sounds like a Hollywood movie plot.

Confused, I asked:

> Then what kind of democratic process did you have in mind?

The speaker replied:

> Something like the Human Genome Project—that was an internationally sponsored research project.

I asked:

> How would different interest groups resolve their conflicts in a structure like the Human Genome Project?

And the speaker said:

> I don’t know.

This exchange puts me in mind of a quote from some dictator or other, who was asked if he had any intentions to move his pet state toward democracy:

> We believe we are already within a democratic system. Some factors are still missing, like the expression of the people’s will.

The substance of a democracy is the specific mechanism that resolves policy conflicts. If all groups had the same preferred policies, there would be no need for democracy—we would automatically cooperate. The resolution process can be a direct majority vote, or an elected legislature, or even a voter-sensitive behavior of an artificial intelligence, but it has to be something. What does it mean to call for a “democratic” solution if you don’t have a conflict-resolution mechanism in mind?

I think it means that you have said the word “democracy,” so the audience is supposed to cheer. It’s not so much a propositional statement or belief, as the equivalent of the “Applause” light that tells a studio audience when to clap.

This case is remarkable only in that I mistook the applause light for a policy suggestion, with subsequent embarrassment for all. Most applause lights are much more blatant, and can be detected by a simple reversal test. For example, suppose someone says:

> We need to balance the risks and opportunities of AI.

If you reverse this statement, you get:

> We shouldn’t balance the risks and opportunities of AI.

Since the reversal sounds abnormal, the unreversed statement is probably normal, implying it does not convey new information.

There are plenty of legitimate reasons for uttering a sentence that would be uninformative in isolation. “We need to balance the risks and opportunities of AI” can introduce a discussion topic; it can emphasize the importance of a specific proposal for balancing; it can criticize an unbalanced proposal. Linking to a normal assertion can convey new information to a bounded rationalist—the link itself may not be obvious. But if no specifics follow, the sentence is probably an applause light.

I am tempted to give a talk sometime that consists of nothing but applause lights, and see how long it takes for the audience to start laughing:

> I am here to propose to you today that we need to balance the risks and opportunities of advanced artificial intelligence. We should avoid the risks and, insofar as it is possible, realize the opportunities. We should not needlessly confront entirely unnecessary dangers. To achieve these goals, we must plan wisely and rationally. We should not act in fear and panic, or give in to technophobia; but neither should we act in blind enthusiasm. We should respect the interests of all parties with a stake in the Singularity. We must try to ensure that the benefits of advanced technologies accrue to as many individuals as possible, rather than being restricted to a few. We must try to avoid, as much as possible, violent conflicts using these technologies; and we must prevent massive destructive capability from falling into the hands of individuals. We should think through these issues before, not after, it is too late to do anything about them . . .
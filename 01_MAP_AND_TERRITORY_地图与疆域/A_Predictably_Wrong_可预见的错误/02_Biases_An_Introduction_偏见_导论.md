## 偏见：导论

作者：Rob Bensinger

想象你把手伸进一个装有七十个白球和三十个红球的罐子里，随机抓出十个神秘球。

也许你抓到的十个球里有三个是红色的，你就能正确猜出罐子里红球的总数。也可能你碰巧抓到四个红球，或者其他数量。那你很可能就会猜错总数。

这种随机误差是知识不完全的代价，就误差而言，这其实不算太糟。你的估算平均来看不会偏离太多，而且你了解得越多，误差通常会越小。

但另一方面，假如白球更重，会沉到罐子底部。那你抓到的样本就可能在某个方向上持续偏离真实分布。

这种误差就叫“统计偏差”。当你了解世界的方法有偏时，学得越多未必有帮助。收集更多数据甚至可能让有偏的预测变得更糟。

如果你习惯于高度重视知识和探索，这种前景会让人害怕。如果我们想确保学得越多越有帮助，而不是反而变得更糟，我们就需要发现并纠正数据中的偏见。

心理学中的“认知偏差”概念与此类似。认知偏差是我们思维方式中的系统性错误，不同于随机错误或仅仅因无知造成的错误。统计偏差让样本与总体不符，认知偏差则让我们的思维与真相偏离（或更难实现其他目标）。

也许你有乐观偏差，你发现红球可以用来治疗困扰你兄弟的罕见热带疾病，于是你因为希望红球多而高估了罐子里红球的数量。

和统计偏差一样，认知偏差会扭曲我们对现实的看法，光靠收集更多数据并不总能解决，而且它们的影响会随着时间累积。但当你要修正的测量工具就是你自己时，去偏见就成了独特的挑战。

不过，这正是最该开始的地方。因为如果你都不能信任自己的大脑，还能信任什么呢？

### 觉察偏见

想象你第一次见到某人，只知道他们很害羞。

问题：这个人更可能是图书管理员，还是推销员？

大多数人会回答“图书管理员”。但这是个错误：害羞的推销员比害羞的图书管理员多得多，因为推销员本来就比图书管理员多——在美国大约多七十五倍。<sup>1</sup>

这就是“基率忽略”：人们根据特征组合的契合感来判断，而忽略了这些特征在总体中的常见程度。<sup>2</sup> 另一个认知偏差的例子是“沉没成本谬误”——人们倾向于对已经投入资源的事物继续投入，而不是及时止损。

不幸的是，知道这些偏见，并不能让你免疫。甚至不意味着你能觉察到它们的发生。

在一项“偏见盲区”研究中，实验对象预测，如果他们知道一幅画是名画家所作，就更难客观评价其质量。实验者后来验证了他们的预测，发现这些对象确实表现出了他们预期的偏见。但事后问他们时，这些对象却坚称自己的评价是客观的，没有受到偏见影响。<sup>3</sup>

即使我们能正确识别他人的偏见，在面对自身缺陷时，我们也会表现出“偏见盲区”。<sup>4</sup> 当我们内省时没发现“有偏见的想法”，就得出“我比别人更少偏见”的结论。<sup>5</sup>

不过，识别并克服偏见是可能的，只是并不容易。比如，已知通过把概率当作对象或事件的频率来思考，能减少基率忽略。

本书的去偏见方法，是系统性地讲解良好推理为何有效，以及大脑为何会偏离它。如果本书做到了这一点，其方法可与 Serfas（2010）描述的类似，他指出“多年金融相关工作经验”并不会影响人们对沉没成本偏见的易感性，而“参加过的会计课程数量”则有帮助。

> 因此，可能有必要区分经验和专业知识，专业知识指的是“发展出包含对问题概念性理解的图式原则”，这反过来能让决策者识别特定偏见。然而，利用专业知识作为对策不仅仅是熟悉情境内容或成为某领域专家。它要求你完全理解该偏见背后的原理，能在特定情境中发现它，并有合适的工具加以应对。<sup>6</sup>

本书的目标，是为建立理性“专业知识”打下基础。这意味着要深入理解一个非常普遍的问题结构：人类偏见、自我欺骗，以及复杂思维如何自我挫败的千百种路径。

### 关于本书

《地图与疆域》最初是一系列由决策理论家 Eliezer Yudkowsky 撰写的随笔，发表于 2006-2009 年间的经济学博客 Overcoming Bias 及其衍生社区博客 Less Wrong。主题相关的文章被归为“序列”，相关序列又被归为书籍。《地图与疆域》是六本书中的第一本，这一系列统称为《理性：从 AI 到僵尸》。<sup>7</sup>

在风格上，这套书从“生动的教科书”到“轶事集锦”再到“激进宣言”不等，内容也相应多样。这本理性入门书常常带有个人色彩和不拘一格——比如引用 Yudkowsky 与其正统犹太教母亲（精神科医生）和父亲（物理学家）的经历，以及聊天室和邮件列表中的讨论。熟悉 Yudkowsky 的读者会在[《哈利·波特与理性方法》](http://hpmor.com/)中看到同样的反叛精神和许多相同主题。

哲学家阿尔弗雷德·柯日布斯基曾写道：“地图不是它所代表的疆域，但如果正确，它的结构会与疆域相似，这正是它有用的原因。”正如柯日布斯基所说，这里关于地图的话，同样适用于信念、断言和语言。

“地图不是疆域。”这个看似简单的命题，是本书及其收录的四个文章序列的核心思想：[可预见的错误](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM)，讲述我们的信念如何系统性地未能映射真实世界；[伪信念](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re)，探讨信念如何才能算作“地图”；[觉察困惑](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK)，分析大脑如何实现世界映射；以及[神秘答案](https://www.lesswrong.com/s/5uZQHpecjn7955faL)，将这些观点汇聚碰撞。最后以“简单的真理”收尾，这是关于真理本身的独立对话。

人类并不理性；但正如行为经济学家 Dan Ariely 所说，我们的非理性是有规律可循的。我们出错有模式，不出错时也有模式。这两者都值得更深入的理解，而这种理解带来了希望，让我们能据此为自己创造更美好的未来。

---

<sup>1</sup> Wayne Weiten, 《心理学：主题与变异》，简明版，第八版（Cengage Learning，2010）。

<sup>2</sup> Richards J. Heuer, 《情报分析心理学》（美国中央情报局情报研究中心，1999）。

<sup>3</sup> Katherine Hansen 等，《人们在明知使用有偏策略后仍声称客观》，《人格与社会心理学通报》40卷6期（2014）：691–699。

<sup>4</sup> Emily Pronin, Daniel Y. Lin, Lee Ross，《偏见盲区：自我与他人偏见的感知》，《人格与社会心理学通报》28卷3期（2002）：369–381。

<sup>5</sup> Joyce Ehrlinger, Thomas Gilovich, Lee Ross，《窥视偏见盲区：人们对自己和他人偏见的评估》，《人格与社会心理学通报》31卷5期（2005）：680–692。

<sup>6</sup> Sebastian Serfas, 《资本投资情境下的认知偏差：规范理性违背的理论与实证研究》（Springer，2010）。

<sup>7</sup> 《理性：从AI到僵尸》第一版最初以一部庞大的电子书形式发布，后来才编辑拆分为多卷。全书可在 http://lesswrong.com/rationality 阅读。

---

## Biases: An Introduction

by Rob Bensinger

Imagine reaching into an urn that contains seventy white balls and thirty red ones, and plucking out ten mystery balls.

Perhaps three of the ten balls will be red, and you’ll correctly guess how many red balls total were in the urn. Or perhaps you’ll happen to grab four red balls, or some other number. Then you’ll probably get the total number wrong.

This random error is the cost of incomplete knowledge, and as errors go, it’s not so bad. Your estimates won’t be incorrect on average, and the more you learn, the smaller your error will tend to be.

On the other hand, suppose that the white balls are heavier, and sink to the bottom of the urn. Then your sample may be unrepresentative in a consistent direction.

That kind of error is called “statistical bias.” When your method of learning about the world is biased, learning more may not help. Acquiring more data can even consistently worsen a biased prediction.

If you’re used to holding knowledge and inquiry in high esteem, this is a scary prospect. If we want to be sure that learning more will help us, rather than making us worse off than we were before, we need to discover and correct for biases in our data.

The idea of cognitive bias in psychology works in an analogous way. A cognitive bias is a systematic error in how we think, as opposed to a random error or one that’s merely caused by our ignorance. Whereas statistical bias skews a sample so that it less closely resembles a larger population, cognitive biases skew our thinking so that it less accurately tracks the truth (or less reliably serves our other goals).

Maybe you have an optimism bias, and you find out that the red balls can be used to treat a rare tropical disease besetting your brother, and you end up overestimating how many red balls the urn contains because you wish the balls were mostly red.

Like statistical biases, cognitive biases can distort our view of reality, they can’t always be fixed by just gathering more data, and their effects can add up over time. But when the miscalibrated measuring instrument you’re trying to fix is you, debiasing is a unique challenge.

Still, this is an obvious place to start. For if you can’t trust your brain, how can you trust anything else?

### Noticing Bias

Imagine meeting someone for the first time, and knowing nothing about them except that they’re shy.

Question: Is it more likely that this person is a librarian, or a salesperson?

Most people answer “librarian.” Which is a mistake: shy salespeople are much more common than shy librarians, because salespeople in general are much more common than librarians—seventy-five times as common, in the United States.¹

This is base rate neglect: grounding one’s judgments in how well sets of characteristics feel like they fit together, and neglecting how common each characteristic is in the population at large.² Another example of a cognitive bias is the sunk cost fallacy—people’s tendency to feel committed to things they’ve spent resources on in the past, when they should be cutting their losses and moving on.

Knowing about these biases, unfortunately, doesn’t make you immune to them. It doesn’t even mean you’ll be able to notice them in action.

In a study of bias blindness, experimental subjects predicted that they would have a harder time neutrally evaluating the quality of paintings if they knew the paintings were by famous artists. And indeed, these subjects exhibited the very bias they had predicted when the experimenters later tested their prediction. When asked afterward, however, the very same subjects claimed that their assessments of the paintings had been objective and unaffected by the bias.³

Even when we correctly identify others’ biases, we exhibit a bias blind spot when it comes to our own flaws.⁴ Failing to detect any “biased-feeling thoughts” when we introspect, we draw the conclusion that we must just be less biased than everyone else.⁵

Yet it is possible to recognize and overcome biases. It’s just not trivial. It’s known that subjects can reduce base rate neglect, for example, by thinking of probabilities as frequencies of objects or events.

The approach to debiasing in this book is to communicate a systematic understanding of why good reasoning works, and of how the brain falls short of it. To the extent this volume does its job, its approach can be compared to the one described in Serfas (2010), who notes that “years of financially related work experience” didn’t affect people’s susceptibility to the sunk cost bias, whereas “the number of accounting courses attended” did help.

> As a consequence, it might be necessary to distinguish between experience and expertise, with expertise meaning “the development of a schematic principle that involves conceptual understanding of the problem,” which in turn enables the decision maker to recognize particular biases. However, using expertise as countermeasure requires more than just being familiar with the situational content or being an expert in a particular domain. It requires that one fully understand the underlying rationale of the respective bias, is able to spot it in the particular setting, and also has the appropriate tools at hand to counteract the bias.⁶

The goal of this book is to lay the groundwork for creating rationality “expertise.” That means acquiring a deep understanding of the structure of a very general problem: human bias, self-deception, and the thousand paths by which sophisticated thought can defeat itself.

### A Word About This Text

Map and Territory began its life as a series of essays by decision theorist Eliezer Yudkowsky, published between 2006 and 2009 on the economics blog Overcoming Bias and its spin-off community blog Less Wrong. Thematically linked essays were grouped together in “sequences,” and thematically linked sequences were grouped into books. Map and Territory is the first of six such books, with the series as a whole going by the name Rationality: From AI to Zombies.⁷

In style, this series run the gamut from “lively textbook” to “compendium of vignettes” to “riotous manifesto,” and the content is correspondingly varied. The resultant rationality primer is frequently personal and irreverent—drawing, for example, from Yudkowsky’s experiences with his Orthodox Jewish mother (a psychiatrist) and father (a physicist), and from conversations on chat rooms and mailing lists. Readers who are familiar with Yudkowsky from [Harry Potter and the Methods of Rationality](http://hpmor.com/), his science-oriented take-off of J.K. Rowling’s Harry Potter books, will recognize the same iconoclasm, and many of the same themes.

The philosopher Alfred Korzybski once wrote: “A map is not the territory it represents, but, if correct, it has a similar structure to the territory, which accounts for its usefulness.” And what can be said of maps here, as Korzybski noted, can also be said of beliefs, and assertions, and words.

“The map is not the territory.” This deceptively simple claim is the organizing idea behind this book, and behind the four sequences of essays collected here: [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM), which concerns the systematic ways our beliefs fail to map the real world; [Fake Beliefs](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re), on what makes a belief a “map” in the first place; [Noticing Confusion](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK), on how this world-mapping thing our brains do actually works; and [Mysterious Answers](https://www.lesswrong.com/s/5uZQHpecjn7955faL), which collides these points together. The book then concludes with “The Simple Truth,” a stand-alone dialogue on the idea of truth itself.

Humans aren’t rational; but, as behavioral economist Dan Ariely notes, we’re predictably irrational. There are patterns to how we screw up. And there are patterns to how we behave when we don’t screw up. Both admit of fuller understanding, and with it, the hope of leaning on that understanding to build a better future for ourselves.

---

¹ Wayne Weiten, Psychology: Themes and Variations, Briefer Version, Eighth Edition (Cengage Learning, 2010).

² Richards J. Heuer, Psychology of Intelligence Analysis (Center for the Study of Intelligence, Central Intelligence Agency, 1999) .

³ Katherine Hansen et al., “People Claim Objectivity After Knowingly Using Biased Strategies,” Personality and Social Psychology Bulletin 40, no. 6 (2014): 691–699 .

⁴ Emily Pronin, Daniel Y. Lin, and Lee Ross, “The Bias Blind Spot: Perceptions of Bias in Self versus Others,” Personality and Social Psychology Bulletin 28, no. 3 (2002): 369–381 .

⁵ Joyce Ehrlinger, Thomas Gilovich, and Lee Ross, “Peering Into the Bias Blind Spot: People’s Assessments of Bias in Themselves and Others,” Personality and Social Psychology Bulletin 31, no. 5 (2005): 680–692.

⁶ Sebastian Serfas, Cognitive Biases in the Capital Investment Context: Theoretical Considerations and Empirical Experiments on Violations of Normative Rationality (Springer, 2010).

⁷ The first edition of Rationality: From AI to Zombies was released as a single sprawling ebook, before the series was edited and split up into separate volumes. The full book can also be found on http://lesswrong.com/rationality.


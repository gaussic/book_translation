## 期待推理距离很短

智人（Homo sapiens）进化适应的环境（即EEA，祖先环境）是由最多200人的狩猎采集小团体组成的，没有文字。所有的知识都靠口头和记忆代代相传。

在那样的世界里，所有背景知识都是普遍知识。除了严格的私人信息外，所有信息都是公开的。

在祖先环境中，你和其他人之间的推理距离很难超过一步。当你发现一个新绿洲时，你不需要向部落成员解释什么是绿洲，为什么喝水是好主意，或者怎么走路。只有你知道绿洲的具体位置，这属于私人知识。但每个人都有理解你描述绿洲所需的背景知识，也有思考水的相关概念，这些都是普遍知识。在那样的环境下，你几乎从不用解释自己的概念。最多只需要解释一个新概念，而不是同时解释两个或更多。

在祖先环境中，没有那些拥有大量证据、被归纳为优雅理论、通过书面书籍传递、其结论距离普遍共识有上百步推理距离的抽象学科。

在祖先环境中，任何说出没有明显依据的话的人，不是骗子就是傻子。你不会想着：“嘿，也许这个人有我们部落没人听说过的、充分支持的背景知识。”因为在祖先环境中，这种事基本不会发生。

反过来，如果你说了个显而易见的事实，对方却没明白，那就是对方傻，或者故意装傻来气你。

更甚者，如果有人说了没有明显依据的话，还指望你相信——当你不信时还一脸愤怒——那他一定是疯了。

结合“透明错觉”和“自我锚定”（即倾向于把他人当作自己稍加修改的版本来建模），我认为这很好地解释了为什么大多数科学家在与普通大众沟通时——甚至与其他学科的科学家沟通时——会遇到著名的困难。当我观察到解释失败时，通常是讲解者只退了一步，而其实需要退两步甚至更多；或者听众以为一两步就能明白，其实需要两步以上。双方都像是默认从普遍知识到任何新知识的推理距离应该很短。

比如，一个生物学家和物理学家交流时，可以用“这是最简单的解释”来为进化论辩护。但地球上不是每个人都受过那种从牛顿到爱因斯坦的科学史熏陶，能体会“最简单的解释”这句话的分量：那是理论诞生时的权威之言，也是理论墓碑上的铭文。对另一些人来说，“但这是最简单的解释！”听起来只是个有趣但远非决定性的理由；它并不像理解办公室政治或修理坏车时那么有用。显然，生物学家只是迷恋自己的想法，太傲慢，不愿接受听起来同样合理的其他解释。（如果对我来说听起来合理，那对我部落里任何理智的人都应该合理。）

而从生物学家的角度，他能理解进化论一开始听起来有点奇怪——但当他解释了“这是最简单的解释”后，对方还拒绝进化论，那就很明显，非科学家都是傻瓜，没必要再和他们交流。

一个清晰的论证必须铺设一条推理路径，从听众已经知道或接受的内容出发。如果你递归得不够深，你其实只是在自言自语。

如果你在某处提出了一个没有明显依据、也没在之前论证中支持过的观点，听众只会觉得你疯了。

当你让自己在听众看来，明显比他们认为合理的程度更重视某个论据时，也会出现同样的问题。例如，你表现得好像“更简单的解释”就是进化论的决定性论据（实际上确实如此），但在没受过奥卡姆剃刀熏陶的人看来，这只是个有点意思的想法。

还有，千万别让听众觉得你自认为自己距离他们的知识有十几步推理距离，或者你有他们无法获得的特殊背景知识。听众根本不知道什么“进化心理学解释的推理距离低估导致沟通拥堵”之类的理论。他们只会觉得你在居高临下。

如果你觉得“系统性低估推理距离”这个概念可以用几句话简单解释清楚，那我只能遗憾地告诉你……

---

## Expecting Short Inferential Distances

Homo sapiens’s environment of evolutionary adaptedness (a.k.a. EEA or “ancestral environment”) consisted of hunter-gatherer bands of at most 200 people, with no writing. All inherited knowledge was passed down by speech and memory.

In a world like that, all background knowledge is universal knowledge. All information not strictly private is public, period.

In the ancestral environment, you were unlikely to end up more than one inferential step away from anyone else. When you discover a new oasis, you don’t have to explain to your fellow tribe members what an oasis is, or why it’s a good idea to drink water, or how to walk. Only you know where the oasis lies; this is private knowledge. But everyone has the background to understand your description of the oasis, the concepts needed to think about water; this is universal knowledge. When you explain things in an ancestral environment, you almost never have to explain your concepts. At most you have to explain one new concept, not two or more simultaneously.

In the ancestral environment there were no abstract disciplines with vast bodies of carefully gathered evidence generalized into elegant theories transmitted by written books whose conclusions are a hundred inferential steps removed from universally shared background premises.

In the ancestral environment, anyone who says something with no obvious support is a liar or an idiot. You’re not likely to think, “Hey, maybe this person has well-supported background knowledge that no one in my band has even heard of,” because it was a reliable invariant of the ancestral environment that this didn’t happen.

Conversely, if you say something blatantly obvious and the other person doesn’t see it, they’re the idiot, or they’re being deliberately obstinate to annoy you.

And to top it off, if someone says something with no obvious support and expects you to believe it—acting all indignant when you don’t—then they must be crazy.

Combined with the illusion of transparency and self-anchoring (the tendency to model other minds as though the were slightly modified versions of oneself), I think this explains a lot about the legendary difficulty most scientists have in communicating with a lay audience—or even communicating with scientists from other disciplines. When I observe failures of explanation, I usually see the explainer taking one step back, when they need to take two or more steps back. Or listeners assume that things should be visible in one step, when they take two or more steps to explain. Both sides act as if they expect very short inferential distances from universal knowledge to any new knowledge.

A biologist, speaking to a physicist, can justify evolution by saying it is the simplest explanation. But not everyone on Earth has been inculcated with that legendary history of science, from Newton to Einstein, which invests the phrase “simplest explanation” with its awesome import: a Word of Power, spoken at the birth of theories and carved on their tombstones. To someone else, “But it’s the simplest explanation!” may sound like an interesting but hardly knockdown argument; it doesn’t feel like all that powerful a tool for comprehending office politics or fixing a broken car. Obviously the biologist is infatuated with their own ideas, too arrogant to be open to alternative explanations which sound just as plausible. (If it sounds plausible to me, it should sound plausible to any sane member of my band.)

And from the biologist’s perspective, they can understand how evolution might sound a little odd at first—but when someone rejects evolution even after the biologist explains that it’s the simplest explanation, well, it’s clear that nonscientists are just idiots and there’s no point in talking to them.

A clear argument has to lay out an inferential pathway, starting from what the audience already knows or accepts. If you don’t recurse far enough, you’re just talking to yourself.

If at any point you make a statement without obvious justification in arguments you’ve previously supported, the audience just thinks you’re crazy.

This also happens when you allow yourself to be seen visibly attaching greater weight to an argument than is justified in the eyes of the audience at that time. For example, talking as if you think “simpler explanation” is a knockdown argument for evolution (which it is), rather than a sortainteresting idea (which it sounds like to someone who hasn’t been raised to revere Occam’s Razor).

Oh, and you’d better not drop any hints that you think you’re working a dozen inferential steps away from what the audience knows, or that you think you have special background knowledge not available to them. The audience doesn’t know anything about an evolutionary-psychological argument for a cognitive bias to underestimate inferential distances leading to traffic jams in communication. They’ll just think you’re condescending.

And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . .
## 期待推理距离很短

作者：Eliezer Yudkowsky

智人的进化适应环境（即 EEA，或称“祖先环境”）是由最多 [200 人](https://en.wikipedia.org/wiki/Dunbar%27s_number) 组成的狩猎采集小团体，没有文字。所有的知识都靠口头和记忆代代相传。

在那样的世界里，所有背景知识都是普遍知识。所有非私人信息都是公开的，仅此而已。

在祖先环境中，你和其他人之间几乎不会相隔超过一步推理。当你发现一个新绿洲时，你不需要向部落成员解释什么是绿洲，为什么喝水是好主意，或者怎么走路。只有你知道绿洲的位置，这是私人知识。但每个人都有理解你描述绿洲所需的背景知识，理解水的概念；这些是普遍知识。在祖先环境中解释事情时，你几乎从不需要解释你的概念。最多只需解释一个新概念，而不是同时解释两个或更多。

在祖先环境中，没有抽象学科，没有庞大的证据体系被归纳为优雅的理论，也没有通过书面文字传递、其结论距离普遍共识有上百步推理距离的知识。

在祖先环境中，任何说出没有明显依据的话的人，不是骗子就是傻子。你不会想：“嘿，也许这个人有我们部落没人听说过的坚实背景知识。”因为在祖先环境中，这种事基本不会发生。

反过来，如果你说了个显而易见的道理，对方却没明白，那他要么是傻子，要么就是故意装傻来气你。

更甚者，如果有人说了没有明显依据的话，还指望你相信——你不信他还一副受委屈的样子——那他肯定疯了。

结合“透明错觉”和[自我锚定](https://www.lesswrong.com/lw/kf/selfanchoring/)（即倾向于把他人当作自己略有变化的版本来建模），我认为这很好地解释了为什么大多数科学家与大众沟通——甚至与其他学科的科学家沟通——会如此困难。当我观察到解释失败时，通常是解释者只往回退了一步，其实需要退两步甚至更多；或者听众以为一两步就能明白，其实需要两步以上。双方都像是默认从普遍知识到新知识的推理距离非常短。

比如，一个生物学家对物理学家说，进化是最简单的解释。但地球上不是每个人都被灌输过那段从牛顿到爱因斯坦的科学史，也没有体会到“最简单解释”这句话的分量：它是理论诞生时的权威之言，也是理论墓碑上的铭文。对别人来说，“但这是最简单的解释！”也许只是个有趣但远非决定性的理由；它并不像理解办公室政治或修理汽车那样有用。显然，这生物学家只是迷恋自己的想法，太傲慢，不愿接受听起来同样合理的其他解释。（如果对我来说听起来合理，对我部落里任何理智的人也该合理。）

而从生物学家的角度，他能理解进化论一开始听起来有点奇怪——但当他解释了“这是最简单的解释”后，对方还拒绝接受，那就说明非科学家都是傻子，没必要再和他们交流。

一个清晰的论证，必须铺设一条推理路径，从听众已经知道或接受的内容出发。如果你递归得不够深，你只是在自言自语。

如果你在某处提出了一个没有明显依据、也没用之前论证支撑的观点，听众只会觉得你疯了。

当你让别人看到你对某个论据赋予的权重，远超出他们此刻能接受的程度时，也会出现同样的问题。例如，你表现得好像“更简单的解释”是进化论的决定性论据（它确实是），但在没被灌输奥卡姆剃刀崇拜的人看来，这只是个“还算有趣”的想法。

哦，还有，千万别让人觉得你是在离他们已知内容十几步推理距离的地方工作，或者你有他们无法获得的特殊背景知识。听众根本不知道什么“进化心理学解释的推理距离低估偏差导致沟通拥堵”之类的理论。他们只会觉得你在居高临下。

如果你以为自己能用几句话简明扼要地解释“系统性低估推理距离”这个概念，那我有个坏消息要告诉你……

---

## Expecting Short Inferential Distances

by Eliezer Yudkowsky

Homo sapiens’s environment of evolutionary adaptedness (a.k.a. EEA or “ancestral environment”) consisted of hunter-gatherer bands of at most [200 people](https://en.wikipedia.org/wiki/Dunbar%27s_number), with no writing. All inherited knowledge was passed down by speech and memory.

In a world like that, all background knowledge is universal knowledge. All information not strictly private is public, period.

In the ancestral environment, you were unlikely to end up more than one inferential step away from anyone else. When you discover a new oasis, you don’t have to explain to your fellow tribe members what an oasis is, or why it’s a good idea to drink water, or how to walk. Only you know where the oasis lies; this is private knowledge. But everyone has the background to understand your description of the oasis, the concepts needed to think about water; this is universal knowledge. When you explain things in an ancestral environment, you almost never have to explain your concepts. At most you have to explain one new concept, not two or more simultaneously.

In the ancestral environment there were no abstract disciplines with vast bodies of carefully gathered evidence generalized into elegant theories transmitted by written books whose conclusions are a hundred inferential steps removed from universally shared background premises.

In the ancestral environment, anyone who says something with no obvious support is a liar or an idiot. You’re not likely to think, “Hey, maybe this person has well-supported background knowledge that no one in my band has even heard of,” because it was a reliable invariant of the ancestral environment that this didn’t happen.

Conversely, if you say something blatantly obvious and the other person doesn’t see it, they’re the idiot, or they’re being deliberately obstinate to annoy you.

And to top it off, if someone says something with no obvious support and expects you to believe it—acting all indignant when you don’t—then they must be crazy.

Combined with the illusion of transparency and [self-anchoring](https://www.lesswrong.com/lw/kf/selfanchoring/) (the tendency to model other minds as though the were slightly modified versions of oneself), I think this explains a lot about the legendary difficulty most scientists have in communicating with a lay audience—or even communicating with scientists from other disciplines. When I observe failures of explanation, I usually see the explainer taking one step back, when they need to take two or more steps back. Or listeners assume that things should be visible in one step, when they take two or more steps to explain. Both sides act as if they expect very short inferential distances from universal knowledge to any new knowledge.

A biologist, speaking to a physicist, can justify evolution by saying it is the simplest explanation. But not everyone on Earth has been inculcated with that legendary history of science, from Newton to Einstein, which invests the phrase “simplest explanation” with its awesome import: a Word of Power, spoken at the birth of theories and carved on their tombstones. To someone else, “But it’s the simplest explanation!” may sound like an interesting but hardly knockdown argument; it doesn’t feel like all that powerful a tool for comprehending office politics or fixing a broken car. Obviously the biologist is infatuated with their own ideas, too arrogant to be open to alternative explanations which sound just as plausible. (If it sounds plausible to me, it should sound plausible to any sane member of my band.)

And from the biologist’s perspective, they can understand how evolution might sound a little odd at first—but when someone rejects evolution even after the biologist explains that it’s the simplest explanation, well, it’s clear that nonscientists are just idiots and there’s no point in talking to them.

A clear argument has to lay out an inferential pathway, starting from what the audience already knows or accepts. If you don’t recurse far enough, you’re just talking to yourself.

If at any point you make a statement without obvious justification in arguments you’ve previously supported, the audience just thinks you’re crazy.

This also happens when you allow yourself to be seen visibly attaching greater weight to an argument than is justified in the eyes of the audience at that time. For example, talking as if you think “simpler explanation” is a knockdown argument for evolution (which it is), rather than a sorta-interesting idea (which it sounds like to someone who hasn’t been raised to revere Occam’s Razor).

Oh, and you’d better not drop any hints that you think you’re working a dozen inferential steps away from what the audience knows, or that you think you have special background knowledge not available to them. The audience doesn’t know anything about an evolutionary-psychological argument for a cognitive bias to underestimate inferential distances leading to traffic jams in communication. They’ll just think you’re condescending.

And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . .
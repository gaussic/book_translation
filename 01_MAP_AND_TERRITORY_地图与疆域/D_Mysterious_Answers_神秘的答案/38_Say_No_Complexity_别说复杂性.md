## 别说“复杂性”

很久很久以前……

这是我第一次见到 Marcello 时的故事，后来我和他一起做了一年 AI 理论研究；但那时我还没正式收他为徒。我知道他在数学和计算机奥林匹克竞赛中是全国级选手，这足以让我想更深入了解他，但我还不知道他是否能学会如何思考 AI。

我让 Marcello 说说他认为 AI 会如何学会解魔方。不是预先编程的那种（那太简单了），而是 AI 自己如何发现魔方世界的规律，并推理如何利用这些规律。AI 会如何自己发明“操作符”或“宏”这样的概念，这是解魔方的关键？

讨论到某个阶段，Marcello 说：“嗯，我觉得 AI 需要复杂性来做 X，也需要复杂性来做 Y——”

我说：“别说‘复杂性’。”

Marcello 问：“为什么？”

我说：“复杂性本身永远不该是目标。你可能需要用某个算法，这会带来一定的复杂性，但为复杂性而复杂，只会让事情更难。”（我当时想到很多人说互联网“足够复杂”时就会“觉醒”变成 AI。）

Marcello 说：“但总得有某种程度的复杂性才能实现吧。”

我闭上眼睛，努力思考该怎么解释。对我来说，说“复杂性”就像是在 AI 的舞蹈中走错了步。没人能快到用语言推敲自己意识流里的每一句话，否则那会陷入无限递归。我们用语言思考，但意识流的走向是在语言之下，由过往的洞见和惨痛经验残留引导的……

我说：“你读过《技术解释的技术解释》吗？”<sup>1</sup>

Marcello 说：“读过。”

我说：“好。说‘复杂性’并不能集中你的概率质量。”

Marcello 说：“哦，像‘涌现’一样。嗯……那我得认真想想 X 到底是怎么发生的了……”

那时我心想：“也许这个人可以教。”

“复杂性”并不是没用的概念。它有数学定义，比如 Kolmogorov 复杂度和 Vapnik-Chervonenkis 复杂度。即使在直观层面，复杂性也常常值得思考——你需要判断一个假说的复杂度，决定在现有证据下它是否“太复杂”，或者审视一个设计，试着让它更简单。

但概念本身没有用没用之分，只有用法对不对。在 Marcello 想迈出的那一步里，他试图“免费”解释某事，想不劳而获。这在我的领域极为常见。你可以去参加一场通用人工智能的讨论，看到人们不断跳过自己不懂的部分，却没意识到自己在跳过。

这一切发生在一瞬间：你在某个神秘现象后面放了一个没有控制力的因果节点，这个节点让你感觉像是解释了什么，其实并没有。这个错误发生在语言之下。它不需要什么性格缺陷；这就是人类默认的思维方式，自古以来如此。

你必须避免的，是跳过神秘之处；你要停留在谜团前，直面它。有很多词都能让你跳过谜团，有些词在别的语境下是合法的——比如“复杂性”。但本质的错误就是跳过，无论你在后面放了什么因果节点。跳过不是思考，而是“微思考”。你必须高度警觉，才能抓住自己跳过的瞬间。当你训练自己避免跳过，这会变成一种本能，而不是语言推理。你要能感觉到地图上哪些地方还是空白，更重要的是，关注这种感觉。

我怀疑学术界有巨大的压力，要把问题扫到地毯下，这样你才能发表一篇看起来完整的论文。如果你有个貌似完整、包含“涌现现象”的模型，会比你明确标注“这里我完全不知道怎么回事”或“这里发生了奇迹”的论文更受欢迎。期刊甚至可能不会接受后者，因为谁知道那些未知步骤里是不是才有真正有趣的东西？<sup>2</sup>

如果你在做革命性的 AI 创业，压力只会更大；否则你就得承认自己还不知道怎么造出正确的 AI，你的人生计划可能会瞬间崩塌。但也许我说得太多了，因为“跳过”对人类来说是默认操作。如果你想找例子，只要看人们讨论宗教、哲学、灵性，或者任何他们没受过专业训练的科学领域。

我和 Marcello 后来在 AI 研究中约定：每当遇到不懂的地方（这种情况很常见），我们就说“魔法”——比如“X 魔法般地实现了 Y”——提醒自己这里有个未解之谜，是理解的空白。说“魔法”远比说“复杂性”或“涌现”好；后两者会制造一种理解的假象。更明智的做法是说“魔法”，给自己留个占位符，提醒以后还要补上这块内容。

---

<sup>1</sup>链接：http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation

<sup>2</sup>确实，有时你地图上所有非魔法的部分最后都不重要。这是你探索未知领域、尝试逐步解决问题时必须付出的代价。但这也让你更需要知道自己还没完成。大多数人根本不敢踏入未知领域，因为太怕浪费时间。

---

## Say Not “Complexity”

Once upon a time . . .

This is a story from when I first met Marcello, with whom I would later work for a year on AI theory; but at this point I had not yet accepted him as my apprentice. I knew that he competed at the national level in mathematical and computing olympiads, which sufficed to attract my attention for a closer look; but I didn’t know yet if he could learn to think about AI.

I had asked Marcello to say how he thought an AI might discover how to solve a Rubik’s Cube. Not in a preprogrammed way, which is trivial, but rather how the AI itself might figure out the laws of the Rubik universe and reason out how to exploit them. How would an AI invent for itself the concept of an “operator,” or “macro,” which is the key to solving the Rubik’s Cube?

At some point in this discussion, Marcello said: “Well, I think the AI needs complexity to do X, and complexity to do Y—”

And I said, “Don’t say ‘complexity.’ ”

Marcello said, “Why not?”

I said, “Complexity should never be a goal in itself. You may need to use a particular algorithm that adds some amount of complexity, but complexity for the sake of complexity just makes things harder.” (I was thinking of all the people whom I had heard advocating that the Internet would “wake up” and become an AI when it became “sufficiently complex.”)

And Marcello said, “But there’s got to be some amount of complexity that does it.”

I closed my eyes briefly, and tried to think of how to explain it all in words. To me, saying “complexity” simply felt like the wrong move in the AI dance. No one can think fast enough to deliberate, in words, about each sentence of their stream of consciousness; for that would require an infinite recursion. We think in words, but our stream of consciousness is steered below the level of words, by the trained-in remnants of past insights and harsh experience . . .

I said, “Did you read ‘A Technical Explanation of Technical Explanation’?”<sup>1</sup>

“Yes,” said Marcello.

“Okay,” I said. “Saying ‘complexity’ doesn’t concentrate your probability mass.”

“Oh,” Marcello said, “like ‘emergence.’ Huh. So . . . now I’ve got to think about how X might actually happen . . .”

That was when I thought to myself, “Maybe this one is teachable.”

Complexity is not a useless concept. It has mathematical definitions attached to it, such as Kolmogorov complexity and Vapnik-Chervonenkis complexity. Even on an intuitive level, complexity is often worth thinking about—you have to judge the complexity of a hypothesis and decide if it’s “too complicated” given the supporting evidence, or look at a design and try to make it simpler.

But concepts are not useful or useless of themselves. Only usages are correct or incorrect. In the step Marcello was trying to take in the dance, he was trying to explain something for free, get something for nothing. It is an extremely common misstep, at least in my field. You can join a discussion on artificial general intelligence and watch people doing the same thing, left and right, over and over again—constantly skipping over things they don’t understand, without realizing that’s what they’re doing.

In an eyeblink it happens: putting a non-controlling causal node behind something mysterious, a causal node that feels like an explanation but isn’t. The mistake takes place below the level of words. It requires no special character flaw; it is how human beings think by default, how they have thought since the ancient times.

What you must avoid is skipping over the mysterious part; you must linger at the mystery to confront it directly. There are many words that can skip over mysteries, and some of them would be legitimate in other contexts— “complexity,” for example. But the essential mistake is that skip-over, regardless of what causal node goes behind it. The skip-over is not a thought, but a microthought. You have to pay close attention to catch yourself at it. And when you train yourself to avoid skipping, it will become a matter of instinct, not verbal reasoning. You have to feel which parts of your map are still blank, and more importantly, pay attention to that feeling.

I suspect that in academia there is a huge pressure to sweep problems under the rug so that you can present a paper with the appearance of completeness. You’ll get more kudos for a seemingly complete model that includes some “emergent phenomena,” versus an explicitly incomplete map where the label says “I got no clue how this part works” or “then a miracle occurs.” A journal may not even accept the latter paper, since who knows but that the unknown steps are really where everything interesting happens?<sup>2</sup>

And if you’re working on a revolutionary AI startup, there is an even huger pressure to sweep problems under the rug; or you will have to admit to yourself that you don’t know how to build the right kind of AI yet, and your current life plans will come crashing down in ruins around your ears. But perhaps I am over-explaining, since skip-over happens by default in humans. If you’re looking for examples, just watch people discussing religion or philosophy or spirituality or any science in which they were not professionally trained.

Marcello and I developed a convention in our AI work: when we ran into something we didn’t understand, which was often, we would say “magic”—as in, X magically does Y”—to remind ourselves that here was an unsolved problem, a gap in our understanding. It is far better to say “magic” than “complexity” or “emergence”; the latter words create an illusion of understanding. Wiser to say “magic,” and leave yourself a placeholder, a reminder of work you will have to do later.

---

<sup>1</sup>Link: http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation.

<sup>2</sup>And yes, it sometimes happens that all the non-magical parts of your map turn out to also be non-important. That’s the price you sometimes pay, for entering into terra incognita and trying to solve problems incrementally. But that makes it even more important to know when you aren’t finished yet. Mostly, people don’t dare to enter terra incognita at all, for the deadly fear of wasting their time.
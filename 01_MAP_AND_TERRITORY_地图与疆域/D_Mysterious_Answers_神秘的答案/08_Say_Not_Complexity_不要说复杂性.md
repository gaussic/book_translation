## 不要说“复杂性”

作者：Eliezer Yudkowsky

从前……

这是我第一次遇见马尔切洛（Marcello）时的一个故事，我们后来共同工作了一年，研究人工智能理论；但在这个时候，我还没有接受他作为我的学徒。我知道他在数学和计算机奥林匹克竞赛中取得过全国级的成绩，这足以引起我对他更深入观察的兴趣；但我还不知道他能否学会如何思考人工智能。

我曾让马尔切洛谈谈他认为人工智能如何发现解决魔方的方法。不是通过预先编程的方式，这显然是微不足道的，而是让人工智能自己找出魔方宇宙的规律，并推理出如何利用这些规律。人工智能如何发明“算子”或“宏”这一概念，而这是解决魔方的关键？

在讨论的某个时刻，马尔切洛说：“嗯，我认为人工智能需要复杂性来做X，需要复杂性来做Y——”

我说：“不要说‘复杂性’。”

马尔切洛问：“为什么不行？”

我说：“复杂性永远不应该是一个目标。你可能需要使用一个特定的算法，来增加一些复杂性，但单纯为了复杂性而去追求复杂性只会让事情变得更难。”（我当时想到的是，许多人曾经提到互联网会在变得“足够复杂”后‘觉醒’，成为一种人工智能。）

马尔切洛说：“但是一定有某种复杂性能够做到。”

我闭上了眼睛，短暂地思考如何用语言来解释这一切。对我来说，说“复杂性”只是感觉在人工智能这场舞蹈中走错了步伐。没有人能足够快地思考并用语言推敲自己意识流中的每一句话；那将需要无限的递归。我们用语言思考，但我们的意识流是在语言之下，被过去的洞察力和严酷经验的痕迹所引导……

我说：“你读过《技术解释的技术解释》吗？”<sup>1</sup>

“读过，”马尔切洛说。

“好，”我说，“说‘复杂性’并没有集中你的概率质量。”

“哦，”马尔切洛说，“就像‘涌现’。嗯。所以……现在我得考虑X是怎么发生的……”

那时我心想：“*也许**这个**能教会他*。”

复杂性不是一个无用的概念。它有数学上的定义，比如柯尔莫哥洛夫复杂性和瓦普尼克-切尔沃宁基斯复杂性。即使在直观层面，复杂性也常常值得思考——你需要判断一个假设的复杂性，看看它在给定证据下是否“太复杂”，或者看看一个设计并尝试简化它。

但概念本身并不是有用或无用的。只有用法是正确或错误的。在马尔切洛试图在这场舞蹈中迈出的步伐中，他试图“白得”一些东西，试图得到什么而不付出代价。这是我所在领域中极为常见的失误。你可以加入一场关于人工通用智能的讨论，看到人们左冲右突地做同样的事——不断跳过他们不理解的东西，而没有意识到自己在做什么。

眨眼之间，这就发生了：把一个不具备控制能力的因果节点放在某个神秘的事物后面，这个因果节点看起来像是解释，但其实并不是。这个错误发生在语言之下。它不需要特殊的性格缺陷；它是人类默认的思维方式，是自古以来人类的思维方式。

你必须避免的是跳过那些神秘的部分；你必须停留在神秘处，直接面对它。有很多词语能够跳过神秘，而其中一些在其他语境下是合法的——比如“复杂性”。但本质的错误是那种跳过，不管它后面有什么因果节点。跳过并不是一种思考，而是一种微型思考。你必须密切关注，才能察觉自己在做什么。当你训练自己避免跳过时，它会成为本能，而非语言推理。你必须感受到你地图上哪些部分仍然是空白，最重要的是，注意这种感觉。

我怀疑在学术界存在巨大的压力，要把问题扫到地毯下，这样你就可以呈现一篇看起来完整的论文。你会因为一个包含一些“涌现现象”的看似完整模型而获得更多的赞誉，而不是一个明确不完整的地图，上面写着“我不知道这部分是如何工作的”或者“然后发生了一个奇迹”。期刊甚至可能不接受后一篇论文，因为谁知道这些未知步骤是否正是所有有趣之处？<sup>2</sup>

如果你在做一个革命性的人工智能初创公司，那么有一个更大的压力去掩盖问题；否则你将不得不承认自己还不知道如何构建正确的人工智能，而你目前的生活计划将会轰然崩塌。但也许我在[过度解释](https://www.lesswrong.com/rationality/correspondence-bias)，因为跳过是人类的默认反应。如果你想找例子，只需看看人们讨论宗教、哲学、灵性或任何他们没有接受过专业训练的科学。

马尔切洛和我在我们的人工智能工作中制定了一个惯例：当我们遇到我们不理解的东西时，这种情况常常发生，我们会说“魔法”——就像“X 神奇地做了 Y”——来提醒自己这里是一个未解决的问题，是我们理解中的一个空白。说“魔法”远比说“复杂性”或“涌现”要好；后者会创造一种理解的假象。更明智的是说“魔法”，并给自己留一个占位符，提醒自己将来需要做的工作。

---

<sup>1</sup> [《技术解释的技术解释》](http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation)

<sup>2</sup> 是的，有时你地图上所有非魔法部分的东西也被证明是无关紧要的。这就是你进入未知领域、试图逐步解决问题时需要付出的代价。但这也使得知道自己还没有完成变得更加重要。大多数人根本不敢进入未知领域，因为他们对浪费时间的恐惧。

---

## Say Not “Complexity”

by Eliezer Yudkowsky

Once upon a time . . .

This is a story from when I first met Marcello, with whom I would later work for a year on AI theory; but at this point I had not yet accepted him as my apprentice. I knew that he competed at the national level in mathematical and computing olympiads, which sufficed to attract my attention for a closer look; but I didn’t know yet if he could learn to think about AI.

I had asked Marcello to say how he thought an AI might discover how to solve a Rubik’s Cube. Not in a preprogrammed way, which is trivial, but rather how the AI itself might figure out the laws of the Rubik universe and reason out how to exploit them. How would an AI invent for itself the concept of an “operator,” or “macro,” which is the key to solving the Rubik’s Cube?

At some point in this discussion, Marcello said: “Well, I think the AI needs complexity to do X, and complexity to do Y—”

And I said, “Don’t say ‘complexity.’ ”

Marcello said, “Why not?”

I said, “Complexity should never be a goal in itself. You may need to use a particular algorithm that adds some amount of complexity, but complexity for the sake of complexity just makes things harder.” (I was thinking of all the people whom I had heard advocating that the Internet would “wake up” and become an AI when it became “sufficiently complex.”)

And Marcello said, “But there’s got to be some amount of complexity that does it.”

I closed my eyes briefly, and tried to think of how to explain it all in words. To me, saying “complexity” simply felt like the wrong move in the AI dance. No one can think fast enough to deliberate, in words, about each sentence of their stream of consciousness; for that would require an infinite recursion. We think in words, but our stream of consciousness is steered below the level of words, by the trained-in remnants of past insights and harsh experience . . .

I said, “Did you read ‘A Technical Explanation of Technical Explanation’?”<sup>1</sup>

“Yes,” said Marcello.

“Okay,” I said. “Saying ‘complexity’ doesn’t concentrate your probability mass.”

“Oh,” Marcello said, “like ‘emergence.’ Huh. So . . . now I’ve got to think about how X might actually happen . . .”

That was when I thought to myself, “_Maybe **this** one is teachable._”

Complexity is not a useless concept. It has mathematical definitions attached to it, such as Kolmogorov complexity and Vapnik-Chervonenkis complexity. Even on an intuitive level, complexity is often worth thinking about—you have to judge the complexity of a hypothesis and decide if it’s “too complicated” given the supporting evidence, or look at a design and try to make it simpler.

But concepts are not useful or useless of themselves. Only usages are correct or incorrect. In the step Marcello was trying to take in the dance, he was trying to explain something for free, get something for nothing. It is an extremely common misstep, at least in my field. You can join a discussion on artificial general intelligence and watch people doing the same thing, left and right, over and over again—constantly skipping over things they don’t understand, without realizing that’s what they’re doing.

In an eyeblink it happens: putting a non-controlling causal node behind something mysterious, a causal node that feels like an explanation but isn’t. The mistake takes place below the level of words. It requires no special character flaw; it is how human beings think by default, how they have thought since the ancient times.

What you must avoid is skipping over the mysterious part; you must linger at the mystery to confront it directly. There are many words that can skip over mysteries, and some of them would be legitimate in other contexts—“complexity,” for example. But the essential mistake is that skip-over, regardless of what causal node goes behind it. The skip-over is not a thought, but a microthought. You have to pay close attention to catch yourself at it. And when you train yourself to avoid skipping, it will become a matter of instinct, not verbal reasoning. You have to feel which parts of your map are still blank, and more importantly, pay attention to that feeling.

I suspect that in academia there is a huge pressure to sweep problems under the rug so that you can present a paper with the appearance of completeness. You’ll get more kudos for a seemingly complete model that includes some “emergent phenomena,” versus an explicitly incomplete map where the label says “I got no clue how this part works” or “then a miracle occurs.” A journal may not even accept the latter paper, since who knows but that the unknown steps are really where everything interesting happens?<sup>2</sup>

And if you’re working on a revolutionary AI startup, there is an even huger pressure to sweep problems under the rug; or you will have to admit to yourself that you don’t know how to build the right kind of AI yet, and your current life plans will come crashing down in ruins around your ears. But perhaps I am [over-explaining](https://www.lesswrong.com/rationality/correspondence-bias), since skip-over happens by default in humans. If you’re looking for examples, just watch people discussing religion or philosophy or spirituality or any science in which they were not professionally trained.

Marcello and I developed a convention in our AI work: when we ran into something we didn’t understand, which was often, we would say “magic”—as in, X magically does Y”—to remind ourselves that here was an unsolved problem, a gap in our understanding. It is far better to say “magic” than “complexity” or “emergence”; the latter words create an illusion of understanding. Wiser to say “magic,” and leave yourself a placeholder, a reminder of work you will have to do later.

---

<sup>1</sup>http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation

<sup>2</sup>And yes, it sometimes happens that all the non-magical parts of your map turn out to also be non-important. That’s the price you sometimes pay, for entering into terra incognita and trying to solve problems incrementally. But that makes it even more important to know when you aren’t finished yet. Mostly, people don’t dare to enter terra incognita at all, for the deadly fear of wasting their time.